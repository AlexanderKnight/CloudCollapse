{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano import *\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "from theano import pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dscalar('x')\n",
    "y = T.dscalar('y')\n",
    "z = x + y\n",
    "f = function([x,y], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.allclose(f(16.3, 12.1), 28.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x + y)\n"
     ]
    }
   ],
   "source": [
    "print(pp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.     2.  1026.]\n"
     ]
    }
   ],
   "source": [
    "a = theano.tensor.vector()\n",
    "out = a + a**10\n",
    "f = theano.function([a], out)\n",
    "print(f([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float64, vector)>)]\n",
      "Looping 1000 times took 38.526862 seconds\n",
      "Result is [ 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753\n",
      "  1.62323285]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 #define _CUDA_NDARRAY_C\n",
      "2 \n",
      "3 #include <Python.h>\n",
      "4 #include <structmember.h>\n",
      "5 \n",
      "6 #include <numpy/arrayobject.h>\n",
      "7 #include <iostream>\n",
      "8 \n",
      "9 #include \"cuda_ndarray.cuh\"\n",
      "10 \n",
      "11 //If true, when there is a gpu malloc or free error, we print the size of allocated memory on the device.\n",
      "12 #define COMPUTE_GPU_MEM_USED 0\n",
      "13 \n",
      "14 //If true, we fill with NAN allocated device memory.\n",
      "15 #define ALLOC_MEMSET 0\n",
      "16 \n",
      "17 //If true, we print out when we free a device pointer, uninitialize a\n",
      "18 //CudaNdarray, or allocate a device pointer\n",
      "19 #define PRINT_FREE_MALLOC 0\n",
      "20 \n",
      "21 //If true, we do error checking at the start of functions, to make sure there\n",
      "22 //is not a pre-existing error when the function is called.\n",
      "23 //You probably need to set the environment variable\n",
      "24 //CUDA_LAUNCH_BLOCKING=1, and/or modify the CNDA_THREAD_SYNC\n",
      "25 //preprocessor macro in cuda_ndarray.cuh\n",
      "26 //if you want this to work.\n",
      "27 #define PRECHECK_ERROR 0\n",
      "28 \n",
      "29 //If true, we release the GIL around blocking GPU calls, to allow other Python\n",
      "30 //threads to run in the meantime. For a single-threaded program, the overhead\n",
      "31 //is neglectible (about 20ms for 1 million GIL release/reclaim cycles). Can\n",
      "32 //still be overridden on compilation with -DRELEASE_GIL=0 in nvcc.flags.\n",
      "33 #ifndef RELEASE_GIL\n",
      "34 #define RELEASE_GIL 1\n",
      "35 #endif\n",
      "36 #if RELEASE_GIL\n",
      "37 #define CNDA_BEGIN_ALLOW_THREADS Py_BEGIN_ALLOW_THREADS\n",
      "38 #define CNDA_END_ALLOW_THREADS Py_END_ALLOW_THREADS\n",
      "39 #else\n",
      "40 #define CNDA_BEGIN_ALLOW_THREADS\n",
      "41 #define CNDA_END_ALLOW_THREADS\n",
      "42 #endif\n",
      "43 \n",
      "44 cublasHandle_t handle = NULL;\n",
      "45 \n",
      "46 /////////////////////////\n",
      "47 // Alloc and Free\n",
      "48 /////////////////////////\n",
      "49 \n",
      "50 static int g_gpu_context_active = 0;\n",
      "51 \n",
      "52 \n",
      "53 PyObject *\n",
      "54 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args);\n",
      "55 static PyObject *CudaNdarray_get_shape(CudaNdarray *self, void *closure);\n",
      "56 \n",
      "57 \n",
      "58 /**\n",
      "59  *\n",
      "60  * In the test program I'm using, the _outstanding_mallocs decreases with every call.\n",
      "61  * This suggests there are more free() calls being made than alloc(), but I can't figure out why.\n",
      "62  *\n",
      "63  */\n",
      "64 int _outstanding_mallocs[] = {0,0};\n",
      "65 \n",
      "66 #if COMPUTE_GPU_MEM_USED\n",
      "67 size_t _allocated_size = 0;\n",
      "68 size_t _max_allocated_size = 0;\n",
      "69 \n",
      "70 const int TABLE_SIZE = 10000;\n",
      "71 struct table_struct{\n",
      "72     void* ptr;\n",
      "73     size_t size;\n",
      "74 };\n",
      "75 table_struct _alloc_size_table[TABLE_SIZE];\n",
      "76 #endif\n",
      "77 \n",
      "78 void * device_malloc(size_t size)\n",
      "79 {\n",
      "80     return device_malloc(size, VERBOSE_DEVICE_MALLOC);\n",
      "81 }\n",
      "82 \n",
      "83 void * device_malloc(size_t size, int verbose)\n",
      "84 {\n",
      "85     #if PRECHECK_ERROR\n",
      "86         cudaThreadSynchronize();\n",
      "87         cudaError_t prevError = cudaGetLastError();\n",
      "88         if (cudaSuccess != prevError)\n",
      "89         {\n",
      "90             fprintf(stderr,\n",
      "91                     \"Error existed before calling device_malloc. %s\\n\",\n",
      "92                     cudaGetErrorString(prevError)\n",
      "93                     );\n",
      "94         }\n",
      "95     #endif\n",
      "96     void * rval=NULL;\n",
      "97     cudaError_t err = cudaMalloc(&rval, size);\n",
      "98     if (cudaSuccess != err)\n",
      "99     {\n",
      "100         // Clear the error flag, cudaMalloc doesn't do it.\n",
      "101         // Currently this returns the same thing as err, but if in future\n",
      "102         // it returns something else I still don't see why we should ignore\n",
      "103         // it.  All we want to do here is reset the flag.\n",
      "104         cudaGetLastError();\n",
      "105         if (verbose)\n",
      "106         {\n",
      "107             size_t free = 0, total = 0;\n",
      "108             cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "109             if (err2 != cudaSuccess){\n",
      "110                 cudaGetLastError();\n",
      "111                 fprintf(stderr,\n",
      "112                         \"Error when trying to find the memory information\"\n",
      "113                         \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "114             }\n",
      "115             #if COMPUTE_GPU_MEM_USED\n",
      "116                 fprintf(stderr,\n",
      "117                         \"Error allocating %zd bytes of device memory (%s).\"\n",
      "118                         \" new total bytes allocated: %d.\"\n",
      "119                         \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
      "120                         size, cudaGetErrorString(err), _allocated_size,\n",
      "121                         free, total);\n",
      "122             #else\n",
      "123                 fprintf(stderr,\n",
      "124                         \"Error allocating %zd bytes of device memory (%s).\"\n",
      "125                         \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
      "126                         size, cudaGetErrorString(err), free, total);\n",
      "127             #endif\n",
      "128         }\n",
      "129         PyErr_Format(PyExc_MemoryError,\n",
      "130                      \"Error allocating %zd bytes of device memory (%s).\",\n",
      "131                      size, cudaGetErrorString(err));\n",
      "132         return NULL;\n",
      "133     }\n",
      "134     if (rval != NULL){\n",
      "135         // Can it happen that cudaMalloc return cudaSuccess, but return a NULL ptr?\n",
      "136         // Could this be what happen if size is 0?\n",
      "137         _outstanding_mallocs[0] += 1;\n",
      "138 \n",
      "139 #if COMPUTE_GPU_MEM_USED\n",
      "140         _allocated_size += size;\n",
      "141         _max_allocated_size = std::max(_max_allocated_size, _allocated_size);\n",
      "142         int i = 0;\n",
      "143         for(;i<TABLE_SIZE;i++){\n",
      "144             if(NULL==_alloc_size_table[i].ptr){\n",
      "145                 _alloc_size_table[i].ptr=rval;\n",
      "146                 _alloc_size_table[i].size=size;\n",
      "147                 break;\n",
      "148             }\n",
      "149         }\n",
      "150         if (i == TABLE_SIZE){\n",
      "151             fprintf(stderr,\n",
      "152                     \"When tracking GPU malloc, our table size wasn't big enough.\"\n",
      "153                     \" So we loose some tracking. Raise the value of TABLE_SIZE in the file cuda_ndarra.cu\");\n",
      "154         }\n",
      "155 #endif\n",
      "156     }\n",
      "157     //fprintf(stderr,\n",
      "158     //\"allocated %li bytes of device memory (%s). new total bytes allocated: %d. ptr: %p\\n\",\n",
      "159     //(long)size, cudaGetErrorString(err),_allocated_size,rval);\n",
      "160 \n",
      "161     if(ALLOC_MEMSET){\n",
      "162         //We init them to nan to make sure we catch more debug case.\n",
      "163         cudaMemset(rval, 0xFF, size);\n",
      "164         //printf(\"MEMSET\\n\");\n",
      "165     }\n",
      "166     #if PRINT_FREE_MALLOC\n",
      "167         fprintf(stderr, \"device malloc %p of size %d\\n\", rval, size);\n",
      "168     #endif\n",
      "169     return rval;\n",
      "170 }\n",
      "171 \n",
      "172 int device_free(void *ptr)\n",
      "173 {\n",
      "174     #if PRECHECK_ERROR\n",
      "175         cudaThreadSynchronize();\n",
      "176         cudaError_t prevError = cudaGetLastError();\n",
      "177         if (cudaSuccess != prevError)\n",
      "178         {\n",
      "179             fprintf(stderr,\n",
      "180                     \"Error existed before calling device_free. %s\\n\",\n",
      "181                     cudaGetErrorString(prevError)\n",
      "182                     );\n",
      "183         }\n",
      "184     #endif\n",
      "185     #if PRINT_FREE_MALLOC\n",
      "186         size_t free = 0, total = 0;\n",
      "187         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "188         if (err2 != cudaSuccess){\n",
      "189             cudaGetLastError();\n",
      "190             fprintf(stderr,\n",
      "191                     \"Error when tring to find the memory information\"\n",
      "192                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "193         }\n",
      "194         #if COMPUTE_GPU_MEM_USED\n",
      "195         {\n",
      "196             int i = 0;\n",
      "197             for(;i<TABLE_SIZE;i++)\n",
      "198                 if(_alloc_size_table[i].ptr==ptr){\n",
      "199                     break;\n",
      "200                 }\n",
      "201             assert(i<TABLE_SIZE);\n",
      "202             fprintf(stderr, \"device_free %p of size %d.\"\n",
      "203                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "204                     ptr, _alloc_size_table[i].size, free, total);\n",
      "205         }\n",
      "206         #else\n",
      "207             fprintf(stderr, \"device_free %p.\"\n",
      "208                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "209                     ptr, free, total);\n",
      "210         #endif\n",
      "211     #endif\n",
      "212 \n",
      "213     // if there is no gpu context, the call to cudaFree will fail; skip it entirely\n",
      "214     if(!g_gpu_context_active) {\n",
      "215         return 0;\n",
      "216     }\n",
      "217 \n",
      "218     // We need sync as the Theano's GC could remove intermediate variable that\n",
      "219     // are still needed as the gpu kernel are running or in the queue.\n",
      "220     CNDA_BEGIN_ALLOW_THREADS\n",
      "221     cudaThreadSynchronize();\n",
      "222     CNDA_END_ALLOW_THREADS\n",
      "223 \n",
      "224     cudaError_t err =  cudaFree(ptr);\n",
      "225     if (cudaSuccess != err)\n",
      "226     {\n",
      "227         // Clear the error flag, cudaFree doesn't do it.\n",
      "228         // Currently this returns the same thing as err, but if in future\n",
      "229         // it returns something else I still don't see why we should ignore\n",
      "230         // it.  All we want to do here is reset the flag.\n",
      "231         cudaGetLastError();\n",
      "232         size_t free = 0, total = 0;\n",
      "233         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "234         if (err2 != cudaSuccess){\n",
      "235             cudaGetLastError();\n",
      "236             fprintf(stderr,\n",
      "237                     \"Error when tring to find the memory information\"\n",
      "238                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "239         }\n",
      "240         #if COMPUTE_GPU_MEM_USED\n",
      "241         {\n",
      "242             int i = 0;\n",
      "243             for(;i<TABLE_SIZE;i++)\n",
      "244                 if(_alloc_size_table[i].ptr==ptr){\n",
      "245                     break;\n",
      "246                 }\n",
      "247             assert(i<TABLE_SIZE);\n",
      "248             fprintf(stderr,\n",
      "249                     \"Error freeing device pointer %p (%s) of size %d. %zd byte already allocated.\"\n",
      "250                     \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
      "251                     ptr, cudaGetErrorString(err),\n",
      "252                     _alloc_size_table[i].size, _allocated_size, free, total);\n",
      "253         }\n",
      "254         #else\n",
      "255             fprintf(stderr,\n",
      "256                     \"Error freeing device pointer %p (%s).\"\n",
      "257                     \" Driver report %zd bytes free and %zd bytes total \\n\",\n",
      "258                     ptr,\n",
      "259                     cudaGetErrorString(err), free, total);\n",
      "260         #endif\n",
      "261         if (NULL != PyErr_Occurred()){\n",
      "262             fprintf(stderr,\n",
      "263                     \"device_free: cudaFree() returned an error, but there is already an\"\n",
      "264                     \" Python error set. This happen during the clean up when there is a\"\n",
      "265                     \" first error and the CUDA driver is in a so bad state that it don't\"\n",
      "266                     \" work anymore. We keep the previous error set to help debugging it.\");\n",
      "267             return -1;\n",
      "268         }\n",
      "269         PyErr_Format(PyExc_MemoryError,\n",
      "270                 \"error freeing device pointer %p (%s)\",\n",
      "271                 ptr,\n",
      "272                 cudaGetErrorString(err));\n",
      "273         return -1;\n",
      "274     }\n",
      "275     _outstanding_mallocs[0] -= (ptr != NULL);\n",
      "276     #if COMPUTE_GPU_MEM_USED\n",
      "277         int i=0;\n",
      "278         size_t total_freed = 0;\n",
      "279         for(;i<TABLE_SIZE;i++)\n",
      "280             if(_alloc_size_table[i].ptr==ptr){\n",
      "281                 _allocated_size -= _alloc_size_table[i].size;\n",
      "282                 total_freed += _alloc_size_table[i].size;\n",
      "283                 _alloc_size_table[i].ptr=0;\n",
      "284                 _alloc_size_table[i].size=0;\n",
      "285 \n",
      "286                 break;\n",
      "287             }\n",
      "288         //if(i==TABLE_SIZE)\n",
      "289         //    printf(\"Unallocated unknow size!\\n\");\n",
      "290         //fprintf(stderr, \"freed %li bytes of device memory (%s). %d already allocated, ptr=%p\\n\", (long)total_freed, cudaGetErrorString(err),_allocated_size,ptr);\n",
      "291     #endif\n",
      "292     return 0;\n",
      "293 }\n",
      "294 \n",
      "295 static PyObject *\n",
      "296 outstanding_mallocs(PyObject* self, PyObject * args)\n",
      "297 {\n",
      "298     return PyInt_FromLong(_outstanding_mallocs[0]);\n",
      "299 }\n",
      "300 \n",
      "301 \n",
      "302 static void *work_mem = NULL;\n",
      "303 static size_t work_size = 0;\n",
      "304 \n",
      "305 /*\n",
      "306  * Returns a chunk of memory for temporary work inside of an op. You can only\n",
      "307  * request a single chunk of memory at a time since it is reused.\n",
      "308  */\n",
      "309 void *get_work_mem(size_t sz) {\n",
      "310     if (sz < work_size)\n",
      "311         return work_mem;\n",
      "312     device_free(work_mem);\n",
      "313     work_mem = device_malloc(sz);\n",
      "314     work_size = sz;\n",
      "315     if (work_mem == NULL)\n",
      "316         work_size = 0;\n",
      "317     return work_mem;\n",
      "318 }\n",
      "319 \n",
      "320 /////////////////////////\n",
      "321 // Static helper methods\n",
      "322 /////////////////////////\n",
      "323 \n",
      "324 static void\n",
      "325 CudaNdarray_null_init(CudaNdarray*self)\n",
      "326 {\n",
      "327     self->base = NULL;\n",
      "328     self->nd = -1;\n",
      "329     self->host_structure = NULL;\n",
      "330     self->data_allocated = 0;\n",
      "331     self->dev_structure_fresh = 1;\n",
      "332     self->dev_structure = NULL;\n",
      "333     self->devdata = NULL;\n",
      "334 }\n",
      "335 \n",
      "336 static int\n",
      "337 CudaNdarray_uninit(CudaNdarray*self)\n",
      "338 {\n",
      "339     #if PRINT_FREE_MALLOC\n",
      "340         fprintf(stderr, \"CudaNdarray_uninit %p\\n\", self);\n",
      "341     #endif\n",
      "342     int rval = 0;\n",
      "343     if (self->data_allocated) {\n",
      "344         assert(self->devdata);\n",
      "345         if (device_free(self->devdata))\n",
      "346         {\n",
      "347             fprintf(stderr,\n",
      "348                     \"CudaNdarray_uninit: error freeing self->devdata. (self=%p, self->devata=%p)\\n\",\n",
      "349                     self, self->devdata);\n",
      "350             rval = -1;\n",
      "351         }\n",
      "352         self->devdata = NULL;\n",
      "353         self->data_allocated = 0;\n",
      "354     }\n",
      "355     if (self->dev_structure)\n",
      "356     {\n",
      "357         if (device_free(self->dev_structure))\n",
      "358         {\n",
      "359             fprintf(stderr,\n",
      "360                     \"CudaNdarray_uninit: error freeing dev_structure memory %p (self=%p)\\n\",\n",
      "361                     self->dev_structure, self);\n",
      "362             rval = -1;\n",
      "363         }\n",
      "364         self->dev_structure = NULL;\n",
      "365     }\n",
      "366     if (self->host_structure)\n",
      "367     {\n",
      "368         free(self->host_structure);\n",
      "369         self->host_structure = NULL;\n",
      "370     }\n",
      "371     self->nd = -1;\n",
      "372     Py_XDECREF(self->base);\n",
      "373     self->base = NULL;\n",
      "374     return rval;\n",
      "375 }\n",
      "376 \n",
      "377 \n",
      "378 //make the rightmost coords change fastest\n",
      "379 //TODO: why does a downward for-loop not work????\n",
      "380 //TODO: use the log2_dims and driver code to remove / and %\n",
      "381 //TODO: skip the last division (when d == 0)\n",
      "382 #define decl_k_elemwise_unary_rowmajor(name, F) \\\n",
      "383 __global__ void name (unsigned int numEls,  \\\n",
      "384         unsigned int nd, \\\n",
      "385         const int * dim,  \\\n",
      "386         const float * a_data, const int * a_str, \\\n",
      "387         float * z_data, const int * z_str) \\\n",
      "388 { \\\n",
      "389     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
      "390     const unsigned int numThreads = blockDim.x * gridDim.x; \\\n",
      "391  \\\n",
      "392     for (unsigned int i = idx; i < numEls; i += numThreads) \\\n",
      "393     { \\\n",
      "394         unsigned int ii = i; \\\n",
      "395         const float * a_i = a_data; \\\n",
      "396         float * z_i = z_data; \\\n",
      "397         for (unsigned int _d = 0; _d < nd; ++_d) \\\n",
      "398         { \\\n",
      "399             unsigned int d = nd - _d-1;  \\\n",
      "400             int i_d = ii % dim[d]; /* i_d is our position in the d'th dimension   */ \\\n",
      "401             ii = ii / dim[d]; \\\n",
      "402             a_i += i_d * a_str[d]; /* increment our a and z pointers by i_d elements */ \\\n",
      "403             z_i += i_d * z_str[d]; \\\n",
      "404         } \\\n",
      "405         z_i[0] = F(a_i[0]); \\\n",
      "406     } \\\n",
      "407 }\n",
      "408 \n",
      "409 template<typename T> __device__ T unary_copy(T a) { return a; }\n",
      "410 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_copy, unary_copy<float>)\n",
      "411 \n",
      "412 template<typename T> __device__ T unary_exp(T a) { return exp(a); }\n",
      "413 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_exp, unary_exp<float>)\n",
      "414 \n",
      "415 /////////////////////////////\n",
      "416 // Satisfying reqs to be Type\n",
      "417 /////////////////////////////\n",
      "418 \n",
      "419 //DON'T use directly(if their is other CudaNdarray that point to it, it will cause problem)! use Py_DECREF() instead\n",
      "420 static void\n",
      "421 CudaNdarray_dealloc(CudaNdarray* self)\n",
      "422 {\n",
      "423     if (0) std::cerr << \"CudaNdarray dealloc \" << self << \" \" << self->devdata << '\\n';\n",
      "424     if(Py_REFCNT(self) > 1)\n",
      "425       printf(\"WARNING:CudaNdarray_dealloc called when there is still active reference to it.\\n\");\n",
      "426     CudaNdarray_uninit(self);\n",
      "427     Py_TYPE(self)->tp_free((PyObject*)self);\n",
      "428     --_outstanding_mallocs[1];\n",
      "429     if (0)\n",
      "430     {\n",
      "431         fprintf(stderr, \"device_malloc_counts: (device) %i (obj) %i\\n\",\n",
      "432                 _outstanding_mallocs[0],\n",
      "433                 _outstanding_mallocs[1]);\n",
      "434     }\n",
      "435 }\n",
      "436 \n",
      "437 static PyObject *\n",
      "438 CudaNdarray_new(PyTypeObject *type, PyObject *args, PyObject *kwds)\n",
      "439 {\n",
      "440     CudaNdarray *self;\n",
      "441 \n",
      "442     self = (CudaNdarray *)type->tp_alloc(type, 0);\n",
      "443     if (self != NULL)\n",
      "444     {\n",
      "445         CudaNdarray_null_init(self);\n",
      "446         ++_outstanding_mallocs[1];\n",
      "447     }\n",
      "448     return (PyObject *)self;\n",
      "449 }\n",
      "450 static int\n",
      "451 CudaNdarray_init(CudaNdarray *self, PyObject *args, PyObject *kwds)\n",
      "452 {\n",
      "453     PyObject *arr=NULL;\n",
      "454 \n",
      "455     if (! PyArg_ParseTuple(args, \"O\", &arr))\n",
      "456         return -1;\n",
      "457     if (! PyArray_Check(arr))\n",
      "458     {\n",
      "459         PyErr_SetString(PyExc_TypeError, \"PyArray arg required\");\n",
      "460         return -1;\n",
      "461     }\n",
      "462     int rval = CudaNdarray_CopyFromArray(self, (PyArrayObject*)arr);\n",
      "463     return rval;\n",
      "464 }\n",
      "465 static PyMemberDef CudaNdarray_members[] =\n",
      "466 {\n",
      "467     /*\n",
      "468     {\"first\", T_OBJECT_EX, offsetof(CudaNdarray, first), 0,\n",
      "469      \"first name\"},\n",
      "470     {\"last\", T_OBJECT_EX, offsetof(CudaNdarray, last), 0,\n",
      "471      \"last name\"},\n",
      "472     {\"number\", T_INT, offsetof(CudaNdarray, number), 0,\n",
      "473      \"noddy number\"},\n",
      "474      */\n",
      "475     {NULL}  /* Sentinel */\n",
      "476 };\n",
      "477 \n",
      "478 PyObject * CudaNdarray_CreateArrayObj(CudaNdarray * self, PyObject *args)\n",
      "479 {\n",
      "480     PyObject * dtype = NULL;\n",
      "481     if (args && !PyArg_ParseTuple(args, \"|O\", &dtype))\n",
      "482         return NULL;\n",
      "483     if (dtype) {\n",
      "484         PyArray_Descr* dtype2;\n",
      "485         // PyArray_DescrConverter try to convert anything to a PyArray_Descr.\n",
      "486         if(!PyArray_DescrConverter(dtype, &dtype2))\n",
      "487         {\n",
      "488             PyObject * str = PyObject_Repr(dtype);\n",
      "489             PyErr_Format(PyExc_TypeError,\n",
      "490                          \"CudaNdarray dtype parameter not understood: %s\",\n",
      "491                          PyString_AsString(str)\n",
      "492                          );\n",
      "493             Py_CLEAR(str);\n",
      "494             return NULL;\n",
      "495         }\n",
      "496         int typeNum = dtype2->type_num;\n",
      "497         Py_DECREF(dtype2);\n",
      "498         if (typeNum != NPY_FLOAT32)\n",
      "499         {\n",
      "500             PyObject * str = PyObject_Repr(dtype);\n",
      "501             PyErr_Format(PyExc_TypeError,\n",
      "502                          \"CudaNdarray support only support float32 dtype, provided: %d\",\n",
      "503                          typeNum\n",
      "504                          );\n",
      "505             Py_CLEAR(str);\n",
      "506             return NULL;\n",
      "507         }\n",
      "508     }\n",
      "509 \n",
      "510     int verbose = 0;\n",
      "511     if(self->nd>=0 && CudaNdarray_SIZE(self)==0){\n",
      "512         npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "513         assert (npydims);\n",
      "514         for (int i = 0; i < self->nd; ++i) npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "515         PyObject * rval = PyArray_SimpleNew(self->nd, npydims, REAL_TYPENUM);\n",
      "516         free(npydims);\n",
      "517         if (!rval){\n",
      "518             return NULL;\n",
      "519         }\n",
      "520         assert (PyArray_ITEMSIZE((PyArrayObject *)rval) == sizeof(real));\n",
      "521         return rval;\n",
      "522     }\n",
      "523     if ((self->nd < 0) || (self->devdata == 0))\n",
      "524     {\n",
      "525         PyErr_SetString(PyExc_ValueError, \"can't copy from un-initialized CudaNdarray\");\n",
      "526         return NULL;\n",
      "527     }\n",
      "528     CudaNdarray * contiguous_self = NULL;\n",
      "529     if (CudaNdarray_is_c_contiguous(self))\n",
      "530     {\n",
      "531         contiguous_self = self;\n",
      "532         Py_INCREF(contiguous_self);\n",
      "533         if (verbose) std::cerr << \"CreateArrayObj already contiguous\" << contiguous_self << '\\n';\n",
      "534     }\n",
      "535     else\n",
      "536     {\n",
      "537         contiguous_self = (CudaNdarray*)CudaNdarray_Copy(self);\n",
      "538         if (verbose) std::cerr << \"CreateArrayObj created contiguous\" << contiguous_self << '\\n';\n",
      "539     }\n",
      "540     if (!contiguous_self)\n",
      "541     {\n",
      "542         return NULL;\n",
      "543     }\n",
      "544 \n",
      "545     npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "546     assert (npydims);\n",
      "547     for (int i = 0; i < self->nd; ++i)\n",
      "548         npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "549     PyArrayObject * rval = (PyArrayObject *) PyArray_SimpleNew(self->nd,\n",
      "550                                                                npydims,\n",
      "551                                                                REAL_TYPENUM);\n",
      "552     free(npydims);\n",
      "553     if (!rval)\n",
      "554     {\n",
      "555         Py_DECREF(contiguous_self);\n",
      "556         return NULL;\n",
      "557     }\n",
      "558 \n",
      "559     assert (PyArray_ITEMSIZE(rval) == sizeof(real));\n",
      "560 \n",
      "561     npy_intp rval_size = PyArray_SIZE(rval);\n",
      "562     void *rval_data = PyArray_DATA(rval);\n",
      "563     cublasStatus_t err;\n",
      "564     CNDA_BEGIN_ALLOW_THREADS\n",
      "565     err = cublasGetVector(rval_size, sizeof(real),\n",
      "566                           contiguous_self->devdata, 1,\n",
      "567                           rval_data, 1);\n",
      "568     //CNDA_THREAD_SYNC;  // unneeded because cublasGetVector is blocking anyway\n",
      "569     CNDA_END_ALLOW_THREADS\n",
      "570 \n",
      "571     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "572     {\n",
      "573         PyErr_SetString(PyExc_RuntimeError, \"error copying data to host\");\n",
      "574         Py_DECREF(rval);\n",
      "575         rval = NULL;\n",
      "576     }\n",
      "577 \n",
      "578     Py_DECREF(contiguous_self);\n",
      "579     return (PyObject *)rval;\n",
      "580 }\n",
      "581 \n",
      "582 // TODO-- we have two functions here, ZEROS and Zeros.\n",
      "583 // ZEROS is meant to be called just from C code (you don't need to pass it PyObject * s)\n",
      "584 // but this naming is very weird, makes it look like a macro\n",
      "585 // we should figure out the correct convention and change to that\n",
      "586 PyObject* CudaNdarray_ZEROS(int n, int * dims)\n",
      "587 {\n",
      "588 \n",
      "589     size_t total_elements = 1;\n",
      "590 \n",
      "591     for(size_t i=0;i<n;i++){\n",
      "592         // Detect overflow on unsigned integer\n",
      "593         if (dims[i] != 0 && total_elements > (SIZE_MAX / dims[i])) {\n",
      "594             PyErr_Format(PyExc_RuntimeError,\n",
      "595                          \"Can't store in size_t for the bytes requested %llu * %llu\",\n",
      "596                          (unsigned long long)total_elements,\n",
      "597                          (unsigned long long)dims[i]);\n",
      "598             return NULL;\n",
      "599         }\n",
      "600         total_elements*=dims[i];\n",
      "601     }\n",
      "602 \n",
      "603     // total_elements now contains the size of the array, in reals\n",
      "604     if (total_elements > (SIZE_MAX / sizeof(real))){\n",
      "605         PyErr_Format(PyExc_RuntimeError,\n",
      "606                      \"Can't store in size_t for the bytes requested %llu * 4\",\n",
      "607                      (unsigned long long)total_elements);\n",
      "608         return NULL;\n",
      "609     }\n",
      "610     size_t total_size = total_elements * sizeof(real);\n",
      "611 \n",
      "612     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_New();\n",
      "613     if (!rval)\n",
      "614     {\n",
      "615         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: call to New failed\");\n",
      "616         return NULL;\n",
      "617     }\n",
      "618 \n",
      "619     if (CudaNdarray_alloc_contiguous(rval, n, dims))\n",
      "620     {\n",
      "621         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: allocation failed.\");\n",
      "622         Py_DECREF(rval);\n",
      "623         return NULL;\n",
      "624     }\n",
      "625 \n",
      "626     // Fill with zeros\n",
      "627     //fprintf(stdout, \"Sizeof: %d\\n\", total_size);\n",
      "628     if (cudaSuccess != cudaMemset(rval->devdata, 0, total_size))\n",
      "629     {\n",
      "630         PyErr_Format(PyExc_MemoryError,\n",
      "631                      \"CudaNdarray_ZEROS: Error memsetting %llu bytes of device memory.\",\n",
      "632                      (unsigned long long)total_size);\n",
      "633         Py_DECREF(rval);\n",
      "634         return NULL;\n",
      "635     }\n",
      "636 \n",
      "637     if (cnda_copy_structure_to_device(rval))\n",
      "638     {\n",
      "639         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: syncing structure to device failed\");\n",
      "640         Py_DECREF(rval);\n",
      "641         return NULL;\n",
      "642     }\n",
      "643     return (PyObject*) rval;\n",
      "644 }\n",
      "645 \n",
      "646 // declared as a static method (hence 1st parameter is not used)\n",
      "647 // Based on _Copy and _dimshuffle\n",
      "648 PyObject* CudaNdarray_Zeros(PyObject* _unused, PyObject* shape)\n",
      "649 {\n",
      "650     if(!shape)\n",
      "651     {\n",
      "652         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_Zeros: function takes at least 1 argument (0 given)\");\n",
      "653         return NULL;\n",
      "654     }\n",
      "655     if(!PySequence_Check(shape))\n",
      "656     {\n",
      "657         PyErr_SetString(PyExc_TypeError, \"shape argument must be a sequence\");\n",
      "658         return NULL;\n",
      "659     }\n",
      "660 \n",
      "661     int shplen = PySequence_Length(shape);\n",
      "662 \n",
      "663     if (shplen == 0)\n",
      "664     {\n",
      "665         return CudaNdarray_ZEROS(0, NULL);\n",
      "666     }\n",
      "667 \n",
      "668     int* newdims = (int *)malloc(sizeof(int) * shplen);\n",
      "669 \n",
      "670     if (!newdims)\n",
      "671     {\n",
      "672         PyErr_SetString(PyExc_MemoryError,\n",
      "673             \"CudaNdarray_Zeros: Failed to allocate temporary space\");\n",
      "674         return NULL;\n",
      "675     }\n",
      "676 \n",
      "677     // start from the end to compute strides\n",
      "678     for (int i = shplen-1; i >= 0; --i)\n",
      "679     {\n",
      "680         PyObject* shp_el_obj = PySequence_GetItem(shape, i);\n",
      "681         if(shp_el_obj == NULL)\n",
      "682         {\n",
      "683             // shouldn't happen since we checked length before...\n",
      "684             PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_Zeros: Index out of bound in sequence\");\n",
      "685             free(newdims);\n",
      "686             return NULL;\n",
      "687         }\n",
      "688 \n",
      "689         int shp_el = PyInt_AsLong(shp_el_obj);\n",
      "690         Py_DECREF(shp_el_obj);\n",
      "691 \n",
      "692         if (shp_el < 0)\n",
      "693         {\n",
      "694             PyErr_SetString(PyExc_ValueError, \"CudaNdarray_Zeros: shape must contain only non-negative values for size of a dimension\");\n",
      "695             free(newdims);\n",
      "696             return NULL;\n",
      "697         }\n",
      "698 \n",
      "699         newdims[i] = shp_el;\n",
      "700     }\n",
      "701 \n",
      "702     PyObject* rval = CudaNdarray_ZEROS(shplen,newdims);\n",
      "703 \n",
      "704     free(newdims);\n",
      "705 \n",
      "706     return (PyObject*)rval;\n",
      "707 }\n",
      "708 \n",
      "709 \n",
      "710 \n",
      "711 \n",
      "712 \n",
      "713 PyObject * CudaNdarray_Copy(const CudaNdarray * self)\n",
      "714 {\n",
      "715     PyObject * rval = CudaNdarray_New();\n",
      "716     if ((!rval) || (-1 == self->nd))\n",
      "717     {\n",
      "718         return rval;\n",
      "719     }\n",
      "720     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "721     {\n",
      "722         Py_DECREF(rval);\n",
      "723         return NULL;\n",
      "724     }\n",
      "725     if (CudaNdarray_CopyFromCudaNdarray((CudaNdarray*)rval, self))\n",
      "726     {\n",
      "727         Py_DECREF(rval);\n",
      "728         return NULL;\n",
      "729     }\n",
      "730     return rval;\n",
      "731 }\n",
      "732 PyObject * CudaNdarray_DeepCopy(CudaNdarray * self, PyObject * memo)\n",
      "733 {\n",
      "734     assert(PyDict_Check(memo));\n",
      "735     PyObject * selfkey = PyInt_FromLong((long)self);\n",
      "736     assert(selfkey);\n",
      "737     if (PyDict_Contains(memo, selfkey))\n",
      "738     {\n",
      "739         PyObject * rval = PyDict_GetItem(memo, selfkey);\n",
      "740         Py_DECREF(selfkey);\n",
      "741         Py_XINCREF(rval);\n",
      "742         return rval;\n",
      "743     }\n",
      "744     else\n",
      "745     {\n",
      "746         PyObject * rval = CudaNdarray_Copy(self);\n",
      "747         if (0) std::cerr << \"DeepCopy created \" << rval << \" devdata \" << ((CudaNdarray*)rval)->devdata << \"\\n\";\n",
      "748         if (NULL == rval)\n",
      "749         {\n",
      "750             Py_DECREF(selfkey);\n",
      "751             return NULL;\n",
      "752         }\n",
      "753         if (PyDict_SetItem(memo, selfkey, rval))\n",
      "754         {\n",
      "755             Py_DECREF(rval);\n",
      "756             Py_DECREF(selfkey);\n",
      "757             return NULL;\n",
      "758         }\n",
      "759         Py_DECREF(selfkey);\n",
      "760         return rval;\n",
      "761     }\n",
      "762 }\n",
      "763 PyObject * CudaNdarray_ReduceSum(CudaNdarray * self, PyObject * py_reduce_mask)\n",
      "764 {\n",
      "765     if (!PySequence_Check(py_reduce_mask))\n",
      "766     {\n",
      "767         PyErr_SetString(PyExc_TypeError, \"reduce_mask must be sequence of ints\");\n",
      "768         return NULL;\n",
      "769     }\n",
      "770     int len = PySequence_Length(py_reduce_mask);\n",
      "771     if (len != self->nd)\n",
      "772     {\n",
      "773         PyErr_SetString(PyExc_TypeError, \"length of reduce_mask must match self->nd\");\n",
      "774         return NULL;\n",
      "775     }\n",
      "776     CudaNdarray * self_sum = (CudaNdarray*)CudaNdarray_New();\n",
      "777     if (!self_sum)\n",
      "778     {\n",
      "779         return NULL;\n",
      "780     }\n",
      "781     //TODO: allocate a fixed size dimshuffle_pattern_cache on the stack,\n",
      "782     //      and use it if it is big enough.\n",
      "783     int * dimshuffle_pattern = (int*)malloc(len * 2 * sizeof(int));\n",
      "784     int * sum_dims = dimshuffle_pattern + len;\n",
      "785     int n_remaining_dims = 0;\n",
      "786     if (!dimshuffle_pattern)\n",
      "787     {\n",
      "788         Py_DECREF(self_sum);\n",
      "789         PyErr_SetString(PyExc_MemoryError, \"failed to alloc internal storage\");\n",
      "790         return NULL;\n",
      "791     }\n",
      "792     for (int i = 0; i < len; ++i)\n",
      "793     {\n",
      "794         PyObject *o_i = PySequence_GetItem(py_reduce_mask, i);\n",
      "795         int o_i_int = PyInt_AsLong(o_i);\n",
      "796         Py_XDECREF(o_i);\n",
      "797         if (PyErr_Occurred())\n",
      "798         {\n",
      "799             Py_DECREF(self_sum);\n",
      "800             free(dimshuffle_pattern);\n",
      "801             return NULL;\n",
      "802         }\n",
      "803         if (o_i_int) // this is a dimension over which we are reducing\n",
      "804         {\n",
      "805             sum_dims[i] = 1;\n",
      "806         }\n",
      "807         else\n",
      "808         {\n",
      "809             sum_dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "810             dimshuffle_pattern[n_remaining_dims++] = i;\n",
      "811         }\n",
      "812     }\n",
      "813     if (0   || CudaNdarray_alloc_contiguous(self_sum, len, sum_dims)\n",
      "814             || CudaNdarray_reduce_sum(self_sum, self)\n",
      "815             || CudaNdarray_dimshuffle(self_sum, n_remaining_dims, dimshuffle_pattern))\n",
      "816     {\n",
      "817         Py_DECREF(self_sum);\n",
      "818         free(dimshuffle_pattern);\n",
      "819         return NULL;\n",
      "820     }\n",
      "821     free(dimshuffle_pattern);\n",
      "822     return (PyObject*)self_sum;\n",
      "823 }\n",
      "824 \n",
      "825 // Reshape self to the new shape gived by the tuple shape.\n",
      "826 //\n",
      "827 // If self is c contiguous, it return a view. Otherwise it always do a copy.\n",
      "828 // TODO: make it return a view when the strides allow it even if it is not\n",
      "829 //       c contiguous\n",
      "830 PyObject * CudaNdarray_Reshape(CudaNdarray * self, PyObject * shape)\n",
      "831 {\n",
      "832     if(!CudaNdarray_is_c_contiguous(self))\n",
      "833     {\n",
      "834         // allocate new space\n",
      "835         //TODO: test to see if we can re-use old one and take a new param to\n",
      "836         //  use this\n",
      "837         CudaNdarray* rval = (CudaNdarray*) CudaNdarray_Copy(self);\n",
      "838         if (!rval)\n",
      "839         {\n",
      "840             return NULL;\n",
      "841         }\n",
      "842 \n",
      "843         CudaNdarray* ret = (CudaNdarray*) CudaNdarray_Reshape(rval, shape);\n",
      "844         Py_XDECREF(rval);\n",
      "845         return (PyObject*)ret;\n",
      "846     }\n",
      "847 \n",
      "848     // check shape tuple\n",
      "849     unsigned int rval_nd;\n",
      "850     unsigned int * rval_dims;\n",
      "851     unsigned int rval_size = 1;\n",
      "852 \n",
      "853     if (PyTuple_Check(shape)){\n",
      "854         // copy shape to integer array\n",
      "855         rval_nd = PyTuple_Size(shape);\n",
      "856     }else if (PyInt_Check(shape)){\n",
      "857         rval_nd = 1;\n",
      "858     }else{\n",
      "859         PyErr_SetString(PyExc_TypeError, \"shape must be tuple of integers or an integer\");\n",
      "860         return NULL;\n",
      "861     }\n",
      "862     rval_dims = (unsigned int*)malloc(rval_nd * sizeof(int));\n",
      "863 \n",
      "864     if(PyTuple_Check(shape)){\n",
      "865         for (int i = 0; i < rval_nd; ++i)\n",
      "866         {\n",
      "867             rval_dims[i] = PyInt_AsLong(PyTuple_GetItem(shape, i)); //GetItem returns borrowed reference\n",
      "868             if (PyErr_Occurred()) //error in AsLong\n",
      "869             {\n",
      "870                 free(rval_dims);\n",
      "871                 return NULL;\n",
      "872             }\n",
      "873             if(rval_dims[i]<=0){\n",
      "874                 PyErr_Format(PyExc_ValueError, \"Reshape has invalid dimension %i (must be >0)\",rval_dims[i]);\n",
      "875                 free(rval_dims);\n",
      "876                 return NULL;\n",
      "877             }\n",
      "878             rval_size = rval_size * rval_dims[i];\n",
      "879         }\n",
      "880     }else{\n",
      "881         rval_size = PyInt_AsLong(shape);\n",
      "882         rval_dims[0] = rval_size;\n",
      "883     }\n",
      "884     // calculate new size, assert same as old size\n",
      "885     if (rval_size != CudaNdarray_SIZE(self))\n",
      "886     {\n",
      "887         PyErr_Format(PyExc_ValueError, \"size must remain unchanged, changed from %i to %i\", CudaNdarray_SIZE(self), rval_size);\n",
      "888         free(rval_dims);\n",
      "889         return NULL;\n",
      "890     }\n",
      "891     if (rval_size==0)\n",
      "892     {\n",
      "893         PyObject * rval = CudaNdarray_NewDims(rval_nd, rval_dims);\n",
      "894         free(rval_dims);\n",
      "895         return rval;\n",
      "896     }\n",
      "897 \n",
      "898     //return a view, not a copy\n",
      "899     //we can do this as we checked self is c_contiguous\n",
      "900     CudaNdarray * rval = (CudaNdarray * )CudaNdarray_New(rval_nd);\n",
      "901 \n",
      "902     if (!rval || 0 != rval->data_allocated\n",
      "903         ||CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "904     {\n",
      "905         Py_XDECREF(rval);\n",
      "906         free(rval_dims);\n",
      "907         return NULL;\n",
      "908     }\n",
      "909     //set dim and stride\n",
      "910     int size = 1;\n",
      "911     for (int i = rval_nd-1; i >= 0; --i)\n",
      "912     {\n",
      "913         CudaNdarray_set_stride(rval, i, (rval_dims[i] == 1) ? 0 : size);\n",
      "914         CudaNdarray_set_dim(rval, i, rval_dims[i]);\n",
      "915         size = size * rval_dims[i];\n",
      "916     }\n",
      "917     free(rval_dims);\n",
      "918     return (PyObject*)rval;\n",
      "919 }\n",
      "920 \n",
      "921 PyObject * CudaNdarray_View(const CudaNdarray * self)\n",
      "922 {\n",
      "923     CudaNdarray * rval = (CudaNdarray*)CudaNdarray_New(self->nd);\n",
      "924     if (!rval || CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "925     {\n",
      "926         Py_XDECREF(rval);\n",
      "927         rval = NULL;\n",
      "928     }\n",
      "929     else\n",
      "930     {\n",
      "931         for (int i = 0; i < self->nd; ++i)\n",
      "932         {\n",
      "933             CudaNdarray_set_dim(rval, i, CudaNdarray_HOST_DIMS(self)[i]);\n",
      "934             CudaNdarray_set_stride(rval, i, CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "935         }\n",
      "936     }\n",
      "937     return (PyObject*)rval;\n",
      "938 }\n",
      "939 \n",
      "940 /*\n",
      "941  * d0,... are the output dims\n",
      "942  * indices are a list of index to operate on\n",
      "943  *         They are int32 viewed as float32.\n",
      "944  * a is the output\n",
      "945  * b is the input\n",
      "946  * dB0, the source leading dimensions size\n",
      "947  */\n",
      "948 template <int operator_num>\n",
      "949 __global__ void k_take_3(const int d0, const int d1, const int d2,\n",
      "950                          const npy_int64* indices,\n",
      "951                          float* a,\n",
      "952                          const int sA0, const int sA1, const int sA2,\n",
      "953                          const float* b, const int dB0,\n",
      "954                          const int sB0, const int sB1, const int sB2,\n",
      "955                          int* err){\n",
      "956     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "957         npy_int64 idx = indices[i0];\n",
      "958         if (idx<0)\n",
      "959             idx += dB0; // To allow negative indexing.\n",
      "960         if ((idx < 0) || (idx >= dB0)){\n",
      "961             // Any value other the 0 probably work. But to be more safe, I want\n",
      "962             // to change all bits to prevent problem with concurrent write that\n",
      "963             // could cross cache line. But this should not happen with the\n",
      "964             // current code and driver.\n",
      "965             *err = 0xFFFF;\n",
      "966             continue;\n",
      "967         }\n",
      "968         for (int i1 = threadIdx.x; i1 < d1; i1 += blockDim.x){\n",
      "969             for (int i2 = threadIdx.y; i2 < d2; i2 += blockDim.y){\n",
      "970                 int a_idx = i0*sA0 + i1*sA1 + i2*sA2;\n",
      "971                 int b_idx = idx*sB0 + i1*sB1 + i2*sB2;\n",
      "972                 a[a_idx] = b[b_idx];\n",
      "973             }\n",
      "974         }\n",
      "975     }\n",
      "976 }\n",
      "977 \n",
      "978 // Pointor to 1 int on the device\n",
      "979 // Used in CudaNdarray_TakeFrom to tell that there is an out of bound error\n",
      "980 // When it is allocated, it should always be 0\n",
      "981 // So if there is an error, we must reset it to 0 BEFORE we raise the error\n",
      "982 // This prevent us from setting it to 0 before each use\n",
      "983 static int* err_var = NULL;\n",
      "984 \n",
      "985 // We try to be similar to the PyArray_TakeFrom function\n",
      "986 //http://docs.scipy.org/doc/numpy/reference/c-api.array.html\n",
      "987 //TODO: support other clip mode then raise(clip, wrap)\n",
      "988 //self is the input that we copy data from.\n",
      "989 //The indices that we receive MUST be an CudaNdarray(float32)\n",
      "990 //    that is in fact a view to int64 indices\n",
      "991 PyObject*\n",
      "992 CudaNdarray_TakeFrom(CudaNdarray * self, PyObject *args){\n",
      "993     int verbose = 0;\n",
      "994     PyObject * indices_obj = NULL;\n",
      "995     //int axis; Default None, that mean the flattened array.\n",
      "996     PyObject * axis_obj = Py_None;\n",
      "997     PyObject * out_obj = Py_None;\n",
      "998     PyObject * clipmode_obj = NULL;\n",
      "999     int max_threads = 1; // max threads per blocks\n",
      "1000 \n",
      "1001     if (! PyArg_ParseTuple(args, \"O|OOOi\", &indices_obj, &axis_obj,\n",
      "1002                            &out_obj, &clipmode_obj, &max_threads))\n",
      "1003         return NULL;\n",
      "1004 \n",
      "1005     //Check argument indices\n",
      "1006     //TODO: if not a numpy.ndarray, convert to numpy.ndarray\n",
      "1007     //TODO: If a CudaNdarray, accept it and suppose the data is int32? is float32 number of int?\n",
      "1008     //TODO: Support ndarray of other dtype then int32\n",
      "1009     //TODO: support list of indices that are not c_contiguous\n",
      "1010     CudaNdarray * indices = NULL;\n",
      "1011     if (CudaNdarray_Check(indices_obj)) {\n",
      "1012         if (verbose) printf(\"cudandarray indices\\n\");\n",
      "1013         indices = (CudaNdarray*) indices_obj;\n",
      "1014         Py_INCREF(indices);\n",
      "1015     } else if (0 && PyArray_Check(indices_obj)) {\n",
      "1016         PyErr_SetString(PyExc_NotImplementedError, \"CudaNdarray_TakeFrom: The indices must cudandarray with float32 value.\");\n",
      "1017         return NULL;\n",
      "1018 \n",
      "1019         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1020         if (PyArray_TYPE((PyArrayObject *)indices_obj) != NPY_INT32) {\n",
      "1021             PyErr_SetString(PyExc_TypeError, \"CudaNdarray_TakeFrom: need a ndarray for indices with dtype int32\");\n",
      "1022             return NULL;\n",
      "1023         }\n",
      "1024         if (PyArray_NDIM(((PyArrayObject*)indices_obj)) != 1) {\n",
      "1025             PyErr_SetString(PyExc_TypeError, \"CudaNdarray_TakeFrom: need a CudaNdarray of indices with only 1 dimensions\");\n",
      "1026             return NULL;\n",
      "1027         }\n",
      "1028         PyArray_Descr* float32_descr = PyArray_DescrFromType(NPY_FLOAT32);\n",
      "1029         PyObject * indices_float32 = NULL;\n",
      "1030         indices_float32 = PyArray_View((PyArrayObject*)indices_obj,\n",
      "1031                                                   float32_descr, NULL);\n",
      "1032         Py_DECREF(float32_descr);\n",
      "1033         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1034         //indices_float32 = PyArray_Cast((PyArrayObject*)indices_obj,\n",
      "1035         //                              NPY_FLOAT32);\n",
      "1036         //Py_INCREF(indices_float32);\n",
      "1037         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1038         if (!indices_float32)\n",
      "1039             return NULL;\n",
      "1040 \n",
      "1041         indices = (CudaNdarray*) CudaNdarray_New();\n",
      "1042         if (verbose) printf(\"\\nndarray after new\\n\");\n",
      "1043         if (! indices){\n",
      "1044             Py_DECREF(indices_float32);\n",
      "1045             return NULL;\n",
      "1046         }\n",
      "1047         if (CudaNdarray_CopyFromArray(indices,\n",
      "1048                                       (PyArrayObject *)indices_float32)){\n",
      "1049             Py_DECREF(indices_float32);\n",
      "1050 \n",
      "1051             return NULL;\n",
      "1052         }\n",
      "1053         Py_DECREF(indices_float32);\n",
      "1054     } else {\n",
      "1055         PyErr_SetString(PyExc_TypeError,\n",
      "1056                         \"CudaNdarray_TakeFrom: need a CudaNdarray(float32) that\"\n",
      "1057                         \" is a view from int64 data for indices\");\n",
      "1058         return NULL;\n",
      "1059     }\n",
      "1060 \n",
      "1061     if (verbose) {\n",
      "1062         printf(\"indices used on the gpu\\n\");\n",
      "1063         fprint_CudaNdarray(stdout, indices);\n",
      "1064         PyObject * used_indices = CudaNdarray_CreateArrayObj(indices);\n",
      "1065         PyObject_Print(used_indices, stdout, 0);\n",
      "1066         Py_DECREF(used_indices);\n",
      "1067     }\n",
      "1068     if (verbose) printf(\"after print of object\\n\");\n",
      "1069     if(!CudaNdarray_is_c_contiguous(indices) != 0) {\n",
      "1070         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1071                         \"CudaNdarray_TakeFrom: The indices must be contiguous in memory.\");\n",
      "1072         Py_DECREF(indices_obj);\n",
      "1073         return NULL;\n",
      "1074     }\n",
      "1075     int nb_indices = CudaNdarray_SIZE((CudaNdarray *)indices) / 2;// int64 are 8 bytes, float32 are 4 bytes\n",
      "1076 \n",
      "1077     //Check argument axis\n",
      "1078     //TODO: implement the default and other axis\n",
      "1079     PyObject * axis_iobj = PyNumber_Long(axis_obj);\n",
      "1080     if (!axis_iobj) {\n",
      "1081         PyErr_SetString(PyExc_NotImplementedError,\"CudaNdarray_TakeFrom: axis must be convertable to a long\");\n",
      "1082         Py_DECREF(indices);\n",
      "1083         return NULL;\n",
      "1084     }\n",
      "1085     long axis = PyInt_AsLong(axis_iobj);\n",
      "1086     Py_DECREF(axis_iobj); axis_iobj=NULL;\n",
      "1087     if (axis != 0) {\n",
      "1088         PyErr_SetString(PyExc_NotImplementedError,\"CudaNdarray_TakeFrom: only axis=0 is currently supported\");\n",
      "1089         Py_DECREF(indices);\n",
      "1090         return NULL;\n",
      "1091     }\n",
      "1092 \n",
      "1093     //Check argument out_obj\n",
      "1094     CudaNdarray * out = NULL;\n",
      "1095     if (out_obj && CudaNdarray_Check(out_obj))\n",
      "1096         out = (CudaNdarray*) out_obj;\n",
      "1097     if (out && (out->nd != self->nd ||\n",
      "1098                 CudaNdarray_HOST_DIMS(out)[0] != nb_indices))\n",
      "1099         out = NULL;\n",
      "1100     int * dims = (int *)malloc(sizeof(int) * self->nd);\n",
      "1101     dims[0] = nb_indices;\n",
      "1102 \n",
      "1103     for (int i=1 ; i<self->nd ; i++) {\n",
      "1104         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "1105         if (out && CudaNdarray_HOST_DIMS(out)[i] != dims[i]) {\n",
      "1106             out = NULL;\n",
      "1107         }\n",
      "1108     }\n",
      "1109     if (!out) {\n",
      "1110         out = (CudaNdarray*)CudaNdarray_New();\n",
      "1111         if (!out){\n",
      "1112             Py_DECREF(indices);\n",
      "1113             free(dims);\n",
      "1114             return NULL;\n",
      "1115         }\n",
      "1116         if (CudaNdarray_alloc_contiguous(out, self->nd, dims)) {\n",
      "1117             Py_DECREF(out);\n",
      "1118             Py_DECREF(indices);\n",
      "1119             free(dims);\n",
      "1120             return NULL;\n",
      "1121         }\n",
      "1122     }else {\n",
      "1123         Py_INCREF(out);\n",
      "1124     }\n",
      "1125 \n",
      "1126     //Check argument clipmode\n",
      "1127     if (clipmode_obj) {\n",
      "1128         char * clipmode = PyString_AsString(clipmode_obj);\n",
      "1129         if (! clipmode){\n",
      "1130             Py_DECREF(indices);\n",
      "1131             Py_DECREF(out);\n",
      "1132             free(dims);\n",
      "1133             return NULL;\n",
      "1134         }\n",
      "1135         if (strcmp(clipmode, \"raise\") != 0) {\n",
      "1136             PyErr_Format(PyExc_NotImplementedError,\n",
      "1137                          \"CudaNdarray_TakeFrom: only the raise mode is currently supported. Got '%s'\",\n",
      "1138                          clipmode);\n",
      "1139             Py_DECREF(indices);\n",
      "1140             Py_DECREF(out);\n",
      "1141             free(dims);\n",
      "1142             return NULL;\n",
      "1143         }\n",
      "1144     }\n",
      "1145     void (*k3)(const int, const int, const int,\n",
      "1146                const npy_int64*,\n",
      "1147                float*, const int, const int, const int,\n",
      "1148                const float*, const int,\n",
      "1149                const int, const int, const int,\n",
      "1150                int*);\n",
      "1151     k3 = k_take_3<CPY>;\n",
      "1152 \n",
      "1153     // Create the memory place that will store the error information.\n",
      "1154     if (err_var == NULL) {\n",
      "1155         err_var = (int*)device_malloc(sizeof(int));\n",
      "1156         if (!err_var) { // PyErr set by device_malloc\n",
      "1157             Py_DECREF(indices);\n",
      "1158             Py_DECREF(out);\n",
      "1159             free(dims);\n",
      "1160             return NULL;\n",
      "1161         }\n",
      "1162         cudaError_t err = cudaMemset((void*)err_var, 0, sizeof(int));\n",
      "1163         if (cudaSuccess != err) {\n",
      "1164             // Clear the error flag, cudaMemset doesn't do it.\n",
      "1165             // Currently this returns the same thing as err, but if in future\n",
      "1166             // it returns something else I still don't see why we should ignore\n",
      "1167             // it.  All we want to do here is reset the flag.\n",
      "1168             cudaGetLastError();\n",
      "1169             PyErr_Format(PyExc_RuntimeError,\n",
      "1170                          \"Error setting device error code to 0. %s\",\n",
      "1171                          cudaGetErrorString(err));\n",
      "1172             Py_DECREF(indices);\n",
      "1173             Py_DECREF(out);\n",
      "1174             free(dims);\n",
      "1175             return NULL;\n",
      "1176         }\n",
      "1177     }\n",
      "1178 \n",
      "1179     dim3 n_blocks(std::min(CudaNdarray_HOST_DIMS(out)[0],65535),1,1);\n",
      "1180     if(CudaNdarray_HOST_DIMS(out)[0] == 0){\n",
      "1181         // We take 0 elements, so no need for the rest of the code.\n",
      "1182         // This speed up that case AND fix crash otherwise.\n",
      "1183         free(dims);\n",
      "1184         Py_DECREF(indices);\n",
      "1185         return (PyObject *)out;\n",
      "1186     }\n",
      "1187 \n",
      "1188     switch (self->nd) {\n",
      "1189         case 1:\n",
      "1190             {\n",
      "1191                 dim3 n_threads(1, 1, 1);\n",
      "1192                 if (verbose)\n",
      "1193                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1194                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1195                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1196                            cudaGetLastError(), self->nd,\n",
      "1197                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1198                 k3<<<n_blocks, n_threads>>>(\n",
      "1199                         dims[0],\n",
      "1200                         1,\n",
      "1201                         1,\n",
      "1202                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1203                         CudaNdarray_DEV_DATA(out),\n",
      "1204                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1205                         1,\n",
      "1206                         1,\n",
      "1207                         CudaNdarray_DEV_DATA(self),\n",
      "1208                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1209                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1210                         1,\n",
      "1211                         1,\n",
      "1212                         err_var);\n",
      "1213             }\n",
      "1214             break;\n",
      "1215         case 2:\n",
      "1216             {\n",
      "1217                 dim3 n_threads(std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads), 1, 1);\n",
      "1218 \n",
      "1219                 if (verbose)\n",
      "1220                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1221                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1222                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1223                            cudaGetLastError(), self->nd,\n",
      "1224                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1225 \n",
      "1226                 k3<<<n_blocks, n_threads>>>(\n",
      "1227                         dims[0], //dimensions\n",
      "1228                         dims[1],\n",
      "1229                         1,\n",
      "1230                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1231                         CudaNdarray_DEV_DATA(out),\n",
      "1232                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1233                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1234                         1,\n",
      "1235                         CudaNdarray_DEV_DATA(self),\n",
      "1236                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1237                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1238                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1239                         1,\n",
      "1240                         err_var);\n",
      "1241             }\n",
      "1242             break;\n",
      "1243         case 3:\n",
      "1244             {\n",
      "1245                 int ty = std::min(CudaNdarray_HOST_DIMS(out)[2], max_threads);\n",
      "1246                 int tx = std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads / ty);\n",
      "1247                 dim3 n_threads(tx, ty, 1);\n",
      "1248                 if (verbose)\n",
      "1249                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1250                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1251                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1252                            cudaGetLastError(), self->nd,\n",
      "1253                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1254                 k3<<<n_blocks, n_threads>>>(\n",
      "1255                         dims[0], //dimensions\n",
      "1256                         dims[1],\n",
      "1257                         dims[2],\n",
      "1258                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1259                         CudaNdarray_DEV_DATA(out),\n",
      "1260                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1261                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1262                         CudaNdarray_HOST_STRIDES(out)[2],\n",
      "1263                         CudaNdarray_DEV_DATA(self),\n",
      "1264                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1265                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1266                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1267                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1268                         err_var);\n",
      "1269             }\n",
      "1270             break;\n",
      "1271     default:\n",
      "1272         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1273                         \"CudaNdarray_TakeFrom: only input with 1, 2 or 3\"\n",
      "1274                         \" dimensions are currently supported\");\n",
      "1275 \n",
      "1276     }\n",
      "1277     free(dims);\n",
      "1278     CNDA_THREAD_SYNC;\n",
      "1279     cudaError_t err = cudaGetLastError();\n",
      "1280     if (cudaSuccess != err) {\n",
      "1281         PyErr_Format(PyExc_RuntimeError,\n",
      "1282                      \"Cuda error: %s: %s.\\n\",\n",
      "1283                      \"CudaNdarray_TakeFrom\",\n",
      "1284                      cudaGetErrorString(err));\n",
      "1285         Py_DECREF(indices);\n",
      "1286         Py_DECREF(out);\n",
      "1287         return NULL;\n",
      "1288     }\n",
      "1289     //-10 could be any value different then 0.\n",
      "1290     int cpu_err_var=-10;\n",
      "1291 \n",
      "1292     CNDA_BEGIN_ALLOW_THREADS\n",
      "1293     // As we execute cudaMemcpy on the default stream, it waits for all\n",
      "1294     // kernels (on all streams) to be finished before starting to copy\n",
      "1295     err = cudaMemcpy(&cpu_err_var, err_var, sizeof(int),\n",
      "1296                      cudaMemcpyDeviceToHost);\n",
      "1297     CNDA_END_ALLOW_THREADS\n",
      "1298     if (cudaSuccess != err) {\n",
      "1299         PyErr_Format(\n",
      "1300             PyExc_RuntimeError,\n",
      "1301             \"Cuda error: %s: %s when trying to get the error value.\\n\",\n",
      "1302             \"CudaNdarray_TakeFrom\",\n",
      "1303             cudaGetErrorString(err));\n",
      "1304         Py_DECREF(indices);\n",
      "1305         Py_DECREF(out);\n",
      "1306         return NULL;\n",
      "1307     }\n",
      "1308 \n",
      "1309     if (cpu_err_var != 0) {\n",
      "1310         PyErr_Format(\n",
      "1311             PyExc_IndexError,\n",
      "1312             \"CudaNdarray_TakeFrom: One of the index value is out of bound.\\n\",\n",
      "1313             cpu_err_var);\n",
      "1314         // Must reset it to 0 to don't reset it before each use.\n",
      "1315         err = cudaMemset((void*)err_var, 0, sizeof(int));\n",
      "1316         if (cudaSuccess != err) {\n",
      "1317             PyErr_Format(PyExc_MemoryError, \"Error setting device error code to 0 after having an index error. %s\", cudaGetErrorString(err));\n",
      "1318             Py_DECREF(indices);\n",
      "1319             Py_DECREF(out);\n",
      "1320             return NULL;\n",
      "1321         }\n",
      "1322         Py_DECREF(indices);\n",
      "1323         Py_DECREF(out);\n",
      "1324         return NULL;\n",
      "1325 \n",
      "1326     }\n",
      "1327 \n",
      "1328     Py_DECREF(indices);\n",
      "1329 \n",
      "1330     if (verbose) printf(\"TAKE SUCCEDED\\n\");\n",
      "1331     return (PyObject *)out;\n",
      "1332 }\n",
      "1333 \n",
      "1334 \n",
      "1335 PyObject * CudaNdarray_SetStride(CudaNdarray * self, PyObject *args)\n",
      "1336 {\n",
      "1337     int pos, stride;\n",
      "1338     if (! PyArg_ParseTuple(args, \"ii\", &pos, &stride))\n",
      "1339         return NULL;\n",
      "1340     if ((pos < 0) || (pos >= self->nd))\n",
      "1341     {\n",
      "1342         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1343         return NULL;\n",
      "1344     }\n",
      "1345     CudaNdarray_set_stride(self, pos, stride);\n",
      "1346     if (cnda_copy_structure_to_device(self))\n",
      "1347     {\n",
      "1348         return NULL;\n",
      "1349     }\n",
      "1350     Py_INCREF(Py_None);\n",
      "1351     return Py_None;\n",
      "1352 }\n",
      "1353 PyObject * CudaNdarray_SetShapeI(CudaNdarray * self, PyObject *args)\n",
      "1354 {\n",
      "1355     int pos, dim;\n",
      "1356     if (! PyArg_ParseTuple(args, \"ii\", &pos, &dim))\n",
      "1357         return NULL;\n",
      "1358     if ((pos < 0) || (pos >= self->nd))\n",
      "1359     {\n",
      "1360         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1361         return NULL;\n",
      "1362     }\n",
      "1363     CudaNdarray_set_dim(self, pos, dim);\n",
      "1364     if (cnda_copy_structure_to_device(self))\n",
      "1365     {\n",
      "1366         return NULL;\n",
      "1367     }\n",
      "1368     Py_INCREF(Py_None);\n",
      "1369     return Py_None;\n",
      "1370 }\n",
      "1371 \n",
      "1372 static PyObject *\n",
      "1373 CudaNdarray_exp(CudaNdarray* self)\n",
      "1374 {\n",
      "1375     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1376     if ((NULL == rval) || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1377     {\n",
      "1378         Py_XDECREF(rval);\n",
      "1379         return NULL;\n",
      "1380     }\n",
      "1381     unsigned int size = 1;\n",
      "1382     for (int i = 0; i < self->nd; i++)\n",
      "1383     {\n",
      "1384         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1385     }\n",
      "1386     unsigned int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1387     unsigned int n_blocks = std::min(ceil_intdiv(size,threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1388     k_elemwise_unary_rowmajor_exp<<<n_blocks,threads_per_block>>>(size, self->nd, CudaNdarray_DEV_DIMS(self),\n",
      "1389             CudaNdarray_DEV_DATA(self), CudaNdarray_DEV_STRIDES(self),\n",
      "1390             CudaNdarray_DEV_DATA(rval), CudaNdarray_DEV_STRIDES(rval));\n",
      "1391 \n",
      "1392     //TODO: don't do this right away, do it when we need the result\n",
      "1393     CNDA_THREAD_SYNC;\n",
      "1394     cudaError_t err = cudaGetLastError();\n",
      "1395     if( cudaSuccess != err)\n",
      "1396     {\n",
      "1397         Py_DECREF(rval);\n",
      "1398         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kExp\", cudaGetErrorString(err));\n",
      "1399         return NULL;\n",
      "1400     }\n",
      "1401 \n",
      "1402     return (PyObject*)rval;\n",
      "1403 }\n",
      "1404 \n",
      "1405 static PyMethodDef CudaNdarray_methods[] =\n",
      "1406 {\n",
      "1407     {\"__array__\",\n",
      "1408         (PyCFunction)CudaNdarray_CreateArrayObj, METH_VARARGS,\n",
      "1409         \"Copy from the device to a numpy ndarray\"},\n",
      "1410     {\"__copy__\",\n",
      "1411         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1412         \"Create a shallow copy of this object. used by module copy\"},\n",
      "1413     {\"__deepcopy__\",\n",
      "1414         (PyCFunction)CudaNdarray_DeepCopy, METH_O,\n",
      "1415         \"Create a copy of this object\"},\n",
      "1416     {\"zeros\",\n",
      "1417         (PyCFunction)CudaNdarray_Zeros, METH_STATIC | METH_O,\n",
      "1418         \"Create a new CudaNdarray with specified shape, filled with zeros.\"},\n",
      "1419     {\"copy\",\n",
      "1420         (PyCFunction)CudaNdarray_Copy, METH_NOARGS,\n",
      "1421         \"Create a copy of this object\"},\n",
      "1422     {\"is_c_contiguous\",\n",
      "1423         (PyCFunction)CudaNdarray_IS_C_Contiguous, METH_NOARGS,\n",
      "1424         \"Return True is the object is c contiguous. False otherwise.\"},\n",
      "1425     {\"reduce_sum\",\n",
      "1426         (PyCFunction)CudaNdarray_ReduceSum, METH_O,\n",
      "1427         \"Reduce over the given dimensions by summation\"},\n",
      "1428     {\"exp\",\n",
      "1429         (PyCFunction)CudaNdarray_exp, METH_NOARGS,\n",
      "1430         \"Return the exponential of all elements\"},\n",
      "1431     {\"reshape\",\n",
      "1432         (PyCFunction)CudaNdarray_Reshape, METH_O,\n",
      "1433         \"Return a reshaped view (or copy) of this ndarray\\n\\\n",
      "1434             The required argument is a tuple of integers specifying the shape of the new ndarray.\"},\n",
      "1435     {\"view\",\n",
      "1436         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1437         \"Return an alias of this ndarray\"},\n",
      "1438     {\"_set_stride\",\n",
      "1439         (PyCFunction)CudaNdarray_SetStride, METH_VARARGS,\n",
      "1440         \"For integer arguments (i, s), set the 'i'th stride to 's'\"},\n",
      "1441     {\"take\",\n",
      "1442         (PyCFunction)CudaNdarray_TakeFrom, METH_VARARGS,\n",
      "1443         \"Equivalent of numpy.take\"},\n",
      "1444     {\"_set_shape_i\",\n",
      "1445         (PyCFunction)CudaNdarray_SetShapeI, METH_VARARGS,\n",
      "1446         \"For integer arguments (i, s), set the 'i'th shape to 's'\"},\n",
      "1447     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "1448 };\n",
      "1449 \n",
      "1450 \n",
      "1451 ////////////////////\n",
      "1452 // Number protocol\n",
      "1453 ////////////////////\n",
      "1454 \n",
      "1455 __global__ void kAdd_contiguous(float* a, float* b, float* dest, unsigned int numEls) {\n",
      "1456     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "1457     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "1458 \n",
      "1459     for (unsigned int i = idx; i < numEls; i += numThreads) {\n",
      "1460         dest[i] = a[i] + b[i];\n",
      "1461     }\n",
      "1462 }\n",
      "1463 \n",
      "1464 // Will be called by __add__ in Python\n",
      "1465 static PyObject *\n",
      "1466 CudaNdarray_add(PyObject* py_self, PyObject * py_other)\n",
      "1467 {\n",
      "1468     if (! CudaNdarray_Check(py_self)) {\n",
      "1469         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on left\");\n",
      "1470         return NULL;\n",
      "1471     }\n",
      "1472     if (! CudaNdarray_Check(py_other)) {\n",
      "1473         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on right\");\n",
      "1474         return NULL;\n",
      "1475     }\n",
      "1476     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1477     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1478     if(!CudaNdarray_is_c_contiguous(self) || !CudaNdarray_is_c_contiguous(other)){\n",
      "1479         PyErr_SetString(PyExc_TypeError, \"We have implementet only the c_contiguous version for now.\");\n",
      "1480         return NULL;\n",
      "1481     }\n",
      "1482 \n",
      "1483     //standard elemwise size checks\n",
      "1484     if (self->nd != other->nd)\n",
      "1485     {\n",
      "1486         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_add: need same number of dims\");\n",
      "1487         return NULL;\n",
      "1488     }\n",
      "1489     //standard elemwise dim checks\n",
      "1490     unsigned int size = 1;\n",
      "1491     for (int i = 0; i< self->nd; ++i)\n",
      "1492     {\n",
      "1493         if (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "1494         {\n",
      "1495             PyErr_SetString(PyExc_TypeError, \"need same dimensions\");\n",
      "1496             return NULL;\n",
      "1497         }\n",
      "1498         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1499     }\n",
      "1500     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1501     if (!rval || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1502     {\n",
      "1503         Py_XDECREF(rval);\n",
      "1504         return NULL;\n",
      "1505     }\n",
      "1506 \n",
      "1507     if(CudaNdarray_SIZE((CudaNdarray *)py_self)==0 && CudaNdarray_SIZE((CudaNdarray *)py_other)==0){\n",
      "1508       return (PyObject *) rval;\n",
      "1509     }\n",
      "1510 \n",
      "1511     int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1512     int n_blocks = std::min(ceil_intdiv(size,(unsigned int)threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1513     kAdd_contiguous<<<n_blocks,threads_per_block>>>(\n",
      "1514             self->devdata, other->devdata, rval->devdata, size);\n",
      "1515     CNDA_THREAD_SYNC;\n",
      "1516     cudaError_t err = cudaGetLastError();\n",
      "1517     if( cudaSuccess != err)\n",
      "1518     {\n",
      "1519         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kAdd\", cudaGetErrorString(err));\n",
      "1520         Py_DECREF(rval);\n",
      "1521         return NULL;\n",
      "1522     }\n",
      "1523     return (PyObject *) rval;\n",
      "1524 }\n",
      "1525 \n",
      "1526 template <int operator_num>\n",
      "1527 __global__ void k_ielem_3(const int d0, const int d1, const int d2,\n",
      "1528         float* a, const int sA0, const int sA1, const int sA2,\n",
      "1529         const float* b, const int sB0, const int sB1, const int sB2){\n",
      "1530     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1531         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1532             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1533                 switch (operator_num)\n",
      "1534                 {\n",
      "1535                   case IADD:\n",
      "1536                     a[i0*sA0 + i1*sA1 + i2*sA2] += b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1537                     break;\n",
      "1538                   case IDIV:\n",
      "1539                     a[i0*sA0 + i1*sA1 + i2*sA2] /= b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1540                     break;\n",
      "1541                   case CPY:\n",
      "1542                     a[i0*sA0 + i1*sA1 + i2*sA2] = b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1543                     break;\n",
      "1544                 }\n",
      "1545             }\n",
      "1546         }\n",
      "1547     }\n",
      "1548 }\n",
      "1549 \n",
      "1550 template <int operator_num>\n",
      "1551 __global__ void k_ielem_4(const int d0, const int d1, const int d2, const int d3,\n",
      "1552                          float* a, const int sA0, const int sA1,\n",
      "1553                          const int sA2, const int sA3,\n",
      "1554                          const float* b, const int sB0, const int sB1,\n",
      "1555                          const int sB2, const int sB3){\n",
      "1556     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1557         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1558             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1559                 for (int i3 = threadIdx.y; i3 < d3; i3 += blockDim.y){\n",
      "1560                     switch (operator_num) {\n",
      "1561                         case IADD:\n",
      "1562                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1563                             += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1564                             break;\n",
      "1565                         case IDIV:\n",
      "1566                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1567                             /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1568                             break;\n",
      "1569                         case CPY:\n",
      "1570                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1571                             = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1572                             break;\n",
      "1573                     }\n",
      "1574                 }\n",
      "1575             }\n",
      "1576         }\n",
      "1577     }\n",
      "1578 }\n",
      "1579 \n",
      "1580 template <int operator_num>\n",
      "1581 __global__ void k_ielem_6(const int d0, const int d1,\n",
      "1582                           const int d2, const int d3,\n",
      "1583                           const int d4, const int d5,\n",
      "1584                           float* a, const int sA0, const int sA1,\n",
      "1585                           const int sA2, const int sA3,\n",
      "1586                           const int sA4, const int sA5,\n",
      "1587                           const float* b, const int sB0, const int sB1,\n",
      "1588                           const int sB2, const int sB3,\n",
      "1589                           const int sB4, const int sB5\n",
      "1590                           ){\n",
      "1591     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1592         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1593             for (int i2 = blockIdx.z; i2 < d2; i2 += gridDim.z){\n",
      "1594                 for (int i3 = threadIdx.x; i3 < d3; i3 += blockDim.x){\n",
      "1595                     for (int i4 = threadIdx.y; i4 < d4; i4 += blockDim.y){\n",
      "1596                         for (int i5 = threadIdx.z; i5 < d5; i5 += blockDim.z){\n",
      "1597                             switch (operator_num) {\n",
      "1598                             case IADD:\n",
      "1599                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1600                                     += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1601                                 break;\n",
      "1602                             case IDIV:\n",
      "1603                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1604                                     /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1605                                 break;\n",
      "1606                             case CPY:\n",
      "1607                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1608                                     = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1609                                 break;\n",
      "1610                             }\n",
      "1611                         }\n",
      "1612                     }\n",
      "1613                 }\n",
      "1614             }\n",
      "1615         }\n",
      "1616     }\n",
      "1617 }\n",
      "1618 \n",
      "1619 /*\n",
      "1620 CudaNdarray_inplace_elemwise\n",
      "1621 Compute elemwise, working inplace on A.\n",
      "1622 Currently implemented A / B, A + B and A = B\n",
      "1623 (the last is not tested and not used!)\n",
      "1624 \n",
      "1625 py_self - the CudaNdarray that we'll modify (A)\n",
      "1626 py_other - the other argument (B)\n",
      "1627 fct_nb - which operation to perform (operator_t)\n",
      "1628 \n",
      "1629 Returns 0 on success.\n",
      "1630 Returns -1 on failure, and sets Python exception.\n",
      "1631 \n",
      "1632 */\n",
      "1633 int\n",
      "1634 CudaNdarray_inplace_elemwise(PyObject* py_self, PyObject * py_other, operator_t fct_nb)\n",
      "1635 {\n",
      "1636     int verbose = 0;\n",
      "1637     void (*k3)(const int, const int, const int,\n",
      "1638                     float*, const int, const int, const int,\n",
      "1639                     const float*, const int, const int, const int);\n",
      "1640     void (*k4)(const int, const int, const int, const int,\n",
      "1641                     float*, const int, const int,\n",
      "1642                     const int, const int,\n",
      "1643                     const float*, const int, const int,\n",
      "1644                     const int, const int);\n",
      "1645     void (*k6)(const int, const int,\n",
      "1646                const int, const int,\n",
      "1647                const int, const int,\n",
      "1648                float*, const int, const int,\n",
      "1649                const int, const int,\n",
      "1650                const int, const int,\n",
      "1651                const float*, const int, const int,\n",
      "1652                const int, const int,\n",
      "1653                const int, const int);\n",
      "1654     switch (fct_nb)\n",
      "1655     {\n",
      "1656         case IADD:\n",
      "1657             k3 = k_ielem_3<IADD>;\n",
      "1658             k4 = k_ielem_4<IADD>;\n",
      "1659             k6 = k_ielem_6<IADD>;\n",
      "1660             break;\n",
      "1661         case IDIV:\n",
      "1662             k3 = k_ielem_3<IDIV>;\n",
      "1663             k4 = k_ielem_4<IDIV>;\n",
      "1664             k6 = k_ielem_6<IDIV>;\n",
      "1665             break;\n",
      "1666         case CPY:\n",
      "1667             k3 = k_ielem_3<CPY>;\n",
      "1668             k4 = k_ielem_4<CPY>;\n",
      "1669             k6 = k_ielem_6<CPY>;\n",
      "1670             break;\n",
      "1671         default:\n",
      "1672             assert (0);\n",
      "1673             PyErr_Format(\n",
      "1674                 PyExc_TypeError,\n",
      "1675                 \"CudaNdarray_inplace_elemwise invalid fct_nb (%i).\",\n",
      "1676                 (int)fct_nb);\n",
      "1677             return -1;\n",
      "1678     }\n",
      "1679     if (!CudaNdarray_Check(py_self)) {\n",
      "1680         PyErr_SetString(\n",
      "1681             PyExc_TypeError,\n",
      "1682             \"CudaNdarray_inplace_elemwise need a CudaNdarray on left\");\n",
      "1683         return -1;\n",
      "1684     }\n",
      "1685     CudaNdarray * new_other = NULL;\n",
      "1686     if (!CudaNdarray_Check(py_other)) {\n",
      "1687         new_other = (CudaNdarray*) CudaNdarray_New();\n",
      "1688         if(!new_other)\n",
      "1689         {\n",
      "1690             return -1;\n",
      "1691         }\n",
      "1692         if(CudaNdarray_CopyFromArray(new_other, (PyArrayObject *) py_other))\n",
      "1693         {\n",
      "1694             Py_XDECREF(new_other);\n",
      "1695             return -1;\n",
      "1696         }\n",
      "1697         py_other = (PyObject *) new_other;\n",
      "1698     }\n",
      "1699 \n",
      "1700     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1701     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1702 \n",
      "1703     if (verbose)\n",
      "1704     {\n",
      "1705         fprintf(stderr,\n",
      "1706             \"INPLACE ADD/DIV for self->nd=%d other->nd=%d\\n\",\n",
      "1707             self->nd, other->nd);\n",
      "1708     }\n",
      "1709 \n",
      "1710     //standard elemwise nb dim checks\n",
      "1711     if (self->nd < other->nd)\n",
      "1712     {\n",
      "1713         PyErr_Format(\n",
      "1714             PyExc_TypeError,\n",
      "1715             \"CudaNdarray_inplace_elemwise: The destination need more or the\"\n",
      "1716             \" same number of dimensions then the source. Got %d and %d.\",\n",
      "1717             self->nd, other->nd);\n",
      "1718         Py_XDECREF(new_other);\n",
      "1719         return -1;\n",
      "1720     }\n",
      "1721 \n",
      "1722     //broadcast to the same number of dimensions.\n",
      "1723     int* other_dims = (int*) alloca(self->nd * sizeof(int));\n",
      "1724     int* other_strides = (int*) alloca(self->nd * sizeof(int));\n",
      "1725     int added_dims = self->nd - other->nd;\n",
      "1726     // Add the added broadcasted dimensions\n",
      "1727     for (int i = 0; i< added_dims; ++i)\n",
      "1728     {\n",
      "1729         other_dims[i] = 1;\n",
      "1730         other_strides[i] = 0;\n",
      "1731     }\n",
      "1732     // Copy the existing dimensions\n",
      "1733     for (int i = 0; i< other->nd; ++i)\n",
      "1734     {\n",
      "1735         other_dims[i+added_dims] = CudaNdarray_HOST_DIMS(other)[i];\n",
      "1736         other_strides[i+added_dims] = CudaNdarray_HOST_STRIDES(other)[i];\n",
      "1737     }\n",
      "1738 \n",
      "1739     //standard elemwise dim checks\n",
      "1740     unsigned int size = 1;\n",
      "1741     for (int i = 0; i< self->nd; ++i)\n",
      "1742     {\n",
      "1743         if ((CudaNdarray_HOST_DIMS(self)[i] != other_dims[i])\n",
      "1744             && (other_dims[i] != 1))\n",
      "1745         {\n",
      "1746             PyErr_SetString(\n",
      "1747                 PyExc_ValueError,\n",
      "1748                 \"CudaNdarray_inplace_elemwise need same dimensions (or broadcastable dimension)\");\n",
      "1749             Py_XDECREF(new_other);\n",
      "1750             return -1;\n",
      "1751         }\n",
      "1752         // if we're broadcasting other, then make sure it has stride 0\n",
      "1753         assert ((CudaNdarray_HOST_DIMS(self)[i] == other_dims[i])\n",
      "1754             || (other_strides[i] == 0));\n",
      "1755         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1756     }\n",
      "1757 \n",
      "1758     if (size==0)\n",
      "1759     {\n",
      "1760         int other_size = CudaNdarray_SIZE((CudaNdarray *)py_other);\n",
      "1761         if (!(other_size == 0 || other_size == 1))\n",
      "1762         {\n",
      "1763             PyErr_SetString(\n",
      "1764                 PyExc_ValueError,\n",
      "1765                 \"CudaNdarray_inplace_elemwise cannot work inplace on\"\n",
      "1766                 \" un-initialized array when the new value have more than\"\n",
      "1767                 \" 0 or 1 broadcastable dimensions\");\n",
      "1768             Py_XDECREF(new_other);\n",
      "1769             return 0;\n",
      "1770         }\n",
      "1771         Py_XDECREF(new_other);\n",
      "1772         return 0;\n",
      "1773     }\n",
      "1774 \n",
      "1775     switch(self->nd)\n",
      "1776     {\n",
      "1777         case 0:\n",
      "1778             {\n",
      "1779                 dim3 n_blocks(1, 1, 1);\n",
      "1780                 dim3 n_threads(1);\n",
      "1781                 k3<<<n_blocks, n_threads>>>(\n",
      "1782                         1, //d0\n",
      "1783                         1, //d1\n",
      "1784                         1, //d2\n",
      "1785                         CudaNdarray_DEV_DATA(self),\n",
      "1786                         1, //strides\n",
      "1787                         1,\n",
      "1788                         1,\n",
      "1789                         CudaNdarray_DEV_DATA(other),\n",
      "1790                         1, //strides\n",
      "1791                         1,\n",
      "1792                         1);\n",
      "1793                 CNDA_THREAD_SYNC;\n",
      "1794                 cudaError_t err = cudaGetLastError();\n",
      "1795                 if (cudaSuccess != err)\n",
      "1796                 {\n",
      "1797                     PyErr_Format(\n",
      "1798                         PyExc_RuntimeError,\n",
      "1799                         \"CudaNdarray_inplace_elemwise case0: Cuda error: %s: %s.\\n\",\n",
      "1800                         \"k3\",\n",
      "1801                         cudaGetErrorString(err));\n",
      "1802                     Py_XDECREF(new_other);\n",
      "1803                     return -1;\n",
      "1804                 }\n",
      "1805             }\n",
      "1806             break;\n",
      "1807         case 1:\n",
      "1808             {\n",
      "1809                 dim3 n_blocks(1, 1, 1);\n",
      "1810                 dim3 n_threads(\n",
      "1811                         std::min(\n",
      "1812                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1813                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1814                 k3<<<n_blocks, n_threads>>>(\n",
      "1815                         1, //dimensions\n",
      "1816                         1,\n",
      "1817                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1818                         CudaNdarray_DEV_DATA(self),\n",
      "1819                         1, //strides\n",
      "1820                         1,\n",
      "1821                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1822                         CudaNdarray_DEV_DATA(other),\n",
      "1823                         1, //strides\n",
      "1824                         1,\n",
      "1825                         other_strides[0]);\n",
      "1826                 CNDA_THREAD_SYNC;\n",
      "1827                 cudaError_t err = cudaGetLastError();\n",
      "1828                 if (cudaSuccess != err)\n",
      "1829                 {\n",
      "1830                     PyErr_Format(\n",
      "1831                         PyExc_RuntimeError,\n",
      "1832                         \"CudaNdarray_inplace_elemwise case1: Cuda error: %s: %s.\\n\",\n",
      "1833                         \"k3\",\n",
      "1834                         cudaGetErrorString(err));\n",
      "1835                     Py_XDECREF(new_other);\n",
      "1836                     return -1;\n",
      "1837                 }\n",
      "1838             }\n",
      "1839             break;\n",
      "1840         case 2:\n",
      "1841             {\n",
      "1842                 //TODO:  if both self and other are f-contiguous\n",
      "1843                 //       Then flip the block and thread dimensions\n",
      "1844                 //       to make contiguous reads & writes\n",
      "1845                 dim3 n_blocks(1,\n",
      "1846                         std::min(\n",
      "1847                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1848                             NUM_VECTOR_OP_BLOCKS));\n",
      "1849                 dim3 n_threads(\n",
      "1850                         std::min(\n",
      "1851                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1852                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1853                 k3<<<n_blocks, n_threads>>>(1,\n",
      "1854                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1855                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1856                         CudaNdarray_DEV_DATA(self),\n",
      "1857                         1,\n",
      "1858                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1859                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1860                         CudaNdarray_DEV_DATA(other),\n",
      "1861                         1,\n",
      "1862                         other_strides[0],\n",
      "1863                         other_strides[1]);\n",
      "1864                 CNDA_THREAD_SYNC;\n",
      "1865                 cudaError_t err = cudaGetLastError();\n",
      "1866                 if (cudaSuccess != err)\n",
      "1867                 {\n",
      "1868                     PyErr_Format(\n",
      "1869                         PyExc_RuntimeError,\n",
      "1870                         \"CudaNdarray_inplace_elemwise case2: Cuda error: %s: %s.\\n\",\n",
      "1871                         \"k3\",\n",
      "1872                         cudaGetErrorString(err));\n",
      "1873                     Py_XDECREF(new_other);\n",
      "1874                     return -1;\n",
      "1875                 }\n",
      "1876             }\n",
      "1877             break;\n",
      "1878         case 3:\n",
      "1879             {\n",
      "1880                 //TODO:  Dimshuffle so that at least one of the arrays\n",
      "1881                 //       has a contiguous dimension on the thread idx.\n",
      "1882                 dim3 n_blocks(\n",
      "1883                         std::min(\n",
      "1884                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1885                             NUM_VECTOR_OP_BLOCKS),\n",
      "1886                         CudaNdarray_HOST_DIMS(self)[1]);\n",
      "1887                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1888                     n_blocks.y /= 2;\n",
      "1889                 dim3 n_threads(\n",
      "1890                         std::min(\n",
      "1891                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1892                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1893                 k3<<<n_blocks, n_threads>>>(\n",
      "1894                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1895                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1896                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1897                         CudaNdarray_DEV_DATA(self),\n",
      "1898                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1899                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1900                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1901                         CudaNdarray_DEV_DATA(other),\n",
      "1902                         other_strides[0],\n",
      "1903                         other_strides[1],\n",
      "1904                         other_strides[2]);\n",
      "1905                 CNDA_THREAD_SYNC;\n",
      "1906                 cudaError_t err = cudaGetLastError();\n",
      "1907                 if (cudaSuccess != err)\n",
      "1908                 {\n",
      "1909                     PyErr_Format(\n",
      "1910                         PyExc_RuntimeError,\n",
      "1911                         \"CudaNdarray_inplace_elemwise case3: Cuda error: %s: %s.\\n\",\n",
      "1912                         \"k3\",\n",
      "1913                         cudaGetErrorString(err));\n",
      "1914                     Py_XDECREF(new_other);\n",
      "1915                     return -1;\n",
      "1916                 }\n",
      "1917             }\n",
      "1918             break;\n",
      "1919         case 4:\n",
      "1920             {\n",
      "1921                 dim3 n_blocks(\n",
      "1922                         std::min(\n",
      "1923                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1924                             NUM_VECTOR_OP_BLOCKS),\n",
      "1925                         CudaNdarray_HOST_DIMS(self)[1]\n",
      "1926                         );\n",
      "1927                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1928                     n_blocks.y /= 2;\n",
      "1929                 dim3 n_threads(\n",
      "1930                         std::min(\n",
      "1931                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1932                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1933                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1934                             );\n",
      "1935                 k4<<<n_blocks, n_threads>>>(\n",
      "1936                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1937                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1938                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1939                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "1940                         CudaNdarray_DEV_DATA(self),\n",
      "1941                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1942                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1943                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1944                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "1945                         CudaNdarray_DEV_DATA(other),\n",
      "1946                         other_strides[0],\n",
      "1947                         other_strides[1],\n",
      "1948                         other_strides[2],\n",
      "1949                         other_strides[3]);\n",
      "1950                 CNDA_THREAD_SYNC;\n",
      "1951                 cudaError_t err = cudaGetLastError();\n",
      "1952                 if (cudaSuccess != err)\n",
      "1953                 {\n",
      "1954                     PyErr_Format(\n",
      "1955                         PyExc_RuntimeError,\n",
      "1956                         \"CudaNdarray_inplace_elemwise case4: Cuda error: %s: %s.\\n\",\n",
      "1957                         \"k4\",\n",
      "1958                         cudaGetErrorString(err));\n",
      "1959                     Py_XDECREF(new_other);\n",
      "1960                     return -1;\n",
      "1961                 }\n",
      "1962             }\n",
      "1963             break;\n",
      "1964         case 5:\n",
      "1965             {\n",
      "1966                 dim3 n_blocks(\n",
      "1967                         std::min(\n",
      "1968                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1969                             NUM_VECTOR_OP_BLOCKS),\n",
      "1970                         CudaNdarray_HOST_DIMS(self)[2]);\n",
      "1971                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1972                     n_blocks.y /= 2;\n",
      "1973                 dim3 n_threads(\n",
      "1974                         std::min(\n",
      "1975                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "1976                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1977                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1978                     );\n",
      "1979                 for (int i = 0; i < CudaNdarray_HOST_DIMS(self)[0]; ++i)\n",
      "1980                 {\n",
      "1981                      k4<<<n_blocks, n_threads>>>(\n",
      "1982                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1983                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1984                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "1985                             CudaNdarray_HOST_DIMS(self)[4],\n",
      "1986                             CudaNdarray_DEV_DATA(self) + i * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1987                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1988                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1989                             CudaNdarray_HOST_STRIDES(self)[3],\n",
      "1990                             CudaNdarray_HOST_STRIDES(self)[4],\n",
      "1991                             CudaNdarray_DEV_DATA(other) + i * other_strides[0],\n",
      "1992                             other_strides[1],\n",
      "1993                             other_strides[2],\n",
      "1994                             other_strides[3],\n",
      "1995                             other_strides[4]);\n",
      "1996                     CNDA_THREAD_SYNC;\n",
      "1997                     cudaError_t err = cudaGetLastError();\n",
      "1998                     if( cudaSuccess != err)\n",
      "1999                     {\n",
      "2000                         PyErr_Format(\n",
      "2001                             PyExc_RuntimeError,\n",
      "2002                             \"CudaNdarray_inplace_elemwise case5: Cuda error: %s: %s. n_block=(%ld,%ld) n_threads=%ld\\n\",\n",
      "2003                             \"k5 with loop over k4\",\n",
      "2004                             cudaGetErrorString(err),\n",
      "2005                             (long) n_blocks.x, (long) n_blocks.y, (long) n_threads.x);\n",
      "2006                         Py_XDECREF(new_other);\n",
      "2007                         return -1;\n",
      "2008                     }\n",
      "2009                 }\n",
      "2010             }\n",
      "2011             break;\n",
      "2012         case 6:\n",
      "2013             {\n",
      "2014                 dim3 n_blocks(\n",
      "2015                         std::min(\n",
      "2016                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "2017                             NUM_VECTOR_OP_BLOCKS),\n",
      "2018                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2019                         CudaNdarray_HOST_DIMS(self)[2]\n",
      "2020                         );\n",
      "2021                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "2022                     n_blocks.y /= 2;\n",
      "2023                 // GTX285(compute capabilities 1.3) don't support n_blocks.z > 1\n",
      "2024                 // (compute capabilities 2.0) support 65535 for n_blocks.z\n",
      "2025                 //while (n_blocks.x * n_blocks.y * n_blocks.z > NUM_VECTOR_OP_BLOCKS)\n",
      "2026                 //    n_blocks.z /= 2;\n",
      "2027                 n_blocks.z = 1;\n",
      "2028                 dim3 n_threads(\n",
      "2029                         std::min(\n",
      "2030                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2031                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "2032                     //TODO: DON'T YOU NEED TO PUT DIMS[4] in here???\n",
      "2033                     //TODO: DON'T YOU NEED TO PUT DIMS[5] in here???\n",
      "2034                             );\n",
      "2035                 k6<<<n_blocks, n_threads>>>(\n",
      "2036                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "2037                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2038                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "2039                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "2040                         CudaNdarray_HOST_DIMS(self)[4],\n",
      "2041                         CudaNdarray_HOST_DIMS(self)[5],\n",
      "2042                         CudaNdarray_DEV_DATA(self),\n",
      "2043                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2044                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2045                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2046                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2047                         CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2048                         CudaNdarray_HOST_STRIDES(self)[5],\n",
      "2049                         CudaNdarray_DEV_DATA(other),\n",
      "2050                         other_strides[0],\n",
      "2051                         other_strides[1],\n",
      "2052                         other_strides[2],\n",
      "2053                         other_strides[3],\n",
      "2054                         other_strides[4],\n",
      "2055                         other_strides[5]);\n",
      "2056                 CNDA_THREAD_SYNC;\n",
      "2057                 cudaError_t err = cudaGetLastError();\n",
      "2058                 if (cudaSuccess != err)\n",
      "2059                 {\n",
      "2060                     PyErr_Format(\n",
      "2061                         PyExc_RuntimeError,\n",
      "2062                         \"CudaNdarray_inplace_elemwise case6: Cuda error: %s: %s. n_blocks=(%ld, %ld, %ld) n_threads=(%ld)\\n\",\n",
      "2063                         \"k6\",\n",
      "2064                         cudaGetErrorString(err),\n",
      "2065                         (long) n_blocks.x, (long) n_blocks.y, (long) n_blocks.z,\n",
      "2066                         (long) n_threads.x);\n",
      "2067                     Py_XDECREF(new_other);\n",
      "2068                     return -1;\n",
      "2069                 }\n",
      "2070             }\n",
      "2071             break;\n",
      "2072         default:\n",
      "2073         {\n",
      "2074             PyErr_Format(\n",
      "2075                 PyExc_NotImplementedError,\n",
      "2076                 \"inplace_elemwise w nd=%i\\n\",\n",
      "2077                 self->nd);\n",
      "2078             Py_XDECREF(new_other);\n",
      "2079             return -1;\n",
      "2080         }\n",
      "2081     }\n",
      "2082     if (verbose)\n",
      "2083         fprintf(stderr, \"INPLACE ADD/DIV end\\n\");\n",
      "2084     Py_XDECREF(new_other);\n",
      "2085     return 0;\n",
      "2086 }\n",
      "2087 \n",
      "2088 /*\n",
      "2089  * We need this inplace Add to support IncSubTensor\n",
      "2090  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2091  */\n",
      "2092 // Will be called by __iadd__ in Python\n",
      "2093 PyObject *\n",
      "2094 CudaNdarray_inplace_add(PyObject* py_self, PyObject * py_other)\n",
      "2095 {\n",
      "2096     if (CudaNdarray_inplace_elemwise(py_self, py_other, IADD))\n",
      "2097     {\n",
      "2098         return NULL;\n",
      "2099     }\n",
      "2100     Py_INCREF(py_self);\n",
      "2101     return py_self;\n",
      "2102 }\n",
      "2103 \n",
      "2104 /*\n",
      "2105  * We need this inplace div for cuda/tests/test_basic_ops.py:test_shared_options\n",
      "2106  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2107  */\n",
      "2108 // Will be called by __idiv__ in Python\n",
      "2109 static PyObject *\n",
      "2110 CudaNdarray_inplace_div(PyObject* py_self, PyObject * py_other)\n",
      "2111 {\n",
      "2112     if (CudaNdarray_inplace_elemwise(py_self, py_other, IDIV))\n",
      "2113     {\n",
      "2114         return NULL;\n",
      "2115     }\n",
      "2116     Py_INCREF(py_self);\n",
      "2117     return py_self;\n",
      "2118 }\n",
      "2119 \n",
      "2120 // The PyNumberMethods struct layout changed in a non-trivial way from 2 to 3.\n",
      "2121 #if PY_MAJOR_VERSION == 3\n",
      "2122 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2123 {\n",
      "2124     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2125     0,  //binaryfunc nb_subtract;\n",
      "2126     0,  //binaryfunc nb_multiply;\n",
      "2127     0,  //binaryfunc nb_remainder;\n",
      "2128     0,  //binaryfunc nb_divmod;\n",
      "2129     0,  //ternaryfunc nb_power;\n",
      "2130     0,  //unaryfunc nb_negative;\n",
      "2131     0,  //unaryfunc nb_positive;\n",
      "2132     0,  //unaryfunc nb_absolute;\n",
      "2133     0,  //inquiry nb_bool;\n",
      "2134     0,  //unaryfunc nb_invert;\n",
      "2135     0,  //binaryfunc nb_lshift;\n",
      "2136     0,  //binaryfunc nb_rshift;\n",
      "2137     0,  //binaryfunc nb_and;\n",
      "2138     0,  //binaryfunc nb_xor;\n",
      "2139     0,  //binaryfunc nb_or;\n",
      "2140     0,  //unaryfunc nb_int;\n",
      "2141     0,  //void *nb_reserved;\n",
      "2142     0,  //unaryfunc nb_float;\n",
      "2143 \n",
      "2144     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2145     0,  //binaryfunc nb_inplace_subtract;\n",
      "2146     0,  //binaryfunc nb_inplace_multiply;\n",
      "2147     0,  //binaryfunc nb_inplace_remainder;\n",
      "2148     0,  //ternaryfunc nb_inplace_power;\n",
      "2149     0,  //binaryfunc nb_inplace_lshift;\n",
      "2150     0,  //binaryfunc nb_inplace_rshift;\n",
      "2151     0,  //binaryfunc nb_inplace_and;\n",
      "2152     0,  //binaryfunc nb_inplace_xor;\n",
      "2153     0,  //binaryfunc nb_inplace_or;\n",
      "2154 \n",
      "2155     0,  //binaryfunc nb_floor_divide;\n",
      "2156     0,  //binaryfunc nb_true_divide;\n",
      "2157     0,  //binaryfunc nb_inplace_floor_divide;\n",
      "2158     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;        __idiv__\n",
      "2159 \n",
      "2160     0,  //unaryfunc nb_index\n",
      "2161 };\n",
      "2162 #else\n",
      "2163 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2164 {\n",
      "2165     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2166     0,  //binaryfunc nb_subtract;      __sub__\n",
      "2167     0,  //binaryfunc nb_multiply;      __mul__\n",
      "2168     0,  //binaryfunc nb_divide;        __div__\n",
      "2169     0,  //binaryfunc nb_remainder;     __mod__\n",
      "2170     0,  //binaryfunc nb_divmod;        __divmod__\n",
      "2171     0,  //ternaryfunc nb_power;        __pow__\n",
      "2172     0,  //unaryfunc nb_negative;       __neg__\n",
      "2173     0,  //unaryfunc nb_positive;       __pos__\n",
      "2174     0,  //unaryfunc nb_absolute;       __abs__\n",
      "2175     0,  //inquiry nb_nonzero;          __nonzero__     /* Used by PyObject_IsTrue */\n",
      "2176     0,  //unaryfunc nb_invert;         __invert__\n",
      "2177     0,  //binaryfunc nb_lshift;        __lshift__\n",
      "2178     0,  //binaryfunc nb_rshift;        __rshift__\n",
      "2179     0,  //binaryfunc nb_and;           __and__\n",
      "2180     0,  //binaryfunc nb_xor;           __xor__\n",
      "2181     0,  //binaryfunc nb_or;            __or__\n",
      "2182     0,  //coercion nb_coerce;          __coerce__     /* Used by the coerce() function */\n",
      "2183     0,  //unaryfunc nb_int;            __int__\n",
      "2184     0,  //unaryfunc nb_long;           __long__\n",
      "2185     0,  //unaryfunc nb_float;          __float__\n",
      "2186     0,  //unaryfunc nb_oct;            __oct__\n",
      "2187     0,  //unaryfunc nb_hex;            __hex__\n",
      "2188 \n",
      "2189     /* Added in release 2.0 */\n",
      "2190     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2191     0,  //binaryfunc nb_inplace_subtract;      __isub__\n",
      "2192     0,  //binaryfunc nb_inplace_multiply;      __imul__\n",
      "2193     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_divide;        __idiv__\n",
      "2194     0,  //binaryfunc nb_inplace_remainder;     __imod__\n",
      "2195     0,  //ternaryfunc nb_inplace_power;        __ipow__\n",
      "2196     0,  //binaryfunc nb_inplace_lshift;        __ilshift__\n",
      "2197     0,  //binaryfunc nb_inplace_rshift;        __irshift__\n",
      "2198     0,  //binaryfunc nb_inplace_and;           __iand__\n",
      "2199     0,  //binaryfunc nb_inplace_xor;           __ixor__\n",
      "2200     0,  //binaryfunc nb_inplace_or;            __ior__\n",
      "2201 \n",
      "2202     /* Added in release 2.2 */\n",
      "2203     0,  //binaryfunc nb_floor_divide;          __floordiv__\n",
      "2204     0,  //binaryfunc nb_true_divide;           __truediv__\n",
      "2205     0,  //binaryfunc nb_inplace_floor_divide;  __ifloordiv__\n",
      "2206     0,  //binaryfunc nb_inplace_true_divide;   __itruediv__\n",
      "2207 \n",
      "2208 #if PY_MINOR_VERSION > 4\n",
      "2209     /* Added in release 2.5 */\n",
      "2210     0  //unaryfunc nb_index;  __index__\n",
      "2211 #endif\n",
      "2212 };\n",
      "2213 #endif\n",
      "2214 \n",
      "2215 \n",
      "2216 /////////////////////\n",
      "2217 // Mapping protocol\n",
      "2218 /////////////////////\n",
      "2219 \n",
      "2220 // Will by called by __len__ in Python\n",
      "2221 static Py_ssize_t\n",
      "2222 CudaNdarray_len(PyObject * py_self)\n",
      "2223 {\n",
      "2224     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2225     if (self->nd <= 0)\n",
      "2226     {\n",
      "2227         return (Py_ssize_t) 0;\n",
      "2228     }\n",
      "2229     else\n",
      "2230     {\n",
      "2231         return (Py_ssize_t) CudaNdarray_HOST_DIMS(self)[0];\n",
      "2232     }\n",
      "2233 }\n",
      "2234 \n",
      "2235 // Will by called by __getitem__ in Python\n",
      "2236 PyObject *\n",
      "2237 CudaNdarray_Subscript(PyObject * py_self, PyObject * key)\n",
      "2238 {\n",
      "2239     int verbose = 0;\n",
      "2240     if (verbose) fprintf(stderr, \"Subscript .... \\n\");\n",
      "2241     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2242     PyObject * py_rval = NULL;\n",
      "2243     CudaNdarray * rval = NULL;\n",
      "2244     PyObject * intobj = NULL;\n",
      "2245 \n",
      "2246     //PyObject_Print(key, stderr, 0);\n",
      "2247 \n",
      "2248     if (key == Py_Ellipsis)\n",
      "2249     {\n",
      "2250         Py_INCREF(py_self);\n",
      "2251         return py_self;\n",
      "2252     }\n",
      "2253     if ((intobj=PyNumber_Int(key))) //INDEXING BY INTEGER\n",
      "2254     //else if (PyInt_Check(key)) //INDEXING BY INTEGER\n",
      "2255     {\n",
      "2256         int d_idx = PyInt_AsLong(intobj);\n",
      "2257         Py_DECREF(intobj); intobj=NULL;\n",
      "2258         //int d_idx = PyInt_AsLong(key);\n",
      "2259         if (self->nd == 0)\n",
      "2260         {\n",
      "2261             PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed\");\n",
      "2262             return NULL;\n",
      "2263         }\n",
      "2264         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2265         int offset = 0;\n",
      "2266 \n",
      "2267         if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2268         {\n",
      "2269             //normal indexing\n",
      "2270             offset += d_idx * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2271         }\n",
      "2272         else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2273         {\n",
      "2274             //end-based indexing\n",
      "2275             // d_idx is negative\n",
      "2276             offset += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2277         }\n",
      "2278         else\n",
      "2279         {\n",
      "2280             PyErr_SetString(PyExc_IndexError, \"index out of bounds\");\n",
      "2281             return NULL;\n",
      "2282         }\n",
      "2283 \n",
      "2284         //allocate our subtensor view\n",
      "2285         py_rval = CudaNdarray_new_nd(self->nd - 1);\n",
      "2286         rval = (CudaNdarray*) py_rval;\n",
      "2287         if (!rval) return NULL;\n",
      "2288         assert (0 == rval->data_allocated);\n",
      "2289 \n",
      "2290         //initialize the view's data pointer to our own.\n",
      "2291         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self) + offset, self))\n",
      "2292         {\n",
      "2293             Py_DECREF(rval);\n",
      "2294             return NULL;\n",
      "2295         }\n",
      "2296         for (int d = 1; d < self->nd; ++d)\n",
      "2297         {\n",
      "2298             CudaNdarray_set_stride(rval, d-1, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2299             CudaNdarray_set_dim(rval, d-1, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2300         }\n",
      "2301     }\n",
      "2302     else\n",
      "2303     {\n",
      "2304         PyErr_Clear();\n",
      "2305     }\n",
      "2306     if (PySlice_Check(key)) //INDEXING BY SLICE\n",
      "2307     {\n",
      "2308         if (verbose) fprintf(stderr, \"by slice\\n\");\n",
      "2309         if (self->nd == 0)\n",
      "2310         {\n",
      "2311             PyErr_SetString(PyExc_ValueError, \"cannot slice a 0-d array\");\n",
      "2312             return NULL;\n",
      "2313         }\n",
      "2314 \n",
      "2315         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2316         Py_ssize_t start, stop, step, slen;\n",
      "2317         if (PySlice_GetIndicesEx(SLICE_CAST(key), d_dim, &start, &stop, &step, &slen))\n",
      "2318         {\n",
      "2319             if (verbose)\n",
      "2320                 fprintf(stderr, \"PySlice_GetIndicesEx failed\\n\");\n",
      "2321             return NULL;\n",
      "2322         }\n",
      "2323         if (verbose)\n",
      "2324         {\n",
      "2325             std::cerr << \"start \" << start << \"\\n\";\n",
      "2326             std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2327             std::cerr << \"step \" << step << \"\\n\";\n",
      "2328             std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2329         }\n",
      "2330 \n",
      "2331         //allocate our subtensor view\n",
      "2332         py_rval = CudaNdarray_new_nd(self->nd);\n",
      "2333         rval = (CudaNdarray*) py_rval;\n",
      "2334         if (!rval) return NULL;\n",
      "2335         assert (0 == rval->data_allocated);\n",
      "2336 \n",
      "2337 \n",
      "2338         //initialize the view's data pointer to our own.\n",
      "2339         if (CudaNdarray_set_device_data(rval,\n",
      "2340                     CudaNdarray_DEV_DATA(self) + start * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2341                     self))\n",
      "2342         {\n",
      "2343             Py_DECREF(rval);\n",
      "2344             return NULL;\n",
      "2345         }\n",
      "2346         //initialize dimension 0 of rval\n",
      "2347         CudaNdarray_set_stride(rval, 0,\n",
      "2348                 (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "2349         CudaNdarray_set_dim(rval, 0, slen);\n",
      "2350         if (verbose) std::cerr << \"rval stride \" << CudaNdarray_HOST_STRIDES(rval)[0] << \"\\n\";\n",
      "2351         // initialize dimensions > 0 of rval\n",
      "2352         for (int d = 1; d < self->nd; ++d)\n",
      "2353         {\n",
      "2354             CudaNdarray_set_stride(rval, d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2355             CudaNdarray_set_dim(rval, d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2356         }\n",
      "2357     }\n",
      "2358     if (PyTuple_Check(key)) //INDEXING BY TUPLE\n",
      "2359     {\n",
      "2360         if (verbose) fprintf(stderr, \"by tuple\\n\");\n",
      "2361         //elements of the tuple can be either integers or slices\n",
      "2362         //the dimensionality of the view we will return is diminished for each slice in the tuple\n",
      "2363 \n",
      "2364         if (PyTuple_Size(key) > self->nd)\n",
      "2365         {\n",
      "2366             PyErr_SetString(PyExc_IndexError, \"index error\");\n",
      "2367             return NULL;\n",
      "2368         }\n",
      "2369 \n",
      "2370         //calculate the number of dimensions in the return value\n",
      "2371         int rval_nd = self->nd;\n",
      "2372         for (int d = 0; d < PyTuple_Size(key); ++d)\n",
      "2373         {\n",
      "2374             //On some paltform PyInt_Check(<type 'numpy.int64'>) return true, other it return false.\n",
      "2375             //So we use PyArray_IsAnyScalar that should covert everything.\n",
      "2376             rval_nd -= PyArray_IsAnyScalar(PyTuple_GetItem(key, d));\n",
      "2377         }\n",
      "2378 \n",
      "2379         //allocate our subtensor view\n",
      "2380         py_rval = CudaNdarray_new_nd(rval_nd);\n",
      "2381         rval = (CudaNdarray*) py_rval;\n",
      "2382         if (!rval) return NULL;\n",
      "2383         assert (0 == rval->data_allocated);\n",
      "2384 \n",
      "2385         //initialize the view's data pointer to our own.\n",
      "2386         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "2387         {\n",
      "2388             Py_DECREF(rval);\n",
      "2389             return NULL;\n",
      "2390         }\n",
      "2391 \n",
      "2392         // rval_d will refer to the current dimension in the rval.\n",
      "2393         // It will not be incremented for integer keys, but will be incremented for slice\n",
      "2394         // keys\n",
      "2395         int rval_d = 0;\n",
      "2396 \n",
      "2397         for (int d = 0; d < self->nd; ++d)\n",
      "2398         {\n",
      "2399             // keys can be shorter than self->nd.\n",
      "2400             // when that happens, it means that the remaining dimensions are \"full slices\"\n",
      "2401             if (d >=PyTuple_Size(key))\n",
      "2402             {\n",
      "2403                 CudaNdarray_set_stride(rval, rval_d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2404                 CudaNdarray_set_dim(rval, rval_d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2405                 ++rval_d;\n",
      "2406             }\n",
      "2407             else\n",
      "2408             {\n",
      "2409                 PyObject * key_d = PyTuple_GetItem(key, d);\n",
      "2410 \n",
      "2411                 if (PySlice_Check(key_d))\n",
      "2412                 {\n",
      "2413                     Py_ssize_t start, stop, step, slen;\n",
      "2414                     if (PySlice_GetIndicesEx(SLICE_CAST(key_d), CudaNdarray_HOST_DIMS(self)[d], &start, &stop, &step, &slen))\n",
      "2415                     {\n",
      "2416                         Py_DECREF(rval);\n",
      "2417                         return NULL;\n",
      "2418                     }\n",
      "2419                     rval->devdata += start * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2420                     CudaNdarray_set_stride(rval, rval_d,\n",
      "2421                             (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2422                     CudaNdarray_set_dim(rval, rval_d, slen);\n",
      "2423                     if (0)\n",
      "2424                     {\n",
      "2425                         std::cerr << \"start \" << start << \"\\n\";\n",
      "2426                         std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2427                         std::cerr << \"step \" << step << \"\\n\";\n",
      "2428                         std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2429                     }\n",
      "2430                     ++rval_d;\n",
      "2431                 }\n",
      "2432                 else if ((intobj=PyNumber_Int(key_d)))\n",
      "2433                 {\n",
      "2434                     assert(PyArray_IsAnyScalar(key_d));\n",
      "2435                     int d_idx = PyInt_AsLong(intobj);\n",
      "2436                     Py_DECREF(intobj);\n",
      "2437                     intobj = NULL;\n",
      "2438                     int d_dim = CudaNdarray_HOST_DIMS(self)[d];\n",
      "2439 \n",
      "2440                     if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2441                     {\n",
      "2442                         //normal indexing\n",
      "2443                         rval->devdata += d_idx * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2444                     }\n",
      "2445                     else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2446                     {\n",
      "2447                         //end-based indexing\n",
      "2448                         rval->devdata += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2449                     }\n",
      "2450                     else\n",
      "2451                     {\n",
      "2452                         PyErr_SetString(PyExc_IndexError, \"index out of bounds\");\n",
      "2453                         Py_DECREF(rval);\n",
      "2454                         return NULL;\n",
      "2455                     }\n",
      "2456                 }\n",
      "2457                 else\n",
      "2458                 {\n",
      "2459                     PyErr_Clear(); // clear the error set by PyNumber_Int\n",
      "2460                     PyErr_SetString(PyExc_IndexError, \"index must be either int or slice\");\n",
      "2461                     Py_DECREF(rval);\n",
      "2462                     return NULL;\n",
      "2463                 }\n",
      "2464             }\n",
      "2465         }\n",
      "2466     }\n",
      "2467     if (py_rval)\n",
      "2468     {\n",
      "2469         if (verbose) fprint_CudaNdarray(stderr, self);\n",
      "2470         if (verbose) fprint_CudaNdarray(stderr, rval);\n",
      "2471     }\n",
      "2472     else\n",
      "2473     {\n",
      "2474         PyErr_SetString(PyExc_NotImplementedError, \"Unknown key type\");\n",
      "2475         return NULL;\n",
      "2476     }\n",
      "2477     return py_rval;\n",
      "2478 }\n",
      "2479 \n",
      "2480 // Will by called by __setitem__ in Python\n",
      "2481 // See http://docs.python.org/dev/py3k/c-api/object.html#PyObject_SetItem\n",
      "2482 // Doesn't handle broadcasting, e.g. a[:] = 5\n",
      "2483 // Can only be assigned from a CudaNdarray on the right side\n",
      "2484 // Or a ndarray\n",
      "2485 // Or a python scalar with value 0 when the left side part is c contiguous.\n",
      "2486 static int\n",
      "2487 CudaNdarray_setitem(PyObject *o, PyObject  *key, PyObject  *value)\n",
      "2488 {\n",
      "2489     int verbose = 0;\n",
      "2490     if (verbose) fprintf(stderr, \"CudaNdarray_setitem start\\n\");\n",
      "2491     // We try to copy directly into this CudaNdarray from the ndarray\n",
      "2492     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_Subscript(o, key);\n",
      "2493     CudaNdarray* new_value = NULL;\n",
      "2494 \n",
      "2495     if(!rval){\n",
      "2496         // CudaNdarray_Subscript failed and set the error msg.\n",
      "2497         Py_XDECREF(rval);\n",
      "2498         return -1;\n",
      "2499     }\n",
      "2500 \n",
      "2501     if(rval != (CudaNdarray*)o &&\n",
      "2502                 (rval->data_allocated ||\n",
      "2503                  // The new array should have a base\n",
      "2504                  !(((CudaNdarray*)rval)->base) ||\n",
      "2505                  // If the original array has no base, the base of the new\n",
      "2506                  // array should be the original one\n",
      "2507                  (!((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != o) ||\n",
      "2508                  // Else, the two arrays should have the same base\n",
      "2509                  (((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != ((CudaNdarray*)o)->base)))\n",
      "2510     {\n",
      "2511         // This case shouldn't happen, based on what I see in Subscript\n",
      "2512         // but just in case it happens sometime in the future\n",
      "2513 \n",
      "2514         PyErr_Format(PyExc_RuntimeError,\n",
      "2515                      \"__getitem__ must return a CudaNdarray that refers to\"\n",
      "2516                      \" the original CudaNdarray, not a copy. rval.base=%p\"\n",
      "2517                      \" o.base=%p o=%p\",\n",
      "2518                      (((CudaNdarray*)rval)->base), ((CudaNdarray*)o)->base, o);\n",
      "2519         Py_DECREF(rval);\n",
      "2520         return -1;\n",
      "2521     }\n",
      "2522 \n",
      "2523     PyObject * intobj = NULL;\n",
      "2524     if (CudaNdarray_Check(o)  && PyArray_Check(value)){\n",
      "2525         if (verbose)\n",
      "2526             fprintf(stderr,\n",
      "2527                     \"CudaNdarray_setitem dest is a CudaNdarray and\"\n",
      "2528                     \" value is a ndarray\\n\");\n",
      "2529         new_value = (CudaNdarray*) CudaNdarray_New();\n",
      "2530         if(!new_value)\n",
      "2531         {\n",
      "2532             return -1;\n",
      "2533         }\n",
      "2534         if (CudaNdarray_CopyFromArray(new_value, (PyArrayObject *) value))\n",
      "2535         {\n",
      "2536             Py_XDECREF(new_value);\n",
      "2537             Py_XDECREF(rval);\n",
      "2538             return -1;\n",
      "2539         }\n",
      "2540         value = (PyObject *) new_value;\n",
      "2541     }\n",
      "2542     else if ((intobj=PyNumber_Int(value)))\n",
      "2543     {\n",
      "2544         if (verbose)\n",
      "2545             fprintf(stderr,\n",
      "2546                     \"CudaNdarray_setitem dest and value is a python number\\n\");\n",
      "2547         if(! CudaNdarray_is_c_contiguous(rval)){\n",
      "2548             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2549                  \"CudaNdarray.__setitem__: When the new value is a scalar\"\n",
      "2550                  \" of value 0 the part where we copy to must be c contiguous.\");\n",
      "2551             Py_XDECREF(rval);\n",
      "2552             return -1;\n",
      "2553         }\n",
      "2554 \n",
      "2555         long val = PyInt_AsLong(intobj);\n",
      "2556         Py_DECREF(intobj); intobj=NULL;\n",
      "2557         if (val == 0)\n",
      "2558         {\n",
      "2559             cudaError_t err = cudaMemset(rval->devdata, 0,\n",
      "2560                                          CudaNdarray_SIZE(rval) * sizeof(real));\n",
      "2561             Py_XDECREF(rval);\n",
      "2562             if (err)\n",
      "2563             {\n",
      "2564                 // Clear the error flag, cudaMemset doesn't do it.\n",
      "2565                 // Currently this returns the same thing as err, but if in future\n",
      "2566                 // it returns something else I still don't see why we should ignore\n",
      "2567                 // it.  All we want to do here is reset the flag.\n",
      "2568                 cudaGetLastError();\n",
      "2569                 PyErr_SetString(PyExc_RuntimeError,\n",
      "2570                                 \"CudaNdarray.__setitem__: cudaMemset failed\");\n",
      "2571                 return -1;\n",
      "2572             }\n",
      "2573             return 0;\n",
      "2574         } else {\n",
      "2575             Py_XDECREF(rval);\n",
      "2576             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2577                   \"CudaNdarray.__setitem__: we support setting only python\"\n",
      "2578                   \" scalar of value 0, numpy nd array and CudaNdarray.\");\n",
      "2579                 return -1;\n",
      "2580         }\n",
      "2581     }\n",
      "2582 \n",
      "2583     PyErr_Clear(); // clear PyNumber_Int error.\n",
      "2584 \n",
      "2585     if(!CudaNdarray_Check(o) || !CudaNdarray_Check(value))\n",
      "2586     {\n",
      "2587         PyErr_SetString(PyExc_TypeError,\n",
      "2588           \"CudaNdarray.__setitem__: left must be a CudaNdarrays and right\"\n",
      "2589           \" must be a CudaNdarrays, an ndarray or a python scalar of value 0.\");\n",
      "2590         Py_XDECREF(new_value);\n",
      "2591         return -1;\n",
      "2592     }\n",
      "2593 \n",
      "2594     if (verbose)\n",
      "2595         fprintf(stderr, \"CudaNdarray_setitem dest and value are CudaNdarray\\n\");\n",
      "2596 \n",
      "2597     if (cnda_copy_structure_to_device(rval))\n",
      "2598     {\n",
      "2599         PyErr_SetString(PyExc_RuntimeError,\n",
      "2600                 \"CudaNdarray.__setitem__: syncing structure to device failed\");\n",
      "2601         Py_DECREF(rval);\n",
      "2602         Py_XDECREF(new_value);\n",
      "2603 \n",
      "2604         if (verbose)\n",
      "2605             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2606         return -1;\n",
      "2607     }\n",
      "2608 \n",
      "2609     PyObject *baseSavedForComparison = rval->base;\n",
      "2610 \n",
      "2611     if (CudaNdarray_CopyFromCudaNdarray(rval, (CudaNdarray*)value, true))\n",
      "2612     {\n",
      "2613         Py_DECREF((PyObject*)rval);\n",
      "2614         Py_XDECREF(new_value);\n",
      "2615 \n",
      "2616         if (verbose)\n",
      "2617             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2618         return -1;\n",
      "2619     }\n",
      "2620 \n",
      "2621     assert (rval->base == baseSavedForComparison);\n",
      "2622     assert (rval->dev_structure_fresh);\n",
      "2623 \n",
      "2624     // Clean up locally-created references\n",
      "2625     Py_DECREF(rval);\n",
      "2626     Py_XDECREF(new_value);\n",
      "2627 \n",
      "2628     return 0;\n",
      "2629 }\n",
      "2630 \n",
      "2631 \n",
      "2632 PyMappingMethods CudaNdarrayMappingMethods = {\n",
      "2633     CudaNdarray_len, //lenfunc mp_length;               __len__\n",
      "2634     CudaNdarray_Subscript, //binaryfunc mp_subscript;   __getitem__\n",
      "2635     CudaNdarray_setitem //objobjargproc mp_ass_subscript;                __setitem__\n",
      "2636 };\n",
      "2637 \n",
      "2638 ////////////////////\n",
      "2639 //\n",
      "2640 ////////////////////\n",
      "2641 \n",
      "2642 static PyObject *\n",
      "2643 CudaNdarray_get_shape(CudaNdarray *self, void *closure)\n",
      "2644 {\n",
      "2645     if (self->nd < 0)\n",
      "2646     {\n",
      "2647         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2648         return NULL;\n",
      "2649     }\n",
      "2650     PyObject * rval = PyTuple_New(self->nd);\n",
      "2651     for (int i = 0; i < self->nd; ++i)\n",
      "2652     {\n",
      "2653         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_DIMS(self)[i])))\n",
      "2654         {\n",
      "2655             Py_XDECREF(rval);\n",
      "2656             return NULL;\n",
      "2657         }\n",
      "2658 \n",
      "2659     }\n",
      "2660     return rval;\n",
      "2661 }\n",
      "2662 \n",
      "2663 static int\n",
      "2664 CudaNdarray_set_shape(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2665 {\n",
      "2666     PyErr_SetString(PyExc_NotImplementedError, \"TODO: call reshape\");\n",
      "2667     return -1;\n",
      "2668 }\n",
      "2669 \n",
      "2670 static PyObject *\n",
      "2671 CudaNdarray_get_strides(CudaNdarray *self, void *closure)\n",
      "2672 {\n",
      "2673     if (self->nd < 0)\n",
      "2674     {\n",
      "2675         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2676         return NULL;\n",
      "2677     }\n",
      "2678     PyObject * rval = PyTuple_New(self->nd);\n",
      "2679     for (int i = 0; i < self->nd; ++i)\n",
      "2680     {\n",
      "2681         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_STRIDES(self)[i])))\n",
      "2682         {\n",
      "2683             Py_XDECREF(rval);\n",
      "2684             return NULL;\n",
      "2685         }\n",
      "2686 \n",
      "2687     }\n",
      "2688     return rval;\n",
      "2689 }\n",
      "2690 \n",
      "2691 static int\n",
      "2692 CudaNdarray_set_strides(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2693 {\n",
      "2694     //npy_intp newstrides_bytes[PyTuple_Size(value)];\n",
      "2695     if (PyTuple_Check(value)){\n",
      "2696         if (PyTuple_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2697             PyErr_SetString(PyExc_ValueError,\n",
      "2698                             \"The new strides tuple must have the same length\"\n",
      "2699                             \" as the number of dimensions\");\n",
      "2700             return -1;\n",
      "2701         }\n",
      "2702     }else if (PyList_Check(value)){\n",
      "2703         if (PyList_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2704             PyErr_SetString(PyExc_ValueError,\n",
      "2705                             \"The new strides list must have the same length\"\n",
      "2706                             \" as the number of dimensions\");\n",
      "2707             return -1;\n",
      "2708         }\n",
      "2709     }else{\n",
      "2710         PyErr_SetString(PyExc_ValueError,\n",
      "2711                         \"The new strides need to be encoded in a tuple or list\");\n",
      "2712         return -1;\n",
      "2713     }\n",
      "2714     npy_intp* newstrides = (npy_intp*) alloca(CudaNdarray_NDIM(self) * sizeof(npy_intp));\n",
      "2715     if (PyTuple_Check(value)){\n",
      "2716         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2717             newstrides[i] = PyInt_AsLong(PyTuple_GetItem(value, Py_ssize_t(i)));\n",
      "2718             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2719         }\n",
      "2720     }else if (PyList_Check(value)){\n",
      "2721         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2722             newstrides[i] = PyInt_AsLong(PyList_GetItem(value, Py_ssize_t(i)));\n",
      "2723             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2724         }\n",
      "2725     }\n",
      "2726     /*\n",
      "2727     // Do not do this check, as ExtractDiag needs that, and NumPy does not seem\n",
      "2728     // to do it.\n",
      "2729     npy_intp dims[PyTuple_Size(value)];\n",
      "2730     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2731         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "2732     }\n",
      "2733     if (!PyArray_CheckStrides(4,\n",
      "2734                               CudaNdarray_NDIM(self),\n",
      "2735                               0, 0,\n",
      "2736                               dims,\n",
      "2737                               newstrides_bytes)){\n",
      "2738         PyErr_SetString(PyExc_ValueError, \"bad new strides\");\n",
      "2739         return -1;\n",
      "2740         }\n",
      "2741     */\n",
      "2742     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2743         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "2744     }\n",
      "2745     return 0;\n",
      "2746 }\n",
      "2747 \n",
      "2748 static PyObject *\n",
      "2749 CudaNdarray_get_dev_data(CudaNdarray *self, void *closure)\n",
      "2750 {\n",
      "2751     float * p =  CudaNdarray_DEV_DATA(self);\n",
      "2752     //printf(\"get_dev_data %p %li \\n\", p, (long int)p );\n",
      "2753     return PyInt_FromSize_t((size_t) CudaNdarray_DEV_DATA(self));\n",
      "2754 }\n",
      "2755 \n",
      "2756 static int\n",
      "2757 CudaNdarray_set_dev_data(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2758 {\n",
      "2759     Py_ssize_t newdevdata = PyInt_AsSsize_t(value);\n",
      "2760     //printf(\"set_dev_data %p %li \\n\",(float*)newdevdata ,newdevdata);\n",
      "2761     if (PyErr_Occurred())\n",
      "2762     {\n",
      "2763         return -1;\n",
      "2764     }\n",
      "2765     return  CudaNdarray_set_device_data(self, (float*)newdevdata, (CudaNdarray*)self->base);\n",
      "2766 }\n",
      "2767 \n",
      "2768 static PyObject *\n",
      "2769 CudaNdarray_get_dtype(CudaNdarray *self, void *closure)\n",
      "2770 {\n",
      "2771     return PyString_FromString(\"float32\");\n",
      "2772 }\n",
      "2773 \n",
      "2774 static PyObject *\n",
      "2775 CudaNdarray_get_ndim(CudaNdarray *self, void *closure)\n",
      "2776 {\n",
      "2777     return PyInt_FromLong(self->nd);\n",
      "2778 }\n",
      "2779 \n",
      "2780 static PyObject *\n",
      "2781 CudaNdarray_get_base(CudaNdarray *self, void *closure)\n",
      "2782 {\n",
      "2783     PyObject * base = self->base;\n",
      "2784     if (!base)\n",
      "2785     {\n",
      "2786         // We cannot return a NULL pointer, use None instead\n",
      "2787         base = Py_None;\n",
      "2788     }\n",
      "2789     Py_INCREF(base);\n",
      "2790     return base;\n",
      "2791 }\n",
      "2792 \n",
      "2793 void put_in_dict(PyObject * dict, const char * key, int val)\n",
      "2794 {\n",
      "2795   PyObject * k = PyString_FromString(key);\n",
      "2796   PyObject * v = PyInt_FromLong(val);\n",
      "2797   PyDict_SetItem(dict, k, v);\n",
      "2798   Py_DECREF(k);\n",
      "2799   Py_DECREF(v);\n",
      "2800 }\n",
      "2801 \n",
      "2802 PyObject *\n",
      "2803 GetDeviceProperties(PyObject* _unused, PyObject* args)\n",
      "2804 {\n",
      "2805   int dev_id = -1;\n",
      "2806   if (! PyArg_ParseTuple(args, \"i\", &dev_id))\n",
      "2807     return NULL;\n",
      "2808   cudaDeviceProp deviceProp;\n",
      "2809   cudaGetDeviceProperties(&deviceProp, dev_id);\n",
      "2810 \n",
      "2811   PyObject * dict = PyDict_New();\n",
      "2812   PyObject * str= PyString_FromString(\"name\");\n",
      "2813   PyObject * i = PyString_FromString(deviceProp.name);\n",
      "2814   PyDict_SetItem(dict, str, i);\n",
      "2815   Py_DECREF(str);\n",
      "2816   Py_DECREF(i);\n",
      "2817 \n",
      "2818   put_in_dict(dict, \"major\", deviceProp.major);\n",
      "2819   put_in_dict(dict, \"minor\", deviceProp.minor);\n",
      "2820 #if CUDART_VERSION >= 2020\n",
      "2821   int driverVersion = 0, runtimeVersion = 0;\n",
      "2822   cudaDriverGetVersion(&driverVersion);\n",
      "2823   cudaRuntimeGetVersion(&runtimeVersion);\n",
      "2824   put_in_dict(dict, \"driverVersion\", driverVersion);\n",
      "2825   put_in_dict(dict, \"runtimeVersion\", runtimeVersion);\n",
      "2826 #endif\n",
      "2827 #if CUDART_VERSION >= 2000\n",
      "2828 \n",
      "2829   put_in_dict(dict, \"multiProcessorCount\", deviceProp.multiProcessorCount);\n",
      "2830   //if ConvertSMVer2Cores is not defined in cuda_runtime_api.h, the run time is too old.\n",
      "2831   int sm_cores = -1;\n",
      "2832   if(deviceProp.major==1)\n",
      "2833     sm_cores = 32;\n",
      "2834   else if(deviceProp.major==2 && deviceProp.minor==0)\n",
      "2835     sm_cores = 32;\n",
      "2836   else if(deviceProp.major==2 && deviceProp.minor==1)\n",
      "2837     sm_cores = 48;\n",
      "2838   put_in_dict(dict, \"coresCount\", sm_cores * deviceProp.multiProcessorCount);\n",
      "2839 #endif\n",
      "2840   put_in_dict(dict, \"totalConstMem\", deviceProp.totalConstMem);\n",
      "2841   put_in_dict(dict, \"sharedMemPerBlock\", deviceProp.sharedMemPerBlock);\n",
      "2842   put_in_dict(dict, \"regsPerBlock\", deviceProp.regsPerBlock);\n",
      "2843   put_in_dict(dict, \"warpSize\", deviceProp.warpSize);\n",
      "2844   put_in_dict(dict, \"maxThreadsPerBlock\", deviceProp.maxThreadsPerBlock);\n",
      "2845   put_in_dict(dict, \"maxThreadsDim0\", deviceProp.maxThreadsDim[0]);\n",
      "2846   put_in_dict(dict, \"maxThreadsDim1\", deviceProp.maxThreadsDim[1]);\n",
      "2847   put_in_dict(dict, \"maxThreadsDim2\", deviceProp.maxThreadsDim[2]);\n",
      "2848   put_in_dict(dict, \"maxGridSize0\", deviceProp.maxGridSize[0]);\n",
      "2849   put_in_dict(dict, \"maxGridSize1\", deviceProp.maxGridSize[1]);\n",
      "2850   put_in_dict(dict, \"maxGridSize2\", deviceProp.maxGridSize[2]);\n",
      "2851   put_in_dict(dict, \"memPitch\", deviceProp.memPitch);\n",
      "2852   put_in_dict(dict, \"textureAlignment\", deviceProp.textureAlignment);\n",
      "2853   put_in_dict(dict, \"clockRate\", deviceProp.clockRate);\n",
      "2854 #if CUDART_VERSION >= 2000\n",
      "2855   put_in_dict(dict, \"deviceOverlap\", deviceProp.deviceOverlap);\n",
      "2856 #endif\n",
      "2857 #if CUDART_VERSION >= 2020\n",
      "2858   put_in_dict(dict, \"kernelExecTimeoutEnabled\", deviceProp.kernelExecTimeoutEnabled);\n",
      "2859   put_in_dict(dict, \"integrated\", deviceProp.integrated);\n",
      "2860   put_in_dict(dict, \"canMapHostMemory\", deviceProp.canMapHostMemory);\n",
      "2861   put_in_dict(dict, \"computeMode\", deviceProp.computeMode);\n",
      "2862   //in the doc of this fct tell that 0 - Normal mode, 1 - only 1 context, 2 - no context\n",
      "2863 #endif\n",
      "2864 #if CUDART_VERSION >= 3000\n",
      "2865   put_in_dict(dict, \"concurrentKernels\", deviceProp.concurrentKernels);\n",
      "2866 #endif\n",
      "2867 #if CUDART_VERSION >= 3010\n",
      "2868   put_in_dict(dict, \"ECCEnabled\", deviceProp.ECCEnabled);\n",
      "2869 #endif\n",
      "2870 #if CUDART_VERSION >= 3020\n",
      "2871   put_in_dict(dict, \"tccDriver\", deviceProp.tccDriver);\n",
      "2872 #endif\n",
      "2873 \n",
      "2874   return dict;\n",
      "2875 }\n",
      "2876 \n",
      "2877 /*\n",
      "2878  * Returns in *free and *total respectively, the free and total amount of memory available for allocation by the device in bytes.\n",
      "2879  */\n",
      "2880 PyObject *\n",
      "2881 GetDeviceMemInfo(PyObject* _unused, PyObject* dummy)\n",
      "2882 {\n",
      "2883     size_t free = 0, total = 0;\n",
      "2884     if(g_gpu_context_active == 0){\n",
      "2885         PyErr_Format(PyExc_RuntimeError, \"No gpu device selected yet. Please make sure the gpu device was initialized by Theano before.\");\n",
      "2886         return NULL;\n",
      "2887     }\n",
      "2888 \n",
      "2889     cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "2890     if (err != cudaSuccess){\n",
      "2891         // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "2892         // Currently this returns the same thing as err, but if in future\n",
      "2893         // it returns something else I still don't see why we should ignore\n",
      "2894         // it.  All we want to do here is reset the flag.\n",
      "2895         cudaGetLastError();\n",
      "2896         PyErr_Format(PyExc_RuntimeError,\n",
      "2897                      \"Error while getting memory info about the gpu: %s\",\n",
      "2898                      cudaGetErrorString(err));\n",
      "2899         return NULL;\n",
      "2900     }\n",
      "2901     return PyTuple_Pack(2, PyLong_FromLong(free), PyLong_FromLong(total));\n",
      "2902 }\n",
      "2903 \n",
      "2904 /*\n",
      "2905  * Synchronize with all the gpu device stream.\n",
      "2906  */\n",
      "2907 PyObject *\n",
      "2908 CudaNdarray_synchronize(PyObject* _unused, PyObject* dummy)\n",
      "2909 {\n",
      "2910     CNDA_BEGIN_ALLOW_THREADS\n",
      "2911     cudaThreadSynchronize();\n",
      "2912     CNDA_END_ALLOW_THREADS\n",
      "2913     Py_INCREF(Py_None);\n",
      "2914     return Py_None;\n",
      "2915 }\n",
      "2916 \n",
      "2917 /*\n",
      "2918  * Exist and return true if we link with cublas v2.\n",
      "2919  */\n",
      "2920 PyObject *\n",
      "2921 CudaNdarray_cublasv2(PyObject* _unused, PyObject* dummy)\n",
      "2922 {\n",
      "2923     Py_INCREF(Py_True);\n",
      "2924     return Py_True;\n",
      "2925 }\n",
      "2926 \n",
      "2927 #if COMPUTE_GPU_MEM_USED\n",
      "2928 /*\n",
      "2929  * Return the size in bytes that Theano currently have allocated on the gpu.\n",
      "2930  */\n",
      "2931 PyObject *\n",
      "2932 GetTheanoAllocInfo(PyObject* _unused, PyObject* dummy)\n",
      "2933 {\n",
      "2934     PyObject* a = PyLong_FromLong(_allocated_size);\n",
      "2935     PyObject* b = PyLong_FromLong(_max_allocated_size);\n",
      "2936 \n",
      "2937     PyObject* tuple = PyTuple_New(2);\n",
      "2938     PyTuple_SetItem(tuple, 0, a);\n",
      "2939     PyTuple_SetItem(tuple, 1, b);\n",
      "2940     return tuple;\n",
      "2941 }\n",
      "2942 #endif\n",
      "2943 \n",
      "2944 static PyGetSetDef CudaNdarray_getset[] = {\n",
      "2945     {\"shape\",\n",
      "2946         (getter)CudaNdarray_get_shape,\n",
      "2947         (setter)CudaNdarray_set_shape,\n",
      "2948         \"shape of this ndarray (tuple)\",\n",
      "2949         NULL},\n",
      "2950     {\"_strides\",\n",
      "2951         (getter)CudaNdarray_get_strides,\n",
      "2952         (setter)CudaNdarray_set_strides,\n",
      "2953         \"data pointer strides (in elements)\",\n",
      "2954         NULL},\n",
      "2955     {\"strides\",\n",
      "2956         (getter)CudaNdarray_get_strides,\n",
      "2957         (setter)CudaNdarray_set_strides,\n",
      "2958         \"data pointer strides (in elements)\",\n",
      "2959         NULL},\n",
      "2960     //gpudata is needed to allow calling pycuda fct with CudaNdarray input.\n",
      "2961     {\"gpudata\",\n",
      "2962         (getter)CudaNdarray_get_dev_data,\n",
      "2963         NULL,\n",
      "2964         \"device data pointer\",\n",
      "2965         NULL},\n",
      "2966     {\"_dev_data\",\n",
      "2967         (getter)CudaNdarray_get_dev_data,\n",
      "2968         (setter)CudaNdarray_set_dev_data,\n",
      "2969         \"device data pointer\",\n",
      "2970         NULL},\n",
      "2971     {\"dtype\",\n",
      "2972         (getter)CudaNdarray_get_dtype,\n",
      "2973         NULL,\n",
      "2974         \"The dtype of the element. Now always float32\",\n",
      "2975         NULL},\n",
      "2976     {\"size\",\n",
      "2977         (getter)CudaNdarray_SIZE_Object,\n",
      "2978         NULL,\n",
      "2979         \"The number of elements in this object.\",\n",
      "2980         NULL},\n",
      "2981     //mem_size is neede for pycuda.elementwise.ElementwiseKernel Why do they use size and mem_size of the same value?\n",
      "2982     {\"mem_size\",\n",
      "2983         (getter)CudaNdarray_SIZE_Object,\n",
      "2984         NULL,\n",
      "2985         \"The number of elements in this object.\",\n",
      "2986         NULL},\n",
      "2987     {\"ndim\",\n",
      "2988         (getter)CudaNdarray_get_ndim,\n",
      "2989         NULL,\n",
      "2990         \"The number of dimensions in this object.\",\n",
      "2991         NULL},\n",
      "2992     {\"base\",\n",
      "2993         (getter)CudaNdarray_get_base,\n",
      "2994         NULL,\n",
      "2995         \"If this ndarray is a view, base is the original ndarray.\",\n",
      "2996         NULL},\n",
      "2997 \n",
      "2998     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "2999 };\n",
      "3000 \n",
      "3001 static PyTypeObject CudaNdarrayType =\n",
      "3002 {\n",
      "3003 #if PY_MAJOR_VERSION >= 3\n",
      "3004     PyVarObject_HEAD_INIT(NULL, 0)\n",
      "3005 #else\n",
      "3006     PyObject_HEAD_INIT(NULL)\n",
      "3007     0,                         /*ob_size*/\n",
      "3008 #endif\n",
      "3009     \"CudaNdarray\",             /*tp_name*/\n",
      "3010     sizeof(CudaNdarray),       /*tp_basicsize*/\n",
      "3011     0,                         /*tp_itemsize*/\n",
      "3012     (destructor)CudaNdarray_dealloc, /*tp_dealloc*/\n",
      "3013     0,                         /*tp_print*/\n",
      "3014     0,                         /*tp_getattr*/\n",
      "3015     0,                         /*tp_setattr*/\n",
      "3016     0,                         /*tp_compare*/\n",
      "3017     0,                         /*tp_repr*/\n",
      "3018     &CudaNdarrayNumberMethods, /*tp_as_number*/\n",
      "3019     0,                         /*tp_as_sequence*/\n",
      "3020     &CudaNdarrayMappingMethods,/*tp_as_mapping*/\n",
      "3021     0,                         /*tp_hash */\n",
      "3022     0,                         /*tp_call*/\n",
      "3023     0,                         /*tp_str*/\n",
      "3024     0,                         /*tp_getattro*/\n",
      "3025     0,                         /*tp_setattro*/\n",
      "3026     0,                         /*tp_as_buffer*/\n",
      "3027 #if PY_MAJOR_VERSION >= 3\n",
      "3028     // Py_TPFLAGS_CHECKTYPES is always true and was removed in Python 3.\n",
      "3029     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /*tp_flags*/\n",
      "3030 #else\n",
      "3031     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_CHECKTYPES, /*tp_flags*/\n",
      "3032 #endif\n",
      "3033     \"CudaNdarray objects\",     /* tp_doc */\n",
      "3034     0,                         /* tp_traverse */\n",
      "3035     0,                         /* tp_clear */\n",
      "3036     0,                         /* tp_richcompare */\n",
      "3037     0,                         /* tp_weaklistoffset */\n",
      "3038     0,                         /* tp_iter */\n",
      "3039     0,                         /* tp_iternext */\n",
      "3040     CudaNdarray_methods,       /* tp_methods */\n",
      "3041     CudaNdarray_members,       /* tp_members */\n",
      "3042     CudaNdarray_getset,        /* tp_getset */\n",
      "3043     0,                         /* tp_base */\n",
      "3044     0,                         /* tp_dict */\n",
      "3045     0,                         /* tp_descr_get */\n",
      "3046     0,                         /* tp_descr_set */\n",
      "3047     0,                         /* tp_dictoffset */\n",
      "3048     (initproc)CudaNdarray_init,/* tp_init */\n",
      "3049     0,                         /* tp_alloc */\n",
      "3050     CudaNdarray_new,           /* tp_new */\n",
      "3051 };\n",
      "3052 \n",
      "3053 static __global__ void get_gpu_ptr_size(int* dst)\n",
      "3054 {\n",
      "3055     dst[0] = sizeof(float*);\n",
      "3056     dst[1] = sizeof(int);\n",
      "3057 }\n",
      "3058 \n",
      "3059 PyObject *\n",
      "3060 CudaNdarray_ptr_int_size(PyObject* _unused, PyObject* args)\n",
      "3061 {\n",
      "3062     int *gpu_data = (int*)device_malloc(sizeof(int)*2);\n",
      "3063     if(gpu_data == NULL){\n",
      "3064         return NULL;\n",
      "3065     }\n",
      "3066     get_gpu_ptr_size<<<1,1>>>(gpu_data);\n",
      "3067 \n",
      "3068     cudaError_t cudaErr = cudaGetLastError();\n",
      "3069     if (cudaSuccess != cudaErr){\n",
      "3070 \n",
      "3071         device_free(gpu_data);\n",
      "3072         return PyErr_Format(PyExc_RuntimeError,\n",
      "3073                             \"CudaNdarray_ptr_int_size: error when calling the gpu code. (%s)\",\n",
      "3074                             cudaGetErrorString(cudaErr));\n",
      "3075     }\n",
      "3076 \n",
      "3077     // Transfer the result to cpu\n",
      "3078     int gpu_sizes[] = {-1,-1};\n",
      "3079     cublasStatus_t err;\n",
      "3080     err = cublasGetVector(2, sizeof(int), gpu_data, 1, gpu_sizes, 1);\n",
      "3081     device_free(gpu_data);\n",
      "3082 \n",
      "3083     if (CUBLAS_STATUS_SUCCESS != err){\n",
      "3084         PyErr_SetString(PyExc_RuntimeError, \"error copying data to from memory\");\n",
      "3085         return NULL;\n",
      "3086     }\n",
      "3087     return Py_BuildValue(\"iiii\", (int) gpu_sizes[0], (int)sizeof(float*),\n",
      "3088                          (int)sizeof(int), (int) gpu_sizes[1]);\n",
      "3089 }\n",
      "3090 \n",
      "3091 static int cublas_init();\n",
      "3092 static void cublas_shutdown();\n",
      "3093 // Initialize the gpu.\n",
      "3094 // Takes one optional parameter, the device number.\n",
      "3095 // If provided, it sets that device to be the active device.\n",
      "3096 // If not provided (usually just to test whether the gpu is available at all),\n",
      "3097 // it does not set an active device.\n",
      "3098 // Raises EnvironmentError or ValueError (as appropriate) if the initialization failed.\n",
      "3099 PyObject *\n",
      "3100 CudaNdarray_gpu_init(PyObject* _unused, PyObject* args)\n",
      "3101 {\n",
      "3102     int card_nb = 0;\n",
      "3103     int card_number_provided = 1;\n",
      "3104 \n",
      "3105     PyArg_ParseTuple(args, \"|i\", &card_nb); // if we're given something wildly invalid, this will throw a TypeError\n",
      "3106 \n",
      "3107     if(PyTuple_Size(args) == 0) {\n",
      "3108         card_number_provided = 0;\n",
      "3109         card_nb = 0;\n",
      "3110     }\n",
      "3111 \n",
      "3112     int deviceCount;\n",
      "3113     cudaError err = cudaGetDeviceCount(&deviceCount);\n",
      "3114     if(cudaSuccess != err) {\n",
      "3115         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3116                             \"Unable to get the number of gpus available: %s\",\n",
      "3117                             cudaGetErrorString(cudaGetLastError()));\n",
      "3118     }\n",
      "3119 \n",
      "3120     // as soon as the first successful call to a cuda* function is made, a\n",
      "3121     // gpu context has been created\n",
      "3122     g_gpu_context_active = 1;\n",
      "3123 \n",
      "3124     if(deviceCount <= 0) {\n",
      "3125         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3126                             \"Can't use the GPU, no devices support CUDA\");\n",
      "3127     }\n",
      "3128     if(card_number_provided && (card_nb < 0 || card_nb > (deviceCount - 1))) {\n",
      "3129         return PyErr_Format(PyExc_ValueError,\n",
      "3130                             \"Bad device number %d. Only %d devices available.\",\n",
      "3131                             card_nb,\n",
      "3132                             deviceCount);\n",
      "3133     }\n",
      "3134 \n",
      "3135     cudaDeviceProp deviceProp;\n",
      "3136     err = cudaGetDeviceProperties(&deviceProp, card_nb);\n",
      "3137     if(cudaSuccess != err) {\n",
      "3138         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3139                             \"Unable to get properties of gpu %i: %s\",\n",
      "3140                             card_nb,\n",
      "3141                             cudaGetErrorString(cudaGetLastError()));\n",
      "3142     }\n",
      "3143 \n",
      "3144     if(deviceProp.major == 9999 && deviceProp.minor == 9999 ){\n",
      "3145         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3146                             \"There is no device that supports CUDA\");\n",
      "3147     }\n",
      "3148 \n",
      "3149     if(card_number_provided) {\n",
      "3150         err = cudaSetDevice(card_nb);\n",
      "3151         if(cudaSuccess != err) {\n",
      "3152             return PyErr_Format(PyExc_EnvironmentError,\n",
      "3153                                 \"Unable to set device %i: %s\",\n",
      "3154                                 card_nb,\n",
      "3155                                 cudaGetErrorString(cudaGetLastError()));\n",
      "3156         }\n",
      "3157         if (cublas_init() == -1)\n",
      "3158             return NULL;\n",
      "3159     }\n",
      "3160 \n",
      "3161     Py_INCREF(Py_None);\n",
      "3162     return Py_None;\n",
      "3163 }\n",
      "3164 \n",
      "3165 PyObject *\n",
      "3166 CudaNdarray_active_device_number(PyObject* _unused, PyObject* _unused_args) {\n",
      "3167     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3168     // really necessary.\n",
      "3169     int currentDevice;\n",
      "3170     cudaGetDevice(&currentDevice);\n",
      "3171     return PyInt_FromLong(currentDevice);\n",
      "3172 }\n",
      "3173 \n",
      "3174 PyObject *\n",
      "3175 CudaNdarray_active_device_name(PyObject* _unused, PyObject* _unused_args) {\n",
      "3176     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3177     // really necessary.\n",
      "3178     int currentDevice;\n",
      "3179     cudaGetDevice(&currentDevice);\n",
      "3180 \n",
      "3181     cudaDeviceProp deviceProp;\n",
      "3182     cudaGetDeviceProperties(&deviceProp, currentDevice);\n",
      "3183     return PyString_FromString(deviceProp.name);\n",
      "3184 }\n",
      "3185 \n",
      "3186 PyObject *\n",
      "3187 CudaNdarray_gpu_shutdown(PyObject* _unused, PyObject* _unused_args) {\n",
      "3188     // Don't handle errors here\n",
      "3189     cublas_shutdown();\n",
      "3190     cudaThreadExit();\n",
      "3191     g_gpu_context_active = 0; // context has now been closed down\n",
      "3192     Py_INCREF(Py_None);\n",
      "3193     return Py_None;\n",
      "3194 }\n",
      "3195 \n",
      "3196 /*\n",
      "3197  * This function is tested in theano/misc/test_pycuda_theano_simple.py\n",
      "3198  */\n",
      "3199 PyObject *\n",
      "3200 CudaNdarray_from_gpu_pointer(PyObject* _unused, PyObject* args)\n",
      "3201 {\n",
      "3202     int verbose = 0;\n",
      "3203     PyObject *gpu_ptr = NULL;\n",
      "3204     PyObject *shapes = NULL;\n",
      "3205     PyObject *strides = NULL;\n",
      "3206     PyObject *base = NULL;\n",
      "3207     PyObject *rval = NULL;\n",
      "3208 \n",
      "3209     //args should consist of 3 python objects\n",
      "3210     //The first is the gpu ptr\n",
      "3211     //The second if the shape\n",
      "3212     //The third if the strides\n",
      "3213     if (! PyArg_ParseTuple(args, \"OOOO\", &gpu_ptr, &shapes, &strides, &base))\n",
      "3214         return NULL;\n",
      "3215 \n",
      "3216     if (verbose) printf(\"In CudaNdarray_from_gpu_pointer\\n\");\n",
      "3217     if (!PyLong_Check(gpu_ptr))\n",
      "3218     {\n",
      "3219         PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: The gpu pointor is not an long\");\n",
      "3220         return NULL;\n",
      "3221     }\n",
      "3222 \n",
      "3223     Py_ssize_t nd =  PyObject_Length(shapes);\n",
      "3224     if (nd < 0)\n",
      "3225     {\n",
      "3226         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of second argument\");\n",
      "3227         return NULL;\n",
      "3228     }\n",
      "3229     Py_ssize_t nd_stride =  PyObject_Length(strides);\n",
      "3230     if (nd_stride < 0)\n",
      "3231     {\n",
      "3232         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of third argument\");\n",
      "3233         return NULL;\n",
      "3234     }\n",
      "3235 \n",
      "3236     if (nd != nd_stride)\n",
      "3237     {\n",
      "3238         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: We need the same number of shapes and strides\");\n",
      "3239         return NULL;\n",
      "3240     }\n",
      "3241 \n",
      "3242     rval = CudaNdarray_New();\n",
      "3243 \n",
      "3244     if (CudaNdarray_set_nd((CudaNdarray *)rval, nd))\n",
      "3245     {\n",
      "3246         //CudaNdarray_set_nd set the error msg\n",
      "3247         return NULL;\n",
      "3248     }\n",
      "3249     // set gpu pointeur\n",
      "3250     assert(((CudaNdarray *)rval)->data_allocated == 0);\n",
      "3251     if (CudaNdarray_set_device_data((CudaNdarray *)rval, (float *)PyInt_AsLong(gpu_ptr), base))\n",
      "3252     {\n",
      "3253         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Error while setting the gpu pointor\");\n",
      "3254         return NULL;\n",
      "3255 \n",
      "3256     }\n",
      "3257 \n",
      "3258     // Set dims and strides\n",
      "3259     for (int i = nd-1; i >= 0; --i)\n",
      "3260     {\n",
      "3261         PyObject * idx = PyLong_FromLong(i);\n",
      "3262         if (idx == NULL)\n",
      "3263         {\n",
      "3264             PyErr_SetString(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: Couldn't make long object to loop over list/tuple\");\n",
      "3265             return NULL;\n",
      "3266         }\n",
      "3267         PyObject* dim_ = PyObject_GetItem(shapes, idx);\n",
      "3268         PyObject* strd_ = PyObject_GetItem(strides, idx);\n",
      "3269         if (!PyInt_Check(dim_))\n",
      "3270         {\n",
      "3271             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: shapes[%d] is not an int\", i);\n",
      "3272             return NULL;\n",
      "3273         }\n",
      "3274         if (!PyInt_Check(strd_))\n",
      "3275         {\n",
      "3276             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: strides[%d] is not an int\", i);\n",
      "3277             return NULL;\n",
      "3278         }\n",
      "3279         int dim = PyInt_AsLong(dim_);\n",
      "3280         int strd = PyInt_AsLong(strd_);\n",
      "3281         CudaNdarray_set_stride((CudaNdarray *)rval, i, strd);\n",
      "3282         CudaNdarray_set_dim((CudaNdarray *)rval, i, dim);\n",
      "3283         Py_DECREF(idx);\n",
      "3284         Py_DECREF(dim_);\n",
      "3285         Py_DECREF(strd_);\n",
      "3286     }\n",
      "3287     if (verbose) printf(\"CudaNdarray_from_gpu_pointer normal return\\n\");\n",
      "3288     return rval;\n",
      "3289 }\n",
      "3290 \n",
      "3291 PyObject *\n",
      "3292 CudaNdarray_Dot(PyObject* _unused, PyObject* args)\n",
      "3293 {\n",
      "3294     PyObject *l=NULL;\n",
      "3295     PyObject *r=NULL;\n",
      "3296     PyObject * rval = NULL;\n",
      "3297 \n",
      "3298     //args should consist of two python objects (\"OO\")\n",
      "3299     if (! PyArg_ParseTuple(args, \"OO\", &l, &r))\n",
      "3300         return NULL;\n",
      "3301 \n",
      "3302     if (!CudaNdarray_Check(l) || !CudaNdarray_Check(r))\n",
      "3303     {\n",
      "3304         PyErr_SetString(PyExc_TypeError, \"CudaNdarray arguments required \");\n",
      "3305         goto CudaNdarray_dot_fail;\n",
      "3306     }\n",
      "3307     if (((CudaNdarray*)l)->nd != 2)\n",
      "3308     {\n",
      "3309         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3310         goto CudaNdarray_dot_fail;\n",
      "3311     }\n",
      "3312     if (((CudaNdarray*)r)->nd != 2)\n",
      "3313     {\n",
      "3314         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3315         goto CudaNdarray_dot_fail;\n",
      "3316     }\n",
      "3317     rval = CudaNdarray_New();\n",
      "3318     if (!rval)\n",
      "3319     {\n",
      "3320         goto CudaNdarray_dot_fail;\n",
      "3321     }\n",
      "3322     int dims[2];\n",
      "3323     dims[0] = CudaNdarray_HOST_DIMS((CudaNdarray*)l)[0];\n",
      "3324     dims[1] = CudaNdarray_HOST_DIMS((CudaNdarray*)r)[1];\n",
      "3325     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, 2, dims))\n",
      "3326     {\n",
      "3327         goto CudaNdarray_dot_fail;\n",
      "3328     }\n",
      "3329     if (CudaNdarray_gemm(1.0, (CudaNdarray*)l, (CudaNdarray*)r, 0.0, (CudaNdarray*)rval))\n",
      "3330     {\n",
      "3331         goto CudaNdarray_dot_fail;\n",
      "3332     }\n",
      "3333 \n",
      "3334     return rval;\n",
      "3335 \n",
      "3336     CudaNdarray_dot_fail:\n",
      "3337     Py_XDECREF(rval);\n",
      "3338     return NULL;\n",
      "3339 }\n",
      "3340 \n",
      "3341 static PyObject *\n",
      "3342 filter(PyObject* __unsed_self, PyObject *args) // args = (data, broadcastable, strict, storage)\n",
      "3343 {\n",
      "3344     /*\n",
      "3345      * TODO: DOC what this function should do in the various cases of\n",
      "3346      * What is 'strict' supposed to mean in the context of this function?\n",
      "3347      * What do we do with input that could be interpreted as matching the broadcastable pattern in strict vs. non-strict cases?\n",
      "3348      *\n",
      "3349      */\n",
      "3350     PyObject *py_data=NULL;\n",
      "3351     PyArrayObject * data = NULL;\n",
      "3352     int strict = 0;\n",
      "3353     PyObject * broadcastable=NULL;\n",
      "3354     PyObject * storage=NULL;\n",
      "3355     CudaNdarray * rval=NULL;\n",
      "3356 \n",
      "3357     //Python object references which are provided to the caller are borrowed references\n",
      "3358     if (!PyArg_ParseTuple(args, \"OOiO\", &py_data, &broadcastable, &strict, &storage)) return NULL;\n",
      "3359 \n",
      "3360     if (!PyTuple_Check(broadcastable)){\n",
      "3361         PyErr_SetString(PyExc_TypeError, \"broadcastable arg should be a tuple of int.\");\n",
      "3362         return NULL;\n",
      "3363     }\n",
      "3364     Py_INCREF(py_data);\n",
      "3365     Py_INCREF(broadcastable);\n",
      "3366 \n",
      "3367     CudaNdarray * cnda = (CudaNdarray*)py_data;\n",
      "3368 \n",
      "3369     if (strict || CudaNdarray_Check(py_data))\n",
      "3370     {\n",
      "3371         //TODO: support non-strict \"casting\" from a vt to the broadcastable/type/size that we need.\n",
      "3372         if (!CudaNdarray_Check(py_data))\n",
      "3373         {\n",
      "3374             Py_DECREF(py_data);\n",
      "3375             Py_DECREF(broadcastable);\n",
      "3376             PyErr_SetString(PyExc_TypeError, \"strict mode requires CudaNdarray\");\n",
      "3377             return NULL;\n",
      "3378         }\n",
      "3379         if (cnda->nd != PyTuple_Size(broadcastable))\n",
      "3380         {\n",
      "3381             Py_DECREF(py_data);\n",
      "3382             Py_DECREF(broadcastable);\n",
      "3383             PyErr_Format(PyExc_TypeError, \"Wrong rank: %i vs %li\", cnda->nd, (long)PyTuple_Size(broadcastable));\n",
      "3384             return NULL;\n",
      "3385         }\n",
      "3386         for (int i = 0; i < cnda->nd; ++i)\n",
      "3387         {\n",
      "3388             if ((CudaNdarray_HOST_DIMS(cnda)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3389             {\n",
      "3390                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable vt dimension %i\", i);\n",
      "3391                 Py_DECREF(py_data);\n",
      "3392                 Py_DECREF(broadcastable);\n",
      "3393                 return NULL;\n",
      "3394             }else if (CudaNdarray_HOST_DIMS(cnda)[i] == 1 && CudaNdarray_HOST_STRIDES(cnda)[i] != 0){\n",
      "3395                 PyErr_Format(PyExc_TypeError, \"Non-zeros strides(%d) on dimension %d of size 1\",\n",
      "3396                              CudaNdarray_HOST_STRIDES(cnda)[i], i);\n",
      "3397                 Py_DECREF(py_data);\n",
      "3398                 Py_DECREF(broadcastable);\n",
      "3399                 return NULL;\n",
      "3400             }\n",
      "3401         }\n",
      "3402         Py_DECREF(broadcastable);\n",
      "3403         return py_data;\n",
      "3404     }\n",
      "3405     else\n",
      "3406     {\n",
      "3407         data = (PyArrayObject*)PyArray_FromObject(py_data, REAL_TYPENUM, PyTuple_Size(broadcastable), PyTuple_Size(broadcastable));\n",
      "3408         if (!data)\n",
      "3409         {\n",
      "3410             //err message already defined\n",
      "3411             Py_DECREF(py_data);\n",
      "3412             Py_DECREF(broadcastable);\n",
      "3413             return NULL;\n",
      "3414         }\n",
      "3415         for (int i = 0; i < PyArray_NDIM(data); ++i)\n",
      "3416         {\n",
      "3417             if ((PyArray_DIMS(data)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3418             {\n",
      "3419                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable dimension %i\", i);\n",
      "3420                 Py_DECREF(data);\n",
      "3421                 Py_DECREF(py_data);\n",
      "3422                 Py_DECREF(broadcastable);\n",
      "3423                 return NULL;\n",
      "3424             }\n",
      "3425         }\n",
      "3426         if (storage && CudaNdarray_Check(storage))\n",
      "3427         {\n",
      "3428             rval = (CudaNdarray*) storage;\n",
      "3429             Py_INCREF(rval);\n",
      "3430         }\n",
      "3431         else\n",
      "3432         {\n",
      "3433             rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3434         }\n",
      "3435         if (rval)\n",
      "3436         {\n",
      "3437             if (CudaNdarray_CopyFromArray(rval, data))\n",
      "3438             {\n",
      "3439                 Py_DECREF(rval);\n",
      "3440                 rval = NULL;\n",
      "3441             }\n",
      "3442         }\n",
      "3443         Py_DECREF(data);\n",
      "3444         Py_DECREF(py_data);\n",
      "3445         Py_DECREF(broadcastable);\n",
      "3446         return (PyObject*)rval;\n",
      "3447     }\n",
      "3448 }\n",
      "3449 \n",
      "3450 //TODO-- CudaNdarray_Dot and CudaNdarray_active_device_name are following different capitalization conventions.\n",
      "3451 //       Pick one and standardize it, this file is already annoying enough to grep through\n",
      "3452 static PyMethodDef module_methods[] = {\n",
      "3453     {\"dimshuffle\", CudaNdarray_Dimshuffle, METH_VARARGS, \"Returns the dimshuffle of a CudaNdarray.\"},\n",
      "3454     {\"dot\", CudaNdarray_Dot, METH_VARARGS, \"Returns the matrix product of two CudaNdarray arguments.\"},\n",
      "3455     {\"gpu_init\", CudaNdarray_gpu_init, METH_VARARGS, \"Select the gpu card to use; also usable to test whether CUDA is available.\"},\n",
      "3456     {\"active_device_name\", CudaNdarray_active_device_name, METH_VARARGS, \"Get the name of the active device.\"},\n",
      "3457     {\"active_device_number\", CudaNdarray_active_device_number, METH_VARARGS, \"Get the number of the active device.\"},\n",
      "3458     {\"gpu_shutdown\", CudaNdarray_gpu_shutdown, METH_VARARGS, \"Shut down the gpu.\"},\n",
      "3459     {\"device_properties\", GetDeviceProperties, METH_VARARGS, \"Return a dictionary with the device properties.\"},\n",
      "3460     {\"mem_info\", GetDeviceMemInfo, METH_NOARGS, \"Return a tuple with the free and total memory on the gpu in bytes.\"},\n",
      "3461 #if COMPUTE_GPU_MEM_USED\n",
      "3462     {\"theano_allocated\", GetTheanoAllocInfo, METH_NOARGS, \"Return the size in bytes of memory Theano currently have allocated on the gpu.\"},\n",
      "3463 #endif\n",
      "3464     {\"ptr_int_size\", CudaNdarray_ptr_int_size, METH_VARARGS, \"Return a tuple with the size of gpu pointer, cpu pointer and int in bytes.\"},\n",
      "3465     {\"filter\", filter, METH_VARARGS, \"filter(obj, broadcastable, strict, storage) returns a CudaNdarray initialized to obj if it matches the constraints of broadcastable.  strict=True prevents any numeric casting. If storage is a CudaNdarray it may be overwritten and used as the return value.\"},\n",
      "3466     {\"outstanding_mallocs\", outstanding_mallocs, METH_VARARGS, \"how many more mallocs have been called than free's\"},\n",
      "3467     {\"from_gpu_pointer\", CudaNdarray_from_gpu_pointer, METH_VARARGS, \"Used to create a CudaNdarray from already allocated memory on the gpu.(example by pycuda)\"},\n",
      "3468     {\"synchronize\", CudaNdarray_synchronize, METH_NOARGS, \"Used to synchronize the device\"},\n",
      "3469     {\"cublas_v2\", CudaNdarray_cublasv2, METH_NOARGS,\n",
      "3470      \"Used to know if this version of cuda_ndarray is linked with cublas v2.\"},\n",
      "3471     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3472 };\n",
      "3473 \n",
      "3474 #ifndef PyMODINIT_FUNC  /* declarations for DLL import/export */\n",
      "3475 #define PyMODINIT_FUNC void\n",
      "3476 #endif\n",
      "3477 \n",
      "3478 #define CNDA_MOD_NAME \"cuda_ndarray\"\n",
      "3479 #define CNDA_DOCSTRING \"CUDA implementation of a numpy ndarray-like object.\"\n",
      "3480 \n",
      "3481 #if PY_MAJOR_VERSION == 3\n",
      "3482 static struct PyModuleDef cuda_ndarray_moduledef =\n",
      "3483 {\n",
      "3484     PyModuleDef_HEAD_INIT,\n",
      "3485     CNDA_MOD_NAME,\n",
      "3486     CNDA_DOCSTRING,\n",
      "3487     -1,     /* size of per-interpreter state of the module,\n",
      "3488                or -1 if the module keeps state in global variables. */\n",
      "3489     module_methods\n",
      "3490 };\n",
      "3491 \n",
      "3492 PyMODINIT_FUNC\n",
      "3493 PyInit_cuda_ndarray(void)\n",
      "3494 #else\n",
      "3495 PyMODINIT_FUNC\n",
      "3496 initcuda_ndarray(void)\n",
      "3497 #endif\n",
      "3498 {\n",
      "3499     import_array();\n",
      "3500 \n",
      "3501     PyObject* m;\n",
      "3502 \n",
      "3503     if (PyType_Ready(&CudaNdarrayType) < 0) {\n",
      "3504 #if PY_MAJOR_VERSION == 3\n",
      "3505         return NULL;\n",
      "3506 #else\n",
      "3507         return;\n",
      "3508 #endif\n",
      "3509     }\n",
      "3510 \n",
      "3511 #if PY_MAJOR_VERSION == 3\n",
      "3512     m = PyModule_Create(&cuda_ndarray_moduledef);\n",
      "3513 #else\n",
      "3514     m = Py_InitModule3(CNDA_MOD_NAME, module_methods, CNDA_DOCSTRING);\n",
      "3515 #endif\n",
      "3516 \n",
      "3517     if (m == NULL) {\n",
      "3518 #if PY_MAJOR_VERSION == 3\n",
      "3519         return NULL;\n",
      "3520 #else\n",
      "3521         return;\n",
      "3522 #endif\n",
      "3523     }\n",
      "3524 \n",
      "3525     Py_INCREF(&CudaNdarrayType);\n",
      "3526     PyModule_AddObject(m, \"CudaNdarray\", (PyObject *)&CudaNdarrayType);\n",
      "3527 #if COMPUTE_GPU_MEM_USED\n",
      "3528     for(int i=0;i<TABLE_SIZE;i++){\n",
      "3529         _alloc_size_table[i].ptr=NULL;\n",
      "3530         _alloc_size_table[i].size=0;\n",
      "3531     }\n",
      "3532 #endif\n",
      "3533     //    cublasInit();\n",
      "3534     //if (0&&CUBLAS_STATUS_SUCCESS != cublasGetError())\n",
      "3535     //{\n",
      "3536         //std::cerr << \"WARNING: initcuda_ndarray: error initializing device\\n\";\n",
      "3537     //}\n",
      "3538     if (0) //TODO: is this necessary?\n",
      "3539     {\n",
      "3540         int deviceId = 0; // TODO: what number goes here?\n",
      "3541         cudaSetDevice(deviceId);\n",
      "3542         cudaError_t err = cudaGetLastError();\n",
      "3543         if( cudaSuccess != err)\n",
      "3544         {\n",
      "3545             std::cerr << \"Error in SetDevice:\" << cudaGetErrorString(err) << \"\\n\";\n",
      "3546         }\n",
      "3547     }\n",
      "3548 \n",
      "3549 #if PY_MAJOR_VERSION == 3\n",
      "3550     return m;\n",
      "3551 #endif\n",
      "3552 }\n",
      "3553 \n",
      "3554 \n",
      "3555 //////////////////////////////////////\n",
      "3556 //\n",
      "3557 // C API FOR CudaNdarray\n",
      "3558 //\n",
      "3559 //////////////////////////////////////\n",
      "3560 \n",
      "3561 int\n",
      "3562 CudaNdarray_Check(const PyObject * ob)\n",
      "3563 {\n",
      "3564     //TODO: doesn't work with inheritance\n",
      "3565     return CudaNdarray_CheckExact(ob);\n",
      "3566 }\n",
      "3567 int\n",
      "3568 CudaNdarray_CheckExact(const PyObject * ob)\n",
      "3569 {\n",
      "3570     return ((Py_TYPE(ob) == &CudaNdarrayType) ? 1 : 0);\n",
      "3571 }\n",
      "3572 \n",
      "3573 PyObject *\n",
      "3574 CudaNdarray_New(int nd)\n",
      "3575 {\n",
      "3576     CudaNdarray *self = (CudaNdarray *)CudaNdarrayType.tp_alloc(&CudaNdarrayType, 0);\n",
      "3577     if (self == NULL)\n",
      "3578     {\n",
      "3579         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_New failed to allocate self\");\n",
      "3580         return NULL;\n",
      "3581     }\n",
      "3582     CudaNdarray_null_init(self);\n",
      "3583 \n",
      "3584     if (nd == 0)\n",
      "3585     {\n",
      "3586         self->nd = 0;\n",
      "3587     }\n",
      "3588     else if (nd > 0)\n",
      "3589     {\n",
      "3590         if (CudaNdarray_set_nd(self, nd))\n",
      "3591         {\n",
      "3592             Py_DECREF(self);\n",
      "3593             return NULL;\n",
      "3594         }\n",
      "3595     }\n",
      "3596     ++_outstanding_mallocs[1];\n",
      "3597     return (PyObject *)self;\n",
      "3598 }\n",
      "3599 \n",
      "3600 \n",
      "3601 \n",
      "3602 //////////////////////////////\n",
      "3603 //\n",
      "3604 // Published helper functions\n",
      "3605 //\n",
      "3606 //////////////////////////////\n",
      "3607 \n",
      "3608 static int\n",
      "3609 cublas_init()\n",
      "3610 {\n",
      "3611     cublasStatus_t err;\n",
      "3612     err = cublasCreate(&handle);\n",
      "3613     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3614     {\n",
      "3615         if(CUBLAS_STATUS_NOT_INITIALIZED == err)\n",
      "3616             PyErr_SetString(PyExc_RuntimeError,\n",
      "3617                             \"cublasCreate() returned this error \"\n",
      "3618                             \"'the CUDA Runtime initialization failed'\");\n",
      "3619         else if(CUBLAS_STATUS_ALLOC_FAILED == err)\n",
      "3620             PyErr_SetString(PyExc_RuntimeError,\n",
      "3621                             \"cublasCreate() returned this error \"\n",
      "3622                             \"'the resources could not be allocated'\");\n",
      "3623         else\n",
      "3624             PyErr_SetString(PyExc_RuntimeError,\n",
      "3625                             \"unknow error during returned by cublasCreate()\");\n",
      "3626         return -1;\n",
      "3627     }\n",
      "3628     // Set the default stream as the one to execute on (default)\n",
      "3629     cublasSetStream(handle, NULL);\n",
      "3630     // Pointer to scalars are on the host (also default)\n",
      "3631     cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST);\n",
      "3632 #if CUDA_VERSION >= 5000\n",
      "3633     // atomics can be used in kernels to speed up operations (not default)\n",
      "3634     // This may lead to a slight variance from run to run in some operations\n",
      "3635     cublasSetAtomicsMode(handle, CUBLAS_ATOMICS_ALLOWED);\n",
      "3636 #endif\n",
      "3637     return 0;\n",
      "3638 }\n",
      "3639 \n",
      "3640 static void\n",
      "3641 cublas_shutdown()\n",
      "3642 {\n",
      "3643     if (handle != NULL)\n",
      "3644         cublasDestroy(handle);\n",
      "3645     // No point in handling any errors here\n",
      "3646     handle = NULL;\n",
      "3647 }\n",
      "3648 \n",
      "3649 int\n",
      "3650 CudaNdarray_CopyFromArray(CudaNdarray * self, PyArrayObject*obj)\n",
      "3651 {\n",
      "3652     int err = CudaNdarray_alloc_contiguous(self, PyArray_NDIM(obj),\n",
      "3653                                            PyArray_DIMS(obj));\n",
      "3654     if (err) {\n",
      "3655         return err;\n",
      "3656     }\n",
      "3657 \n",
      "3658     int typenum = PyArray_TYPE(obj);\n",
      "3659     if (typenum != REAL_TYPENUM)\n",
      "3660     {\n",
      "3661         PyErr_SetString(PyExc_TypeError, \"can only copy from float arrays\");\n",
      "3662         return -1;\n",
      "3663     }\n",
      "3664     assert( 4 ==  PyArray_ITEMSIZE(obj));\n",
      "3665     PyArrayObject * py_src = (PyArrayObject *)PyArray_ContiguousFromAny(\n",
      "3666         (PyObject*)obj, typenum, self->nd, self->nd);\n",
      "3667     if (!py_src) {\n",
      "3668         return -1;\n",
      "3669     }\n",
      "3670     npy_intp py_src_size = PyArray_SIZE(py_src);\n",
      "3671     void *py_src_data = PyArray_DATA(py_src);\n",
      "3672     cublasStatus_t cerr;\n",
      "3673     CNDA_BEGIN_ALLOW_THREADS\n",
      "3674     cerr = cublasSetVector(py_src_size,\n",
      "3675                            sizeof(real),\n",
      "3676                            py_src_data, 1,\n",
      "3677                            self->devdata, 1);\n",
      "3678     //CNDA_THREAD_SYNC;  // unneeded because cublasSetVector is blocking anyway\n",
      "3679     CNDA_END_ALLOW_THREADS\n",
      "3680     if (CUBLAS_STATUS_SUCCESS != cerr)\n",
      "3681     {\n",
      "3682         PyErr_SetString(PyExc_RuntimeError, \"error copying data to device memory\");\n",
      "3683         Py_DECREF(py_src);\n",
      "3684         return -1;\n",
      "3685     }\n",
      "3686     Py_DECREF(py_src);\n",
      "3687     return 0;\n",
      "3688 }\n",
      "3689 \n",
      "3690 PyObject *\n",
      "3691 CudaNdarray_new_nd(int nd)\n",
      "3692 {\n",
      "3693     CudaNdarray * rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3694     if (!rval || CudaNdarray_set_nd(rval, nd))\n",
      "3695     {\n",
      "3696         Py_XDECREF(rval);\n",
      "3697         rval = NULL;\n",
      "3698     }\n",
      "3699     return (PyObject *) rval;\n",
      "3700 }\n",
      "3701 \n",
      "3702 \n",
      "3703 /**\n",
      "3704  * Initialize 'self' as a view of 'base', with memory storage 'data'\n",
      "3705  */\n",
      "3706 \n",
      "3707 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, PyObject * base)\n",
      "3708 {\n",
      "3709     if (self->data_allocated)\n",
      "3710     {\n",
      "3711         assert(self->devdata);\n",
      "3712         if (device_free(self->devdata))\n",
      "3713         {\n",
      "3714             self->devdata = NULL;\n",
      "3715             self->data_allocated = 0;\n",
      "3716             return -1;\n",
      "3717         }\n",
      "3718     }\n",
      "3719     // Get the original base object (base.base.base...)\n",
      "3720     PyObject * orig_base = base;\n",
      "3721     // base is not always a CudaNdarray. It can be a GpuArray from pycuda, ...\n",
      "3722     while (orig_base && CudaNdarray_Check(orig_base) && ((CudaNdarray*) orig_base)->base)\n",
      "3723     {\n",
      "3724         // base_base is itself a view\n",
      "3725         orig_base = ((CudaNdarray*) orig_base)->base;\n",
      "3726     }\n",
      "3727     //N.B. XDECREF and XINCREF are no-ops for NULL pointers\n",
      "3728     if (self->base != orig_base)\n",
      "3729     {\n",
      "3730         Py_XDECREF(self->base);\n",
      "3731         self->base = orig_base;\n",
      "3732         Py_XINCREF(self->base);\n",
      "3733     }\n",
      "3734     self->data_allocated = 0;\n",
      "3735     self->devdata = data;\n",
      "3736     return 0;\n",
      "3737 }\n",
      "3738 \n",
      "3739 static __global__ void k_copy_1d(const int N, const float * x, const int sx, float * y, const int sy)\n",
      "3740 {\n",
      "3741     for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += gridDim.x*blockDim.x)\n",
      "3742     {\n",
      "3743         y[i*sy] = x[i*sx];\n",
      "3744     }\n",
      "3745 }\n",
      "3746 \n",
      "3747 // N1 through N4 are the size of y\n",
      "3748 static __global__ void k_copy_4d(const int N1,\n",
      "3749         const int N2, const int N3, const int N4,\n",
      "3750         const float * x, const int sx1, const int sx2, const int sx3,\n",
      "3751         const int sx4,  float * y, const int sy1, const int sy2,\n",
      "3752         const int sy3, const int sy4)\n",
      "3753 {\n",
      "3754     // These must be made int instead of unsigned int due to a bug in nvcc\n",
      "3755     int bx = blockIdx.x;\n",
      "3756     int by = blockIdx.y;\n",
      "3757 \n",
      "3758     for (int i = bx; i < N1; i += gridDim.x)\n",
      "3759     {\n",
      "3760         for (int j = by; j < N2; j += gridDim.y)\n",
      "3761         {\n",
      "3762             for (int k = threadIdx.x; k < N3; k += (int) blockDim.x)\n",
      "3763             {\n",
      "3764                 for (int l = threadIdx.y; l < N4; l += (int) blockDim.y)\n",
      "3765                 {\n",
      "3766                     y[i * sy1 + j * sy2 + k * sy3 + l * sy4] =\n",
      "3767                         x[i * sx1 + j * sx2 + k * sx3 + l * sx4];\n",
      "3768                 }\n",
      "3769             }\n",
      "3770         }\n",
      "3771     }\n",
      "3772 }\n",
      "3773 \n",
      "3774 //copy from other into self\n",
      "3775 int CudaNdarray_CopyFromCudaNdarray(CudaNdarray * self,\n",
      "3776                                     const CudaNdarray * other,\n",
      "3777                                     bool unbroadcast)\n",
      "3778 {\n",
      "3779     int verbose = 0;\n",
      "3780     if (verbose>1) fprintf(stderr, \"CudaNdarray_CopyFromCudaNdarray\\n\");\n",
      "3781 \n",
      "3782     //standard elemwise size checks\n",
      "3783     if (self->nd == -1)\n",
      "3784     {\n",
      "3785         PyErr_SetString(PyExc_TypeError,\n",
      "3786                         \"can't copy into un-initialized CudaNdarray\");\n",
      "3787         return -1;\n",
      "3788     }\n",
      "3789     CudaNdarray * new_other = NULL;\n",
      "3790 \n",
      "3791     if (self->nd < other->nd)\n",
      "3792     {\n",
      "3793         PyErr_Format(PyExc_NotImplementedError,\n",
      "3794             \"CudaNdarray_CopyFromCudaNdarray: The number of dimensions of the \"\n",
      "3795             \"destination needs to be >= the number of dimensions of the \"\n",
      "3796             \"source. Got %d and %d.\", self->nd, other->nd);\n",
      "3797         return -1;\n",
      "3798     }\n",
      "3799     else if (self->nd != other->nd)\n",
      "3800     {\n",
      "3801         new_other = (CudaNdarray *) CudaNdarray_View(other);\n",
      "3802         int added_dims = self->nd - other->nd;\n",
      "3803         int* pattern = (int*) alloca(self->nd * sizeof(int));\n",
      "3804         for(int i = 0; i < added_dims; i++)\n",
      "3805             pattern[i] = -1;\n",
      "3806         for(int i = 0; i < other->nd; i++)\n",
      "3807             pattern[i + added_dims] = i;\n",
      "3808         CudaNdarray_dimshuffle(new_other, self->nd, pattern);\n",
      "3809         other = new_other;\n",
      "3810     }\n",
      "3811     assert(self->nd == other->nd);\n",
      "3812     //standard elemwise dim checks (also compute total size)\n",
      "3813     unsigned int size = 1;\n",
      "3814     unsigned int size_source = 1;\n",
      "3815     for (int i = 0; i< self->nd; ++i)\n",
      "3816     {\n",
      "3817         if ((CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "3818             && (1!=CudaNdarray_HOST_DIMS(other)[i] || !unbroadcast) )\n",
      "3819         {\n",
      "3820           PyErr_Format(PyExc_ValueError,\n",
      "3821                        \"CudaNdarray_CopyFromCudaNdarray:\"\n",
      "3822                        \" need same dimensions for dim %d,\"\n",
      "3823                        \" destination=%d, source=%d\",\n",
      "3824                        i, CudaNdarray_HOST_DIMS(self)[i],\n",
      "3825                        CudaNdarray_HOST_DIMS(other)[i]);\n",
      "3826           Py_XDECREF(new_other);\n",
      "3827           return -1;\n",
      "3828         }\n",
      "3829         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "3830         size_source *= (unsigned int) CudaNdarray_HOST_DIMS(other)[i];\n",
      "3831     }\n",
      "3832     if (0 == size)\n",
      "3833     {\n",
      "3834         Py_XDECREF(new_other);\n",
      "3835         return 0; //nothing to copy, we're done.\n",
      "3836     }\n",
      "3837     if (CudaNdarray_is_c_contiguous(self) &&\n",
      "3838         CudaNdarray_is_c_contiguous(other) &&\n",
      "3839         size == size_source)\n",
      "3840     {\n",
      "3841         if (verbose)\n",
      "3842             fprintf(stderr, \"Copying contiguous vector with cublasScopy\\n\");\n",
      "3843 \n",
      "3844         cublasStatus_t err;\n",
      "3845         err = cublasScopy(handle, size, CudaNdarray_DEV_DATA(other), 1,\n",
      "3846                           CudaNdarray_DEV_DATA(self), 1);\n",
      "3847         CNDA_THREAD_SYNC;\n",
      "3848         Py_XDECREF(new_other);\n",
      "3849         if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3850         {\n",
      "3851             PyErr_SetString(PyExc_RuntimeError, \"Error copying memory\");\n",
      "3852             return -1;\n",
      "3853         }\n",
      "3854         return 0;\n",
      "3855     }\n",
      "3856     //TODO: rewrite these copy operations to be more efficient\n",
      "3857     //      See, for example the transpose example in the cuda_sdk.\n",
      "3858     switch (self->nd)\n",
      "3859     {\n",
      "3860         case 0: // scalar\n",
      "3861             {\n",
      "3862                 // THIS CASE SHOULD NEVER HAPPEN BECAUSE SCALARS ARE ALWAYS C CONTIGUOUS\n",
      "3863                 assert(0);\n",
      "3864             }; break;\n",
      "3865         case 1: // vector\n",
      "3866             {\n",
      "3867                 if (verbose) fprintf(stderr, \"Copying non-contiguous vector\\n\");\n",
      "3868                 if (verbose) fprint_CudaNdarray(stderr, other);\n",
      "3869                 unsigned int n_blocks = std::min(size,\n",
      "3870                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "3871                 unsigned int n_threads = std::min(ceil_intdiv(size, n_blocks),\n",
      "3872                                                   (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "3873                 k_copy_1d<<<n_blocks, n_threads>>>(size,\n",
      "3874                                             CudaNdarray_DEV_DATA(other),\n",
      "3875                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "3876                                             CudaNdarray_DEV_DATA(self),\n",
      "3877                                             CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "3878                 CNDA_THREAD_SYNC;\n",
      "3879                 cudaError_t err = cudaGetLastError();\n",
      "3880                 if( cudaSuccess != err)\n",
      "3881                 {\n",
      "3882                     PyErr_Format(PyExc_RuntimeError,\n",
      "3883                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "3884                                  \" n_threads_per_block=%i)\\n\", \"k_copy_1d\",\n",
      "3885                                  cudaGetErrorString(err), n_blocks, n_threads);\n",
      "3886                     Py_XDECREF(new_other);\n",
      "3887                     return -1;\n",
      "3888                 }\n",
      "3889             }; break;\n",
      "3890         case 4: // 4-tensor\n",
      "3891             {\n",
      "3892                 if (verbose)\n",
      "3893                 {\n",
      "3894                     if (0 != fprint_CudaNdarray(stderr, other))\n",
      "3895                     {\n",
      "3896                         Py_XDECREF(new_other);\n",
      "3897                         return -1;\n",
      "3898                     }\n",
      "3899                 }\n",
      "3900 \n",
      "3901                 // The blocks implement the looping over the first two axes so\n",
      "3902                 // this needs to be (N1, N2)\n",
      "3903                 dim3 n_blocks( std::min(CudaNdarray_HOST_DIMS(self)[0],\n",
      "3904                                         NUM_VECTOR_OP_BLOCKS),\n",
      "3905                                std::min(CudaNdarray_HOST_DIMS(self)[1],\n",
      "3906                                         NUM_VECTOR_OP_BLOCKS));\n",
      "3907                 // For the threads, just make as many as possible\n",
      "3908                 dim3 n_threads( std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[2],\n",
      "3909                                  (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK),\n",
      "3910                                 std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[3],\n",
      "3911                                     (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "3912 \n",
      "3913                 n_threads.x = std::min( (unsigned int) 32, (unsigned int) n_threads.x);\n",
      "3914                 n_threads.y = std::min( n_threads.y, NUM_VECTOR_OP_THREADS_PER_BLOCK / n_threads.x);\n",
      "3915 \n",
      "3916                 k_copy_4d<<<n_blocks, n_threads>>>(\n",
      "3917                                             // size of y\n",
      "3918                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[0], // N1\n",
      "3919                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[1], // N2\n",
      "3920                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[2], // N3\n",
      "3921                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[3], // N4\n",
      "3922                                             CudaNdarray_DEV_DATA(other), // x\n",
      "3923                                             // x strides\n",
      "3924                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "3925                                             CudaNdarray_HOST_STRIDES(other)[1],\n",
      "3926                                             CudaNdarray_HOST_STRIDES(other)[2],\n",
      "3927                                             CudaNdarray_HOST_STRIDES(other)[3],\n",
      "3928                                             CudaNdarray_DEV_DATA(self), // y\n",
      "3929                                             // y strides\n",
      "3930                                             CudaNdarray_HOST_STRIDES(self)[0],\n",
      "3931                                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "3932                                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "3933                                             CudaNdarray_HOST_STRIDES(self)[3]\n",
      "3934                                             );\n",
      "3935                 CNDA_THREAD_SYNC;\n",
      "3936                 cudaError_t err = cudaGetLastError();\n",
      "3937                 if( cudaSuccess != err)\n",
      "3938                 {\n",
      "3939                     PyErr_Format(PyExc_RuntimeError,\n",
      "3940                                  \"Cuda error: %s: %s.\",\n",
      "3941                                  \"k_copy_4d\",\n",
      "3942                                  cudaGetErrorString(err));\n",
      "3943                     Py_XDECREF(new_other);\n",
      "3944                     return -1;\n",
      "3945                 }\n",
      "3946             }; break;\n",
      "3947         default:\n",
      "3948             {\n",
      "3949                 assert (cudaSuccess == cudaGetLastError());\n",
      "3950                 if (verbose)\n",
      "3951                     fprintf(stderr,\n",
      "3952                             \"Copying with default version unbroadcast=%d\\n\",\n",
      "3953                             unbroadcast);\n",
      "3954                 // call worker routine\n",
      "3955                 unsigned int threads_per_block = std::min(size,\n",
      "3956                                                           (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "3957                 unsigned int n_blocks = std::min(ceil_intdiv(size, threads_per_block),\n",
      "3958                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "3959                 const CudaNdarray * cuda_dims = other;\n",
      "3960                 if(unbroadcast)\n",
      "3961                     cuda_dims = self;\n",
      "3962                 //copy from other into self\n",
      "3963                 k_elemwise_unary_rowmajor_copy<<<n_blocks, threads_per_block>>>(\n",
      "3964                         size,\n",
      "3965                         (unsigned int)other->nd,\n",
      "3966                         (const int *)CudaNdarray_DEV_DIMS(cuda_dims),\n",
      "3967                         (const float*)CudaNdarray_DEV_DATA(other),\n",
      "3968                         (const int *)CudaNdarray_DEV_STRIDES(other),\n",
      "3969                         CudaNdarray_DEV_DATA(self),\n",
      "3970                         (const int *)CudaNdarray_DEV_STRIDES(self));\n",
      "3971                 CNDA_THREAD_SYNC;\n",
      "3972                 cudaError_t err = cudaGetLastError();\n",
      "3973                 if(verbose>1)\n",
      "3974                     fprintf(stderr,\n",
      "3975                             \"INFO k_elemwise_unary_rowmaj (n_blocks=%i,\"\n",
      "3976                             \" n_threads_per_block=%i)\\n\",\n",
      "3977                             n_blocks, threads_per_block);\n",
      "3978                 if( cudaSuccess != err)\n",
      "3979                 {\n",
      "3980                     //fprint_CudaNdarray(stderr, self);\n",
      "3981                     //fprint_CudaNdarray(stderr, other);\n",
      "3982                     PyErr_Format(PyExc_RuntimeError,\n",
      "3983                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "3984                                  \" n_threads_per_block=%i)\\n\",\n",
      "3985                                  \"k_elemwise_unary_rowmajor_copy\",\n",
      "3986                                  cudaGetErrorString(err), n_blocks,\n",
      "3987                                  threads_per_block);\n",
      "3988                     Py_XDECREF(new_other);\n",
      "3989                     return -1;\n",
      "3990                 }\n",
      "3991             }\n",
      "3992     };\n",
      "3993     Py_XDECREF(new_other);\n",
      "3994     return 0;\n",
      "3995 }\n",
      "3996 \n",
      "3997 int CudaNdarray_gemm(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "3998 {\n",
      "3999     if (A->nd != 2)\n",
      "4000     {\n",
      "4001         PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to gemm\");\n",
      "4002         return -1;\n",
      "4003     }\n",
      "4004     if (B->nd != 2)\n",
      "4005     {\n",
      "4006         PyErr_SetString(PyExc_ValueError, \"non-matrix arg B to gemm\");\n",
      "4007         return -1;\n",
      "4008     }\n",
      "4009     if (C->nd != 2)\n",
      "4010     {\n",
      "4011         PyErr_SetString(PyExc_ValueError, \"non-matrix arg C to gemm\");\n",
      "4012         return -1;\n",
      "4013     }\n",
      "4014 \n",
      "4015     // We must allow dimensions to be zeros.\n",
      "4016     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4017             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0])\n",
      "4018             || (CudaNdarray_HOST_DIMS(B)[1] != CudaNdarray_HOST_DIMS(C)[1]))\n",
      "4019     {\n",
      "4020         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemm (%i,%i)x(%i,%i)->(%i,%i)\",\n",
      "4021                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4022                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4023                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4024                 CudaNdarray_HOST_DIMS(B)[1],\n",
      "4025                 CudaNdarray_HOST_DIMS(C)[0],\n",
      "4026                 CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4027         return -1;\n",
      "4028     }\n",
      "4029 \n",
      "4030     // If matrix A or B has non-unit size and non-unit stride in both\n",
      "4031     // dimensions, we can make a copy.\n",
      "4032     CudaNdarray * A_new = NULL;\n",
      "4033     CudaNdarray * B_new = NULL;\n",
      "4034     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4035          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4036          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4037          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4038         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4039         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4040     {\n",
      "4041         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4042         if (!A_new)\n",
      "4043             return -1;\n",
      "4044         A = A_new;\n",
      "4045     }\n",
      "4046 \n",
      "4047     if (((CudaNdarray_HOST_DIMS(B)[0] > 1)\n",
      "4048          && (CudaNdarray_HOST_STRIDES(B)[0] != 1)\n",
      "4049          && (CudaNdarray_HOST_DIMS(B)[1] > 1)\n",
      "4050          && (CudaNdarray_HOST_STRIDES(B)[1] != 1))\n",
      "4051         || (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4052         || (CudaNdarray_HOST_STRIDES(B)[1] < 0))\n",
      "4053     {\n",
      "4054         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4055         if (!B_new)\n",
      "4056         {\n",
      "4057             // If A_new is NULL, meaning A was not copied nothing happens\n",
      "4058             Py_XDECREF(A_new);\n",
      "4059             return -1;\n",
      "4060         }\n",
      "4061         B = B_new;\n",
      "4062     }\n",
      "4063 \n",
      "4064     // If matrix C has non-unit size and non-unit stride in both\n",
      "4065     // dimensions, or negative strides, we can't operate. We cannot copy\n",
      "4066     // C either, because the calling code will expect the result to be\n",
      "4067     // in the original C container.\n",
      "4068     if (((CudaNdarray_HOST_DIMS(C)[0] > 1)\n",
      "4069          && (CudaNdarray_HOST_STRIDES(C)[0] != 1)\n",
      "4070          && (CudaNdarray_HOST_DIMS(C)[1] > 1)\n",
      "4071          && (CudaNdarray_HOST_STRIDES(C)[1] != 1))\n",
      "4072         || (CudaNdarray_HOST_STRIDES(C)[0] < 0)\n",
      "4073         || (CudaNdarray_HOST_STRIDES(C)[1] < 0))\n",
      "4074     {\n",
      "4075         PyErr_Format(PyExc_AssertionError,\n",
      "4076                      \"non-unit or negative stride in gemm arg C (%i,%i) of shape (%i,%i)\",\n",
      "4077                      CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4078                      CudaNdarray_HOST_STRIDES(C)[1],\n",
      "4079                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4080                      CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4081         Py_XDECREF(A_new);\n",
      "4082         Py_XDECREF(B_new);\n",
      "4083         return -1;\n",
      "4084     }\n",
      "4085 \n",
      "4086     // the unit integer is divided logically into three fields of 4 bits\n",
      "4087     // the lowermost 4 bits encode the stride pattern of the output\n",
      "4088     // the next higher 4 bits encode the B variable (or y)\n",
      "4089     // the next higher 4 bits encode the C variable (or x)\n",
      "4090     //\n",
      "4091     // the stride pattern for each input is encoded as 0 for unit stride from col to col (Row major)\n",
      "4092     //                                                 1 for unit stride from row to row (Col major)\n",
      "4093 \n",
      "4094     // a stride of 0 implies a dimension of 1 - so we can actually define\n",
      "4095     // a stride of 0 as a 'unit' stride because gemm will never use it.\n",
      "4096     // If a dimension is 0, its stride will not be used either, so we can\n",
      "4097     // consider it a 'unit' stride too.\n",
      "4098     int unit = 0;\n",
      "4099     if (CudaNdarray_HOST_STRIDES(A)[1] == 1 || CudaNdarray_HOST_DIMS(A)[1] <= 1) {\n",
      "4100         unit |= (0x0 << 8);\n",
      "4101     } else if (CudaNdarray_HOST_STRIDES(A)[0] == 1 || CudaNdarray_HOST_DIMS(A)[0] <= 1) {\n",
      "4102         unit |= (0x1 << 8);\n",
      "4103     } else {\n",
      "4104         unit |= (0x2 << 8);\n",
      "4105     }\n",
      "4106     if (CudaNdarray_HOST_STRIDES(B)[1] == 1 || CudaNdarray_HOST_DIMS(B)[1] <= 1) {\n",
      "4107         unit |= (0x0 << 4);\n",
      "4108     } else if (CudaNdarray_HOST_STRIDES(B)[0] == 1 || CudaNdarray_HOST_DIMS(B)[0] <= 1) {\n",
      "4109         unit |= (0x1 << 4);\n",
      "4110     } else {\n",
      "4111         unit |= (0x2 << 4);\n",
      "4112     }\n",
      "4113     if (CudaNdarray_HOST_STRIDES(C)[1] == 1 || CudaNdarray_HOST_DIMS(C)[1] <= 1) {\n",
      "4114         unit |= (0x0 << 0);\n",
      "4115     } else if (CudaNdarray_HOST_STRIDES(C)[0] == 1 || CudaNdarray_HOST_DIMS(C)[0] <= 1) {\n",
      "4116         unit |= (0x1 << 0);\n",
      "4117     } else {\n",
      "4118         unit |= (0x2 << 0);\n",
      "4119     }\n",
      "4120 \n",
      "4121     /* create appropriate strides for malformed matrices that are row or column\n",
      "4122      * vectors\n",
      "4123      */\n",
      "4124     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4125     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4126     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : CudaNdarray_HOST_DIMS(B)[1];\n",
      "4127     int sb_1 = (CudaNdarray_HOST_DIMS(B)[1] > 1) ? CudaNdarray_HOST_STRIDES(B)[1] : CudaNdarray_HOST_DIMS(B)[0];\n",
      "4128     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : CudaNdarray_HOST_DIMS(C)[1];\n",
      "4129     int sc_1 = (CudaNdarray_HOST_DIMS(C)[1] > 1) ? CudaNdarray_HOST_STRIDES(C)[1] : CudaNdarray_HOST_DIMS(C)[0];\n",
      "4130 \n",
      "4131     float* a = CudaNdarray_DEV_DATA(A);\n",
      "4132     float* b = CudaNdarray_DEV_DATA(B);\n",
      "4133     float* c = CudaNdarray_DEV_DATA(C);\n",
      "4134     cublasOperation_t N = CUBLAS_OP_N;\n",
      "4135     cublasOperation_t T = CUBLAS_OP_T;\n",
      "4136     //std::cerr << (unit/256) MOD 16 << (unit / 16) MOD 16 << unit MOD 16<< '\\\\n';\n",
      "4137     // There should be no negative stride at that point\n",
      "4138 #define CHK_STRIDE_SGEMM(T0, T1, D0, D1, D2, a, x, sx, y, sy, b, z, sz) \\\n",
      "4139     if (sx == 0){sx = 1;}\\\n",
      "4140     if (sy == 0){sy = 1;}\\\n",
      "4141     if (sz == 0){sz = 1;}\\\n",
      "4142     if ((sx > 0) && (sy > 0) && (sz > 0)) { \\\n",
      "4143         err = cublasSgemm(handle, T0, T1, D0, D1, D2, &a, x, sx, y, sy, &b, z, sz); \\\n",
      "4144     } else { \\\n",
      "4145         PyErr_SetString(PyExc_AssertionError, \"negative stride to sGemm\");\\\n",
      "4146         Py_XDECREF(A_new);\\\n",
      "4147         Py_XDECREF(B_new);\\\n",
      "4148         return -1; \\\n",
      "4149     }\n",
      "4150 \n",
      "4151     cublasStatus_t err;\n",
      "4152     switch(unit)\n",
      "4153     {\n",
      "4154         case 0x000: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_0, beta, c, sc_0); break;\n",
      "4155         case 0x100: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_1, beta, c, sc_0); break;\n",
      "4156         case 0x010: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_0, beta, c, sc_0); break;\n",
      "4157         case 0x110: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_1, beta, c, sc_0); break;\n",
      "4158         case 0x001: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_0, beta, c, sc_1); break;\n",
      "4159         case 0x101: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_0, beta, c, sc_1); break;\n",
      "4160         case 0x011: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_1, beta, c, sc_1); break;\n",
      "4161         case 0x111: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_1, beta, c, sc_1); break;\n",
      "4162         default: PyErr_Format(PyExc_ValueError, \"some matrix has no unit stride (unit=%x)\", unit);\n",
      "4163                  return -1;\n",
      "4164     };\n",
      "4165     CNDA_THREAD_SYNC;\n",
      "4166     Py_XDECREF(A_new);\n",
      "4167     Py_XDECREF(B_new);\n",
      "4168 \n",
      "4169     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4170     {\n",
      "4171         PyErr_Format(PyExc_RuntimeError,\n",
      "4172                      \"cublasSgemm failed (%i) %s\\n\"\n",
      "4173                      \" unit=%x N=%d, c.dims=[%d %d], a.dim=[%d %d], alpha=%f, beta=%f, a=%p, b=%p, c=%p\"\n",
      "4174                      \" sa_0=%d, sa_1=%d, sb_0=%d, sb_1=%d, sc_0=%d, sc_1=%d\",\n",
      "4175                      err,  cublasGetErrorString(err),\n",
      "4176                      unit, N,\n",
      "4177                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4178                      CudaNdarray_HOST_DIMS(C)[1],\n",
      "4179                      CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4180                      alpha, beta, a, b, c, sa_0, sa_1, sb_0, sb_1, sc_0, sc_1);\n",
      "4181 \n",
      "4182         return -1;\n",
      "4183     }\n",
      "4184     return 0;\n",
      "4185 }\n",
      "4186 \n",
      "4187 int CudaNdarray_sgemv(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4188 {\n",
      "4189     /**\n",
      "4190     * C <- alpha A B + beta C\n",
      "4191     *    A : matrix\n",
      "4192     *    B, C: vector\n",
      "4193     *    alpha, beta: scalars\n",
      "4194     */\n",
      "4195     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg to gemv\"); return -1; }\n",
      "4196     if (B->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4197     if (C->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4198 \n",
      "4199     // We must allow dimensions to be zeros.\n",
      "4200     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4201             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0]))\n",
      "4202     {\n",
      "4203         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemv (%i,%i)x(%i)->(%i)\",\n",
      "4204                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4205                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4206                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4207                 CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4208         return -1;\n",
      "4209     }\n",
      "4210 \n",
      "4211     // If matrix A has non-unit size and non-unit stride in both\n",
      "4212     // dimensions, or negative strides, we cannot operate, but we can\n",
      "4213     // make a copy.\n",
      "4214     CudaNdarray * A_new = NULL;\n",
      "4215     CudaNdarray * B_new = NULL;\n",
      "4216     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4217          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4218          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4219          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4220         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4221         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4222     {\n",
      "4223         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4224         if (!A_new)\n",
      "4225             return -1;\n",
      "4226         A = A_new;\n",
      "4227     }\n",
      "4228 \n",
      "4229     // If vector B as a negative stride, we also have to make a copy.\n",
      "4230     if (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4231     {\n",
      "4232         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4233         if (!B_new)\n",
      "4234         {\n",
      "4235             // If A was not copied, A_new is NULL, and Py_XDECREF does not\n",
      "4236             // do anything\n",
      "4237             Py_XDECREF(A_new);\n",
      "4238             return -1;\n",
      "4239         }\n",
      "4240         B = B_new;\n",
      "4241     }\n",
      "4242 \n",
      "4243     // cudablas does not handle negative strides as expected\n",
      "4244     if (   (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4245         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4246     {\n",
      "4247         PyErr_Format(PyExc_ValueError, \"illegal strides in args to gemv (%i,%i)\",\n",
      "4248                 CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4249                 CudaNdarray_HOST_STRIDES(A)[1]);\n",
      "4250         Py_XDECREF(A_new);\n",
      "4251         Py_XDECREF(B_new);\n",
      "4252         return -1;\n",
      "4253     }\n",
      "4254 \n",
      "4255     /* create appropriate strides for malformed matrices that are row or column\n",
      "4256      * vectors\n",
      "4257      */\n",
      "4258     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4259     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4260     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : 1;\n",
      "4261     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : 1;\n",
      "4262 \n",
      "4263     if (sa_0 == 0)\n",
      "4264         sa_0 = 1;\n",
      "4265     if (sa_1 == 0)\n",
      "4266         sa_1 = 1;\n",
      "4267 \n",
      "4268     // This is important because we can end up not calling Sgemv at all\n",
      "4269     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4270     if (CudaNdarray_SIZE(C)) {\n",
      "4271         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4272             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4273                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4274         {\n",
      "4275             err = cublasSgemv(handle, CUBLAS_OP_N,\n",
      "4276                     CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4277                     &alpha,\n",
      "4278                     CudaNdarray_DEV_DATA(A), sa_1,\n",
      "4279                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4280                     &beta,\n",
      "4281                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4282         }\n",
      "4283         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4284                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4285                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4286         {\n",
      "4287             err = cublasSgemv(handle, CUBLAS_OP_T,\n",
      "4288                     CudaNdarray_HOST_DIMS(A)[1], CudaNdarray_HOST_DIMS(A)[0],\n",
      "4289                     &alpha,\n",
      "4290                     CudaNdarray_DEV_DATA(A), sa_0,\n",
      "4291                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4292                     &beta,\n",
      "4293                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4294         }\n",
      "4295         else\n",
      "4296         {\n",
      "4297             PyErr_Format(PyExc_AssertionError,\n",
      "4298                          \"Unexpected stride pattern in gemv: (%i, %i) x %i -> %i.\\n\"\n",
      "4299                          \"Shapes are: (%i, %i) x %i -> %i\\n\",\n",
      "4300                          CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4301                          CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4302                          CudaNdarray_HOST_STRIDES(B)[0],\n",
      "4303                          CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4304                          CudaNdarray_HOST_DIMS(A)[0],\n",
      "4305                          CudaNdarray_HOST_DIMS(A)[1],\n",
      "4306                          CudaNdarray_HOST_DIMS(B)[0],\n",
      "4307                          CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4308             Py_XDECREF(A_new);\n",
      "4309             Py_XDECREF(B_new);\n",
      "4310             return -1;\n",
      "4311         }\n",
      "4312     }\n",
      "4313 \n",
      "4314     CNDA_THREAD_SYNC;\n",
      "4315     Py_XDECREF(A_new);\n",
      "4316     Py_XDECREF(B_new);\n",
      "4317 \n",
      "4318     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4319     {\n",
      "4320         PyErr_Format(PyExc_RuntimeError,\n",
      "4321                      \"cublasSgemv failed (%i)\",\n",
      "4322                      err);\n",
      "4323         return -1;\n",
      "4324     }\n",
      "4325     return 0;\n",
      "4326 }\n",
      "4327 \n",
      "4328 int CudaNdarray_sger(float alpha, const CudaNdarray * x, const CudaNdarray * y, CudaNdarray * A) {\n",
      "4329     if (x->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg x to sger\"); return -1; }\n",
      "4330     if (y->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg y to sger\"); return -1; }\n",
      "4331     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to sger\"); return -1; }\n",
      "4332 \n",
      "4333     if ((CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(x)[0])\n",
      "4334         || (CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(y)[0])) {\n",
      "4335         PyErr_Format(PyExc_ValueError,\n",
      "4336                      \"dimension mismatch in args to sger (%i)x(%i)->(%i,%i)\",\n",
      "4337                      CudaNdarray_HOST_DIMS(x)[0],\n",
      "4338                      CudaNdarray_HOST_DIMS(y)[0],\n",
      "4339                      CudaNdarray_HOST_DIMS(A)[0],\n",
      "4340                      CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4341         return -1;\n",
      "4342     }\n",
      "4343 \n",
      "4344     int x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4345     CudaNdarray * x_new = NULL;\n",
      "4346     if(x_strides == 0){\n",
      "4347         if(CudaNdarray_HOST_DIMS(x)[0] != 1){\n",
      "4348             PyErr_Format(PyExc_RuntimeError,\n",
      "4349                          \"CudaNdarray_sger: Invalid input x (should not happen).\"\n",
      "4350                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4351                          \" that has more than 1 element!\");\n",
      "4352             return -1;\n",
      "4353         }\n",
      "4354         x_strides = 1;\n",
      "4355     } else if(x_strides < 0){\n",
      "4356         x_new = (CudaNdarray*) CudaNdarray_Copy(x);\n",
      "4357         x = x_new;\n",
      "4358         x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4359     }\n",
      "4360 \n",
      "4361     int y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4362     CudaNdarray * y_new = NULL;\n",
      "4363     if(y_strides == 0){\n",
      "4364         if(CudaNdarray_HOST_DIMS(y)[0] != 1){\n",
      "4365             PyErr_Format(PyExc_RuntimeError,\n",
      "4366                          \"CudaNdarray_sger: Invalid input y (should not happen).\"\n",
      "4367                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4368                          \" that has more than 1 elements!\");\n",
      "4369             Py_XDECREF(x_new);\n",
      "4370             return -1;\n",
      "4371         }\n",
      "4372         y_strides = 1;\n",
      "4373     } else if(y_strides < 0){\n",
      "4374         y_new = (CudaNdarray*) CudaNdarray_Copy(y);\n",
      "4375         y = y_new;\n",
      "4376         y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4377     }\n",
      "4378 \n",
      "4379     // Create appropriate strides if A is a row or column vector\n",
      "4380     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0]\n",
      "4381                                                  : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4382     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1]\n",
      "4383                                                  : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4384 \n",
      "4385     // This is important because we can end up not calling Sger at all\n",
      "4386     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4387     if(CudaNdarray_SIZE(A)){\n",
      "4388         // If A is in col-major\n",
      "4389         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4390             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4391                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4392         {\n",
      "4393             err = cublasSger(handle, CudaNdarray_HOST_DIMS(x)[0], CudaNdarray_HOST_DIMS(y)[0], &alpha,\n",
      "4394                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4395                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4396                        CudaNdarray_DEV_DATA(A), sa_1);\n",
      "4397         }\n",
      "4398         // Since Sger expects A in col-major, we invert x and y to fake this.\n",
      "4399         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4400                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4401                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4402         {\n",
      "4403             err = cublasSger(handle, CudaNdarray_HOST_DIMS(y)[0], CudaNdarray_HOST_DIMS(x)[0], &alpha,\n",
      "4404                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4405                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4406                        CudaNdarray_DEV_DATA(A), sa_0);\n",
      "4407         }\n",
      "4408         // A has to be either c- or f-contiguous, with no negative strides\n",
      "4409         else\n",
      "4410         {\n",
      "4411             PyErr_SetString(PyExc_NotImplementedError,\n",
      "4412                             \"non-contiguous A, or negative strides, in sger\");\n",
      "4413             Py_XDECREF(x_new);\n",
      "4414             Py_XDECREF(y_new);\n",
      "4415             return -1;\n",
      "4416         }\n",
      "4417     }\n",
      "4418     CNDA_THREAD_SYNC;\n",
      "4419     Py_XDECREF(x_new);\n",
      "4420     Py_XDECREF(y_new);\n",
      "4421 \n",
      "4422     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4423     {\n",
      "4424         PyErr_Format(PyExc_RuntimeError,\n",
      "4425                      \"cublasSger failed (%i)\",\n",
      "4426                      err);\n",
      "4427         return -1;\n",
      "4428     }\n",
      "4429 \n",
      "4430     return 0;\n",
      "4431 }\n",
      "4432 \n",
      "4433 /**\n",
      "4434  *\n",
      "4435  * Precondition:\n",
      "4436  *  a->dim[d] == (dims_a[d]==0) ? (1 << log2_dims_a[d]) : dims_a[d]\n",
      "4437  *  z->dim[d] == (z_str[d]==0) ? 1 : dims_a[d];\n",
      "4438  *\n",
      "4439  *  TODO: templatize this function to support other reductions.\n",
      "4440  *  All that needs to change is the initial value for sum, and the reduction operator.\n",
      "4441  */\n",
      "4442 \n",
      "4443 static __global__ void kernel_reduce_sum(const unsigned int size_z,\n",
      "4444         const unsigned int nd,\n",
      "4445         const int * dims_a,\n",
      "4446         const int * log2_dims_a,\n",
      "4447         const int * a_str,\n",
      "4448         const float * a_data,\n",
      "4449         const int * z_str,\n",
      "4450         float * z_data)\n",
      "4451 {\n",
      "4452     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "4453     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "4454 \n",
      "4455     //structure data contains the strides and dimensions of both a and z\n",
      "4456     // a_dim[0], a_dim[1], ... a_dim[nd-1],\n",
      "4457     // a_log2dim[0], a_log2dim[1], ... a_log2dim[nd-1],\n",
      "4458     // a_str[0], ... a_str[nd-1],\n",
      "4459     // z_str[0], ... z_str[nd-1]\n",
      "4460     extern __shared__ int structure_data[];\n",
      "4461     for (unsigned int i = threadIdx.x; i < nd; i += blockDim.x)\n",
      "4462     {\n",
      "4463         structure_data[i+0*nd] = dims_a[i];\n",
      "4464         structure_data[i+1*nd] = log2_dims_a[i];\n",
      "4465         structure_data[i+2*nd] = a_str[i];\n",
      "4466         structure_data[i+3*nd] = z_str[i];\n",
      "4467     }\n",
      "4468     dims_a = structure_data;\n",
      "4469     log2_dims_a = structure_data + nd;\n",
      "4470     a_str = structure_data + 2*nd;\n",
      "4471     z_str = structure_data + 3*nd;\n",
      "4472 \n",
      "4473     __syncthreads(); //wait for all the shared structure to be loaded\n",
      "4474 \n",
      "4475     for (unsigned int i = idx; i < size_z; i += numThreads)\n",
      "4476     {\n",
      "4477         unsigned int ii = i;\n",
      "4478         const float * a_data_i = a_data;\n",
      "4479         float * z_data_i = z_data;\n",
      "4480         unsigned int n_reduce_elements = 1;\n",
      "4481         unsigned int n_reduce_dims = 0;\n",
      "4482         unsigned int reduce_dim0 = nd-1;\n",
      "4483 \n",
      "4484 \n",
      "4485         //In this loop, we locate the initial element of the slice that we'd like to reduce with this thread\n",
      "4486         //  At the same time, we [re]calculate the size of that slice (n_reduce_elements)\n",
      "4487         for (unsigned int d = 0; d < nd; ++d)\n",
      "4488         {\n",
      "4489             if (a_str[d] && (!z_str[d])) // this means 'd' is a dimension we are reducing over\n",
      "4490             {\n",
      "4491                 n_reduce_elements *= dims_a[d];\n",
      "4492                 n_reduce_dims += 1;\n",
      "4493                 reduce_dim0 = (d < reduce_dim0) ? d : reduce_dim0;\n",
      "4494             }\n",
      "4495             else //'d' is not a dimension that we are reducing over\n",
      "4496             {\n",
      "4497                 unsigned int pos_d;\n",
      "4498                 if (log2_dims_a[d]==-1) //TODO: when things are working, use this switch\n",
      "4499                 {\n",
      "4500                     // this branch is not preferred,\n",
      "4501                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4502                     pos_d = (ii % dims_a[d]);\n",
      "4503                     ii = (ii / dims_a[d]);\n",
      "4504                 }\n",
      "4505                 else\n",
      "4506                 {\n",
      "4507                     pos_d = (ii & ((1 << log2_dims_a[d])-1)); //take the lower log2_dims bits\n",
      "4508                     ii = (ii >> log2_dims_a[d]);  //shift those lower log2_dims bits off of ii\n",
      "4509                 }\n",
      "4510                 a_data_i += pos_d * a_str[d];\n",
      "4511                 z_data_i += pos_d * z_str[d];\n",
      "4512             }\n",
      "4513         }\n",
      "4514         // now we've got pointers a_data_i and z_data_i into element 0 of the slice over which we are reducing\n",
      "4515         // do a similar loop\n",
      "4516 \n",
      "4517         float sum = 0.0f;\n",
      "4518         switch(n_reduce_dims)\n",
      "4519         {\n",
      "4520             case 0:\n",
      "4521                 {\n",
      "4522                     sum = a_data_i[0];\n",
      "4523                 }\n",
      "4524                 break;\n",
      "4525             case 1:\n",
      "4526                 {\n",
      "4527                     const int stride = a_str[reduce_dim0];\n",
      "4528                     const float * a_data_i_max = a_data_i + dims_a[reduce_dim0] * stride;\n",
      "4529                     while (a_data_i != a_data_i_max)\n",
      "4530                     {\n",
      "4531                         sum += a_data_i[0];\n",
      "4532                         a_data_i += stride;\n",
      "4533                     }\n",
      "4534                 }\n",
      "4535                 break;\n",
      "4536             case 2:\n",
      "4537                 {\n",
      "4538                     int rd = reduce_dim0+1;\n",
      "4539                     for (; rd < nd; ++rd)\n",
      "4540                     {\n",
      "4541                         if (a_str[rd] && (!z_str[rd])) // this means 'rd' is a dimension we are reducing over\n",
      "4542                             break;\n",
      "4543                     }\n",
      "4544                     const int stride0 = a_str[reduce_dim0];\n",
      "4545                     const int stride1 = a_str[rd];\n",
      "4546                     for (int ii = 0; ii < dims_a[rd]; ++ii)\n",
      "4547                     {\n",
      "4548                         const float * a_data_ri = a_data_i + ii * stride1;\n",
      "4549                         const float * a_data_ri_max = a_data_ri + dims_a[reduce_dim0] * stride0;\n",
      "4550                         while (a_data_ri != a_data_ri_max)\n",
      "4551                         {\n",
      "4552                             sum += a_data_ri[0];\n",
      "4553                             a_data_ri += stride0;\n",
      "4554                         }\n",
      "4555                     }\n",
      "4556                 };\n",
      "4557                 break;\n",
      "4558             default:\n",
      "4559                 {\n",
      "4560                     for (unsigned int reduce_i = 0; reduce_i < n_reduce_elements; ++reduce_i)\n",
      "4561                     {\n",
      "4562                         //TODO: optimize this loop to work more like theano's Elemwise.  It's serial code.\n",
      "4563                         unsigned int reduce_ii = reduce_i;\n",
      "4564                         const float * a_data_ri = a_data_i;\n",
      "4565 \n",
      "4566                         //This loop finds the element in the a slice to add.\n",
      "4567                         for (unsigned int rd = reduce_dim0; rd < nd; ++rd)\n",
      "4568                         {\n",
      "4569                             unsigned int pos_d;\n",
      "4570                             if (a_str[rd] && (!z_str[rd])) // this means 'd' is a dimension we are reducing over\n",
      "4571                             {\n",
      "4572                                 if (log2_dims_a[rd]==-1)\n",
      "4573                                 {\n",
      "4574                                     // this branch is not preferred,\n",
      "4575                                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4576                                     pos_d = (reduce_ii % dims_a[rd]);\n",
      "4577                                     reduce_ii = (reduce_ii / dims_a[rd]);\n",
      "4578                                 }\n",
      "4579                                 else\n",
      "4580                                 {\n",
      "4581                                     pos_d = (reduce_ii & ((1 << log2_dims_a[rd])-1)); //take the lower log2_dims bits\n",
      "4582                                     reduce_ii = (reduce_ii >> log2_dims_a[rd]);  //shift those lower log2_dims bits off of ii\n",
      "4583                                 }\n",
      "4584                                 a_data_ri += pos_d * a_str[rd];\n",
      "4585                             }\n",
      "4586                         }\n",
      "4587                         sum += a_data_ri[0];\n",
      "4588                     }\n",
      "4589                 }\n",
      "4590         }\n",
      "4591         z_data_i[0] = sum;\n",
      "4592     }\n",
      "4593 }\n",
      "4594 \n",
      "4595 static __global__ void kernel_reduce_sum_1011(\n",
      "4596         const unsigned int d0,\n",
      "4597         const unsigned int d1,\n",
      "4598         const unsigned int d2,\n",
      "4599         const unsigned int d3,\n",
      "4600         const float *A, const int sA0, const int sA1, const int sA2, const int sA3,\n",
      "4601         float * Z, const int sZ0)\n",
      "4602 {\n",
      "4603     const int threadCount = blockDim.x * blockDim.y * blockDim.z;\n",
      "4604     const int threadNum = threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;\n",
      "4605     extern __shared__ float buf[];\n",
      "4606     float mysum = 0.0f;\n",
      "4607 \n",
      "4608     if (warpSize != 32)\n",
      "4609     {\n",
      "4610         return;  //TODO: set error code\n",
      "4611     }\n",
      "4612 \n",
      "4613     for (int i0 = threadIdx.z; i0 < d0; i0 += blockDim.z)\n",
      "4614     {\n",
      "4615         float Ai = A[i0 * sA0 + blockIdx.x * sA1 + threadIdx.y * sA2 + threadIdx.x * sA3];\n",
      "4616         mysum += Ai;\n",
      "4617     }\n",
      "4618     buf[threadNum] = mysum;\n",
      "4619     __syncthreads();\n",
      "4620 \n",
      "4621     // rest of function is handled by one warp\n",
      "4622     if (threadNum < warpSize)\n",
      "4623     {\n",
      "4624         for (int i = threadNum + warpSize; i < threadCount; i += warpSize)\n",
      "4625         {\n",
      "4626             mysum += buf[i];\n",
      "4627         }\n",
      "4628         buf[threadNum] = mysum;\n",
      "4629         if (threadNum < 16)\n",
      "4630         {\n",
      "4631             //reduce so that threadNum 0 has the sum of everything\n",
      "4632             if(threadNum + 16 < threadCount) buf[threadNum] += buf[threadNum+16];\n",
      "4633             if(threadNum + 8 < threadCount) buf[threadNum] += buf[threadNum+8];\n",
      "4634             if(threadNum + 4 < threadCount) buf[threadNum] += buf[threadNum+4];\n",
      "4635             if(threadNum + 2 < threadCount) buf[threadNum] += buf[threadNum+2];\n",
      "4636             if(threadNum + 1 < threadCount) buf[threadNum] += buf[threadNum+1];\n",
      "4637             if (threadNum == 0)\n",
      "4638             {\n",
      "4639                 Z[blockIdx.x*sZ0] = buf[0];\n",
      "4640             }\n",
      "4641         }\n",
      "4642     }\n",
      "4643 }\n",
      "4644 /**\n",
      "4645  * Dimensions in which the self has size 1 and A has size > 1 are considered summing dimensions\n",
      "4646  * Dimensions in which self has size > 1 and A has size > 1 are considered non-summing dimensions, and in this case their sizes must be equal.\n",
      "4647  */\n",
      "4648 int\n",
      "4649 CudaNdarray_reduce_sum(CudaNdarray * self, CudaNdarray * A)\n",
      "4650 {\n",
      "4651     int verbose = 0;\n",
      "4652     //check input rank\n",
      "4653     if (self->nd != A->nd)\n",
      "4654     {\n",
      "4655         PyErr_Format(PyExc_TypeError, \"Rank mismatch in CudaNdarray_sum: %i vs %i\", self->nd, A->nd);\n",
      "4656         return -1;\n",
      "4657     }\n",
      "4658     for (int i = 0; i < self->nd; ++i)\n",
      "4659     {\n",
      "4660         if ((CudaNdarray_HOST_DIMS(self)[i] > 1) && (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(A)[i]))\n",
      "4661         {\n",
      "4662             PyErr_Format(PyExc_TypeError, \"Dimension mismatch in CudaNdarray_sum: self->dim[%i] == %i , A->dim[%i] = %i\",\n",
      "4663                     i, CudaNdarray_HOST_DIMS(self)[i], i, CudaNdarray_HOST_DIMS(A)[i]);\n",
      "4664             return -1;\n",
      "4665         }\n",
      "4666     }\n",
      "4667 \n",
      "4668     int n_summations = (unsigned int)CudaNdarray_SIZE(self);\n",
      "4669     if (verbose)\n",
      "4670     {\n",
      "4671         std::cerr << \"reduce_sum n_summations \" << n_summations  << '\\n';\n",
      "4672         std::cerr << \"reduce_sum nd \" << self->nd  << '\\n';\n",
      "4673         fprint_CudaNdarray(stderr, A);\n",
      "4674         fprint_CudaNdarray(stderr, self);\n",
      "4675     }\n",
      "4676     if (0 && (A->nd == 4) //check to see if kernel_reduce_sum_1011 applies\n",
      "4677             && (CudaNdarray_HOST_DIMS(self)[0] == 1)\n",
      "4678             && (CudaNdarray_HOST_DIMS(self)[2] == 1)\n",
      "4679             && (CudaNdarray_HOST_DIMS(self)[3] == 1)\n",
      "4680        )\n",
      "4681     {\n",
      "4682         dim3 n_threads(CudaNdarray_HOST_DIMS(A)[3], CudaNdarray_HOST_DIMS(A)[2]);\n",
      "4683         dim3 n_blocks(CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4684         while (n_threads.x * n_threads.y * n_threads.z < NUM_VECTOR_OP_THREADS_PER_BLOCK) ++n_threads.z;\n",
      "4685         n_threads.z -= 1;\n",
      "4686         if (n_threads.z > 64) n_threads.z = 64;\n",
      "4687         if (n_threads.z)\n",
      "4688         {\n",
      "4689             if (verbose) printf(\"trying kernel_reduce_sum_1011\\n\");\n",
      "4690             int n_shared = sizeof(float) * n_threads.x * n_threads.y * n_threads.z;\n",
      "4691             kernel_reduce_sum_1011<<<n_blocks, n_threads, n_shared>>>(\n",
      "4692                     CudaNdarray_HOST_DIMS(A)[0],\n",
      "4693                     CudaNdarray_HOST_DIMS(A)[1],\n",
      "4694                     CudaNdarray_HOST_DIMS(A)[2],\n",
      "4695                     CudaNdarray_HOST_DIMS(A)[3],\n",
      "4696                     CudaNdarray_DEV_DATA(A),\n",
      "4697                     CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4698                     CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4699                     CudaNdarray_HOST_STRIDES(A)[2],\n",
      "4700                     CudaNdarray_HOST_STRIDES(A)[3],\n",
      "4701                     CudaNdarray_DEV_DATA(self),\n",
      "4702                     CudaNdarray_HOST_STRIDES(self)[1]);\n",
      "4703             CNDA_THREAD_SYNC;\n",
      "4704             if (cudaSuccess == cudaGetLastError()) return 0;\n",
      "4705             if (verbose) printf(\"failed, falling back to kernel_reduce_sum\\n\");\n",
      "4706         }\n",
      "4707     }\n",
      "4708 \n",
      "4709     int n_threads_per_block = std::min(n_summations,\n",
      "4710             NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4711     int n_blocks = std::min(ceil_intdiv(n_summations,n_threads_per_block),\n",
      "4712             NUM_VECTOR_OP_BLOCKS);\n",
      "4713     int n_structure_cache = self->nd * 4 * sizeof(int);\n",
      "4714 \n",
      "4715     if (verbose)\n",
      "4716     {\n",
      "4717         std::cerr << \"n_blocks, n_threads_per_block \" << n_blocks << ' ' << n_threads_per_block  << '\\n';\n",
      "4718     }\n",
      "4719     assert (self->nd > 0);\n",
      "4720     assert (self->nd == A->nd);\n",
      "4721     kernel_reduce_sum<<<n_blocks, n_threads_per_block, n_structure_cache>>>(\n",
      "4722             n_summations,\n",
      "4723             self->nd,\n",
      "4724             CudaNdarray_DEV_DIMS(A),\n",
      "4725             CudaNdarray_DEV_LOG2DIMS(A),\n",
      "4726             CudaNdarray_DEV_STRIDES(A),\n",
      "4727             CudaNdarray_DEV_DATA(A),\n",
      "4728             CudaNdarray_DEV_STRIDES(self),\n",
      "4729             CudaNdarray_DEV_DATA(self));\n",
      "4730     CNDA_THREAD_SYNC;\n",
      "4731     cudaError_t err = cudaGetLastError();\n",
      "4732     if (cudaSuccess != err)\n",
      "4733     {\n",
      "4734         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kernel_reduce_sum\", cudaGetErrorString(err));\n",
      "4735         return -1;\n",
      "4736     }\n",
      "4737     return 0;\n",
      "4738 }\n",
      "4739 int\n",
      "4740 CudaNdarray_reduce_prod(CudaNdarray * self, const CudaNdarray * A)\n",
      "4741 {\n",
      "4742     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4743     return -1;\n",
      "4744 }\n",
      "4745 int\n",
      "4746 CudaNdarray_reduce_min(CudaNdarray * self, const CudaNdarray * A)\n",
      "4747 {\n",
      "4748     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4749     return -1;\n",
      "4750 }\n",
      "4751 int\n",
      "4752 CudaNdarray_reduce_max(CudaNdarray * self, const CudaNdarray * A)\n",
      "4753 {\n",
      "4754     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4755     return -1;\n",
      "4756 }\n",
      "4757 \n",
      "4758 \n",
      "4759 /**\n",
      "4760  *\n",
      "4761  *  pattern is a permutation of [0, 1, ... self->nd-1] with the following twists:\n",
      "4762  *  - an element 'd' of the permutation can be dropped if CudaNdarray_HOST_DIMS(self)[d] == 1\n",
      "4763  *  - any number of '-1' elements can be in the pattern, and they will cause new ranks (with dim==1) to be inserted.\n",
      "4764  *\n",
      "4765  *  For example, if CudaNdarray_HOST_DIMS(self) == [4, 5, 1, 6], and pattern = [0,3,-1,-1, 1], then CudaNdarray_HOST_DIMS(self) would be modified to become:\n",
      "4766  *     [4, 6, 1, 1, 5] (we dropped the original dim[2]==1, and inserted two singleton dimensions with the -1s.\n",
      "4767  */\n",
      "4768 int\n",
      "4769 CudaNdarray_dimshuffle(CudaNdarray * self, unsigned int len, const int * pattern)\n",
      "4770 {\n",
      "4771     //TODO: pass a workspace pointer to avoid the internal malloc\n",
      "4772     int * newdims = (int *)malloc(sizeof(int) * (len + len + self->nd)); //we tack on the taken buffer here for speed of not having to malloc twice.\n",
      "4773     int * newstrides = newdims + len;\n",
      "4774     int * dims_taken = newstrides + len;\n",
      "4775     if (!newdims)\n",
      "4776     {\n",
      "4777         PyErr_SetString(PyExc_MemoryError, \"CudaNdarray_dimshuffle: Failed to allocate temporary space\");\n",
      "4778         return -1;\n",
      "4779     }\n",
      "4780     for (int i = 0; i < self->nd; ++i)\n",
      "4781     {\n",
      "4782         dims_taken[i] = 0;\n",
      "4783     }\n",
      "4784     for (int i = 0; i < len; ++i)\n",
      "4785     {\n",
      "4786         if (pattern[i] < 0)\n",
      "4787         {\n",
      "4788             newdims[i] = 1;\n",
      "4789             newstrides[i] = 0;\n",
      "4790         }\n",
      "4791         else if(dims_taken[pattern[i]])\n",
      "4792         {\n",
      "4793             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You used the dimensions %d multiple time\",\n",
      "4794                          pattern[i]);\n",
      "4795             free(newdims);\n",
      "4796             return -1;\n",
      "4797         }\n",
      "4798         else if (pattern[i]>= self->nd)\n",
      "4799         {\n",
      "4800             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You asked for a dimensions that don't exist %d for a %d dims CudaNdarray\",\n",
      "4801                          pattern[i], self->nd);\n",
      "4802             free(newdims);\n",
      "4803             return -1;\n",
      "4804         }\n",
      "4805         else\n",
      "4806         {\n",
      "4807             newdims[i] = CudaNdarray_HOST_DIMS(self)[pattern[i]];\n",
      "4808             newstrides[i] = CudaNdarray_HOST_STRIDES(self)[pattern[i]];\n",
      "4809             dims_taken[pattern[i]] = 1;\n",
      "4810         }\n",
      "4811     }\n",
      "4812     //Check if we dropped not broadcastable dims\n",
      "4813     for (int i = 0; i < self->nd; ++i)\n",
      "4814     {\n",
      "4815         if (dims_taken[i]==0 && CudaNdarray_HOST_DIMS(self)[i]!=1)\n",
      "4816         {\n",
      "4817             PyErr_SetString(PyExc_ValueError, \"Cudandarray_dimshuffle: You cannot drop a non-broadcastable dimension.\");\n",
      "4818             free(newdims);\n",
      "4819             return -1;\n",
      "4820         }\n",
      "4821     }\n",
      "4822     //swap this structure in for the one in self, and sync to the card\n",
      "4823     if (CudaNdarray_set_nd(self, len))\n",
      "4824     {\n",
      "4825         free(newdims);\n",
      "4826         return -1;\n",
      "4827     }\n",
      "4828     for (int i = 0; i < len; ++i)\n",
      "4829     {\n",
      "4830         CudaNdarray_set_dim(self, i, newdims[i]);\n",
      "4831         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "4832     }\n",
      "4833     if (cnda_copy_structure_to_device(self))\n",
      "4834     {\n",
      "4835         free(newdims);\n",
      "4836         return -1;\n",
      "4837     }\n",
      "4838     free(newdims);\n",
      "4839     return 0;\n",
      "4840 }\n",
      "4841 \n",
      "4842 \n",
      "4843 \n",
      "4844 /**\n",
      "4845  *\n",
      "4846  *  This is the function that bind to python.\n",
      "4847  *  See CudaNdarray_dimshuffle to call from C.\n",
      "4848  *  We use -1 to mean 'x' as in Tensor Dimshuffle.\n",
      "4849  */\n",
      "4850 PyObject *\n",
      "4851 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args)\n",
      "4852 {\n",
      "4853     PyObject * self = NULL;\n",
      "4854     PyObject * pattern_object = NULL;\n",
      "4855     int * pattern = NULL;\n",
      "4856     PyObject * rval = NULL;\n",
      "4857     int success = -1;\n",
      "4858     //const int * dims = NULL;\n",
      "4859 \n",
      "4860     //args should consist of two python objects (\"OO\")\n",
      "4861     if (! PyArg_ParseTuple(args, \"OO\", &self, &pattern_object))\n",
      "4862         return NULL;\n",
      "4863 \n",
      "4864     if (!CudaNdarray_Check(self) )\n",
      "4865     {\n",
      "4866         PyErr_SetString(PyExc_TypeError, \"First argument to cuda_ndarray.dimshuffle must be a CudaNdarray\");\n",
      "4867         return NULL;\n",
      "4868     }\n",
      "4869 \n",
      "4870     //parse pattern_object into int * pattern\n",
      "4871 \n",
      "4872     Py_ssize_t pattern_dim =  PyObject_Length(pattern_object);\n",
      "4873 \n",
      "4874     if (pattern_dim < 0)\n",
      "4875     {\n",
      "4876         PyErr_SetString(PyExc_TypeError, \"Couldn't get length of third argument to cuda_ndarray.dimshuffle\");\n",
      "4877         return NULL;\n",
      "4878     }\n",
      "4879 \n",
      "4880     pattern = (int *) malloc( pattern_dim * sizeof(int));\n",
      "4881 \n",
      "4882     for (Py_ssize_t i = 0; i < pattern_dim; i++)\n",
      "4883     {\n",
      "4884         PyObject * idx = PyLong_FromLong(i);\n",
      "4885 \n",
      "4886         if (idx == NULL)\n",
      "4887         {\n",
      "4888             PyErr_SetString(PyExc_Exception, \"Couldn't make long object to loop over list/tuple\");\n",
      "4889             goto CudaNdarray_dimshuffle_fail;\n",
      "4890         }\n",
      "4891 \n",
      "4892         long elem_value = 0;\n",
      "4893 \n",
      "4894         PyObject * elem = PyObject_GetItem(pattern_object, idx);\n",
      "4895 \n",
      "4896         if (elem == NULL)\n",
      "4897         {\n",
      "4898             Py_XDECREF( elem);\n",
      "4899             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "4900             goto CudaNdarray_dimshuffle_fail;\n",
      "4901         }\n",
      "4902 \n",
      "4903         elem_value = PyInt_AsLong(elem);\n",
      "4904 \n",
      "4905         if (elem_value == -1 && PyErr_Occurred() )\n",
      "4906         {\n",
      "4907             Py_XDECREF(elem);\n",
      "4908             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "4909             goto CudaNdarray_dimshuffle_fail;\n",
      "4910         }\n",
      "4911 \n",
      "4912         pattern[i] = elem_value;\n",
      "4913 \n",
      "4914         Py_XDECREF( elem );\n",
      "4915         Py_XDECREF( idx );\n",
      "4916     }\n",
      "4917 \n",
      "4918     //allocate rval\n",
      "4919     rval =  (PyObject *) CudaNdarray_View((CudaNdarray *) self);\n",
      "4920 \n",
      "4921     if (rval == NULL)\n",
      "4922     {\n",
      "4923         //CudaNdarray_New should have set the exception string\n",
      "4924         goto CudaNdarray_dimshuffle_fail;\n",
      "4925     }\n",
      "4926 \n",
      "4927 \n",
      "4928     //printf(\"pattern_dim: %d\\n\",pattern_dim);\n",
      "4929     //printf(\"pattern: %d %d\\n\",pattern[0],pattern[1]);\n",
      "4930     //dims = CudaNdarray_HOST_DIMS( (CudaNdarray *) self);\n",
      "4931     //printf(\"dims before: %d %d\\n\",dims[0],dims[1]);\n",
      "4932 \n",
      "4933     success = CudaNdarray_dimshuffle((CudaNdarray *) rval, pattern_dim, pattern);\n",
      "4934 \n",
      "4935     if (success != 0)\n",
      "4936     {\n",
      "4937         //Exception string should already be set by CudaNdarray_dimshuffle\n",
      "4938         goto CudaNdarray_dimshuffle_fail;\n",
      "4939     }\n",
      "4940 \n",
      "4941     free(pattern);\n",
      "4942 \n",
      "4943     return rval;\n",
      "4944 \n",
      "4945     CudaNdarray_dimshuffle_fail:\n",
      "4946 \n",
      "4947     if (pattern != NULL)\n",
      "4948         free(pattern);\n",
      "4949 \n",
      "4950     Py_XDECREF(rval);\n",
      "4951     return NULL;\n",
      "4952 }\n",
      "4953 \n",
      "4954 \n",
      "4955 int\n",
      "4956 cnda_structure_size(int nd)\n",
      "4957 {\n",
      "4958     // dim0, dim1, ...\n",
      "4959     // str0, str1, ...\n",
      "4960     // log2(dim0), log2(dim1), ...\n",
      "4961     return nd + nd + nd;\n",
      "4962 }\n",
      "4963 \n",
      "4964 const int *\n",
      "4965 CudaNdarray_HOST_DIMS(const CudaNdarray * self)\n",
      "4966 {\n",
      "4967     return self->host_structure;\n",
      "4968 }\n",
      "4969 \n",
      "4970 const int *\n",
      "4971 CudaNdarray_HOST_STRIDES(const CudaNdarray * self)\n",
      "4972 {\n",
      "4973     return self->host_structure + self->nd;\n",
      "4974 }\n",
      "4975 const int *\n",
      "4976 CudaNdarray_HOST_LOG2DIMS(const CudaNdarray * self)\n",
      "4977 {\n",
      "4978     return self->host_structure + 2*self->nd;\n",
      "4979 }\n",
      "4980 \n",
      "4981 int\n",
      "4982 CudaNdarray_EqualAndIgnore(CudaNdarray *cnda1, CudaNdarray *cnda2, int ignoreSync, int ignoreBase)\n",
      "4983 {\n",
      "4984     int verbose = 0;\n",
      "4985 \n",
      "4986     if (!ignoreSync && cnda1->dev_structure_fresh != cnda2->dev_structure_fresh)\n",
      "4987     {\n",
      "4988         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 1\\n\");\n",
      "4989         return 0;\n",
      "4990     }\n",
      "4991 \n",
      "4992     if (cnda1->nd != cnda2->nd)\n",
      "4993     {\n",
      "4994         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 2\\n\");\n",
      "4995         return 0;\n",
      "4996     }\n",
      "4997 \n",
      "4998     for (int i=0; i < 2*cnda1->nd; i++)\n",
      "4999     {\n",
      "5000         if (cnda1->host_structure[i] != cnda2->host_structure[i])\n",
      "5001         {\n",
      "5002             if(verbose)\n",
      "5003                 fprintf(stdout, \"CUDANDARRAY_EQUAL : host_structure : %d, %d, %d\\n\", i, cnda1->host_structure[i], cnda2->host_structure[i]);\n",
      "5004             return 0;\n",
      "5005         }\n",
      "5006     }\n",
      "5007 \n",
      "5008     if (!ignoreBase && cnda1->base != cnda2->base)\n",
      "5009     {\n",
      "5010         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 4\");\n",
      "5011         return 0;\n",
      "5012     }\n",
      "5013     else if (cnda1->data_allocated != cnda2->data_allocated)\n",
      "5014     {\n",
      "5015         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 5\");\n",
      "5016         return 0;\n",
      "5017     }\n",
      "5018     else if (cnda1->data_allocated && cnda1->devdata != cnda2->devdata)\n",
      "5019     {\n",
      "5020         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 6\");\n",
      "5021         // no need to check devdata if data is not allocated\n",
      "5022         return 0;\n",
      "5023     }\n",
      "5024 \n",
      "5025     return 1;\n",
      "5026 }\n",
      "5027 \n",
      "5028 \n",
      "5029 int\n",
      "5030 CudaNdarray_Equal(CudaNdarray *cnda1, CudaNdarray *cnda2)\n",
      "5031 {\n",
      "5032     return CudaNdarray_EqualAndIgnore(cnda1, cnda2, 0, 0);\n",
      "5033 }\n",
      "5034 \n",
      "5035 int\n",
      "5036 cnda_copy_structure_to_device(const CudaNdarray * self)\n",
      "5037 {\n",
      "5038     //If the device structure do not exists, create it.\n",
      "5039     //We allocate it here as we do not need it often.\n",
      "5040     //In fact, we need it so infrequently that we expect\n",
      "5041     //that most object won't need it. Not allocating it\n",
      "5042     //save a significant when creating object.\n",
      "5043     //This speed up a benchmark by 8% with the gc.\n",
      "5044     if (!self->dev_structure)\n",
      "5045     {\n",
      "5046         int struct_size = cnda_structure_size(self->nd);\n",
      "5047         if (struct_size)\n",
      "5048         {\n",
      "5049             self->dev_structure = (int*)device_malloc(struct_size* sizeof(int));\n",
      "5050             if (NULL == self->dev_structure)\n",
      "5051             {\n",
      "5052                 return -1;\n",
      "5053             }\n",
      "5054         }\n",
      "5055     }\n",
      "5056     if (cublasSetVector(cnda_structure_size(self->nd),\n",
      "5057                         sizeof(int),\n",
      "5058                         self->host_structure,\n",
      "5059                         1,\n",
      "5060                         self->dev_structure,\n",
      "5061                         1) != CUBLAS_STATUS_SUCCESS)\n",
      "5062     {\n",
      "5063         PyErr_SetString(PyExc_RuntimeError, \"error copying structure to device memory\");\n",
      "5064         return -1;\n",
      "5065     }\n",
      "5066     self->dev_structure_fresh = 1;\n",
      "5067     return 0;\n",
      "5068 }\n",
      "5069 \n",
      "5070 const int *\n",
      "5071 CudaNdarray_DEV_DIMS(const CudaNdarray * self)\n",
      "5072 {\n",
      "5073     if (!self->dev_structure_fresh)\n",
      "5074     {\n",
      "5075         if (cnda_copy_structure_to_device(self))\n",
      "5076             return NULL;\n",
      "5077     }\n",
      "5078     return self->dev_structure;\n",
      "5079 }\n",
      "5080 const int *\n",
      "5081 CudaNdarray_DEV_STRIDES(const CudaNdarray * self)\n",
      "5082 {\n",
      "5083     if (!self->dev_structure_fresh)\n",
      "5084     {\n",
      "5085         if (cnda_copy_structure_to_device(self))\n",
      "5086             return NULL;\n",
      "5087     }\n",
      "5088     return self->dev_structure + self->nd;\n",
      "5089 }\n",
      "5090 const int *\n",
      "5091 CudaNdarray_DEV_LOG2DIMS(const CudaNdarray * self)\n",
      "5092 {\n",
      "5093     if (!self->dev_structure_fresh)\n",
      "5094     {\n",
      "5095         if (cnda_copy_structure_to_device(self))\n",
      "5096             return NULL;\n",
      "5097     }\n",
      "5098     return self->dev_structure + 2*self->nd;\n",
      "5099 }\n",
      "5100 float *\n",
      "5101 CudaNdarray_DEV_DATA(const CudaNdarray * self)\n",
      "5102 {\n",
      "5103     return self->devdata;\n",
      "5104 }\n",
      "5105 \n",
      "5106 /**\n",
      "5107  * Return the number of elements in the ndarray (product of the dimensions)\n",
      "5108  */\n",
      "5109 int\n",
      "5110 CudaNdarray_SIZE(const CudaNdarray *self)\n",
      "5111 {\n",
      "5112     if (self->nd == -1) return 0;\n",
      "5113     int size = 1;\n",
      "5114     for (int i = 0; i < self->nd; ++i)\n",
      "5115     {\n",
      "5116         size *= CudaNdarray_HOST_DIMS(self)[i];\n",
      "5117     }\n",
      "5118     return size;\n",
      "5119 }\n",
      "5120 size_t\n",
      "5121 CudaNdarray_SIZEt(const CudaNdarray *self)\n",
      "5122 {\n",
      "5123     if (self->nd == -1) return 0;\n",
      "5124     size_t size = 1;\n",
      "5125     for (int i = 0; i < self->nd; ++i)\n",
      "5126     {\n",
      "5127         size *= CudaNdarray_HOST_DIMS(self)[i];\n",
      "5128     }\n",
      "5129     return size;\n",
      "5130 }\n",
      "5131 \n",
      "5132 PyObject *\n",
      "5133 CudaNdarray_SIZE_Object(const CudaNdarray *self, void *closure)\n",
      "5134 {\n",
      "5135     return PyInt_FromLong(CudaNdarray_SIZE(self));\n",
      "5136 }\n",
      "5137 \n",
      "5138 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, const CudaNdarray * base)\n",
      "5139 {\n",
      "5140     return CudaNdarray_set_device_data(self, data, (PyObject *) base);\n",
      "5141 }\n",
      "5142 \n",
      "5143 PyObject * CudaNdarray_IS_C_Contiguous(CudaNdarray * self)\n",
      "5144 {\n",
      "5145     return PyBool_FromLong(CudaNdarray_is_c_contiguous(self));\n",
      "5146 }\n",
      "5147 \n",
      "5148 int fprint_CudaNdarray(FILE * fd, const CudaNdarray *self)\n",
      "5149 {\n",
      "5150     cudaError_t err = cudaGetLastError();\n",
      "5151     if( cudaSuccess != err)\n",
      "5152     {\n",
      "5153         PyErr_Format(PyExc_RuntimeError,\n",
      "5154                      \"Cuda error: %s: %s.\",\n",
      "5155                      \"fprint_CudaNdarray was called with an uncleared error\",\n",
      "5156                      cudaGetErrorString(err));\n",
      "5157         return -1;\n",
      "5158     }\n",
      "5159     fprintf(fd, \"CudaNdarray <%p, %p> nd=%i dev_structure_fresh=%d data_allocated=%d\\n\",\n",
      "5160             self, self->devdata, self->nd, self->dev_structure_fresh, self->data_allocated);\n",
      "5161     fprintf(fd, \"\\tHOST_DIMS:      \");\n",
      "5162     for (int i = 0; i < self->nd; ++i)\n",
      "5163     {\n",
      "5164         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_DIMS(self)[i]);\n",
      "5165     }\n",
      "5166     fprintf(fd, \"\\n\\tHOST_STRIDES: \");\n",
      "5167     for (int i = 0; i < self->nd; ++i)\n",
      "5168     {\n",
      "5169         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "5170     }\n",
      "5171 \n",
      "5172     if (self->dev_structure)\n",
      "5173     {\n",
      "5174         int data=0;\n",
      "5175         fprintf(fd, \"\\n\\tDEV_DIMS:      \");\n",
      "5176         for (int i = 0; i < self->nd; ++i)\n",
      "5177         {\n",
      "5178             cublasGetVector(1, sizeof(int),\n",
      "5179                             self->dev_structure+i, 1,\n",
      "5180                             &data, 1);\n",
      "5181             fprintf(fd, \"%i\\t\", data);\n",
      "5182         }\n",
      "5183         fprintf(fd, \"\\n\\tDEV_STRIDES: \");\n",
      "5184         for (int i = 0; i < self->nd; ++i)\n",
      "5185         {\n",
      "5186             cublasGetVector(1, sizeof(int),\n",
      "5187                             self->dev_structure + self->nd+i, 1,\n",
      "5188                             &data, 1);\n",
      "5189             fprintf(fd, \"%i \\t\", data);\n",
      "5190         }\n",
      "5191         fprintf(fd, \"\\n\");\n",
      "5192     }\n",
      "5193     else\n",
      "5194     {\n",
      "5195         fprintf(fd, \"\\n\\tdev_structure not allocated\\n\");\n",
      "5196     }\n",
      "5197 \n",
      "5198     err = cudaGetLastError();\n",
      "5199     if( cudaSuccess != err)\n",
      "5200     {\n",
      "5201         PyErr_Format(PyExc_RuntimeError,\n",
      "5202                      \"Cuda error: %s: %s.\",\n",
      "5203                      \"fprint_CudaNdarray\",\n",
      "5204                      cudaGetErrorString(err));\n",
      "5205         return -1;\n",
      "5206     }\n",
      "5207     return 0;\n",
      "5208 }\n",
      "5209 \n",
      "5210 \n",
      "5211 int CudaNdarray_prep_output(CudaNdarray ** arr, int nd,\n",
      "5212                             const int * dims, int fortran)\n",
      "5213 {\n",
      "5214     bool allocated = false;\n",
      "5215     if (*arr == NULL)\n",
      "5216     {\n",
      "5217         // This allocates the metadata but not the data\n",
      "5218         *arr = (CudaNdarray *) CudaNdarray_new_nd(nd);\n",
      "5219         if (*arr == NULL)\n",
      "5220             return -1;\n",
      "5221         allocated = true;\n",
      "5222     }\n",
      "5223 \n",
      "5224     if (CudaNdarray_alloc_contiguous(*arr, nd, dims, fortran))\n",
      "5225     {\n",
      "5226         if (allocated)\n",
      "5227         {\n",
      "5228             Py_DECREF(*arr);\n",
      "5229             *arr = NULL;\n",
      "5230         }\n",
      "5231         return -1;\n",
      "5232     }\n",
      "5233     return 0;\n",
      "5234 }\n",
      "5235 \n",
      "5236 \n",
      "5237 /*\n",
      "5238   Local Variables:\n",
      "5239   mode:c++\n",
      "5240   c-basic-offset:4\n",
      "5241   c-file-style:\"stroustrup\"\n",
      "5242   indent-tabs-mode:nil\n",
      "5243   fill-column:79\n",
      "5244   End:\n",
      "5245 */\n",
      "5246 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:textwidth=79 :\n",
      "5247 \n",
      "===============================\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:17:0: warning: \"PyString_Check\" redefined\n",
      " #define PyString_Check PyUnicode_Check\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:63:0: note: this is the location of the previous definition\n",
      " #define PyString_Check PyBytes_Check\n",
      " ^\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:18:0: warning: \"PyString_FromString\" redefined\n",
      " #define PyString_FromString PyUnicode_FromString\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:65:0: note: this is the location of the previous definition\n",
      " #define PyString_FromString PyBytes_FromString\n",
      " ^\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:19:0: warning: \"PyString_AsString\" redefined\n",
      " #define PyString_AsString PyUnicode_AsUTF8\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:72:0: note: this is the location of the previous definition\n",
      " #define PyString_AsString PyBytes_AsString\n",
      " ^\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:20:0: warning: \"PyString_FromStringAndSize\" redefined\n",
      " #define PyString_FromStringAndSize PyUnicode_FromStringAndSize\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:66:0: note: this is the location of the previous definition\n",
      " #define PyString_FromStringAndSize PyBytes_FromStringAndSize\n",
      " ^\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:21:0: warning: \"PyString_Size\" redefined\n",
      " #define PyString_Size PyUnicode_GET_SIZE\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:74:0: note: this is the location of the previous definition\n",
      " #define PyString_Size PyBytes_Size\n",
      " ^\n",
      "mod.cu(1019): warning: statement is unreachable\n",
      "mod.cu(2945): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2948): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2950): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2953): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2955): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2958): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2961): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2964): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2966): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2969): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2971): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2974): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2976): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2979): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2982): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2985): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2987): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2990): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2992): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2995): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:17:0: warning: \"PyString_Check\" redefined\n",
      " #define PyString_Check PyUnicode_Check\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:63:0: note: this is the location of the previous definition\n",
      " #define PyString_Check PyBytes_Check\n",
      " ^\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:18:0: warning: \"PyString_FromString\" redefined\n",
      " #define PyString_FromString PyUnicode_FromString\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:65:0: note: this is the location of the previous definition\n",
      " #define PyString_FromString PyBytes_FromString\n",
      " ^\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:19:0: warning: \"PyString_AsString\" redefined\n",
      " #define PyString_AsString PyUnicode_AsUTF8\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:72:0: note: this is the location of the previous definition\n",
      " #define PyString_AsString PyBytes_AsString\n",
      " ^\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:20:0: warning: \"PyString_FromStringAndSize\" redefined\n",
      " #define PyString_FromStringAndSize PyUnicode_FromStringAndSize\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:66:0: note: this is the location of the previous definition\n",
      " #define PyString_FromStringAndSize PyBytes_FromStringAndSize\n",
      " ^\n",
      "In file included from mod.cu:9:0:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:21:0: warning: \"PyString_Size\" redefined\n",
      " #define PyString_Size PyUnicode_GET_SIZE\n",
      " ^\n",
      "In file included from /home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:9:\n",
      "/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:74:0: note: this is the location of the previous definition\n",
      " #define PyString_Size PyBytes_Size\n",
      " ^\n",
      "mod.cu(1019): warning: statement is unreachable\n",
      "mod.cu(2945): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2948): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2950): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2953): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2955): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2958): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2961): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2964): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2966): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2969): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2971): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2974): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2976): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2979): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2982): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2985): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2987): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2990): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2992): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "mod.cu(2995): warning: conversion from a string literal to \"char *\" is deprecated\n",
      "/usr/include/string.h: In function ‘void* __mempcpy_inline(void*, const void*, size_t)’:\n",
      "/usr/include/string.h:652:42: error: ‘memcpy’ was not declared in this scope\n",
      "   return (char *) memcpy (__dest, __src, __n) + __n;\n",
      "                                          ^\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 1, 'for cmd', 'nvcc -shared -O3 -m64 -Xcompiler -DCUDA_NDARRAY_CUH=m11b90075e2397c684f9dc0f7276eab8f,-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC -Xlinker -rpath,/home/silver/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.1-64/cuda_ndarray -I/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda -I/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include -I/home/silver/anaconda3/include/python3.5m -o /home/silver/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.1-64/cuda_ndarray/cuda_ndarray.so mod.cu -L/home/silver/anaconda3/lib -lpython3.5m -lcublas -lcudart')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['nvcc', '-shared', '-O3', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=m11b90075e2397c684f9dc0f7276eab8f,-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC', '-Xlinker', '-rpath,/home/silver/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.1-64/cuda_ndarray', '-I/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda', '-I/home/silver/anaconda3/lib/python3.5/site-packages/numpy/core/include', '-I/home/silver/anaconda3/include/python3.5m', '-o', '/home/silver/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.1-64/cuda_ndarray/cuda_ndarray.so', 'mod.cu', '-L/home/silver/anaconda3/lib', '-lpython3.5m', '-lcublas', '-lcudart']\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "('The following error happened while compiling the node', GpuFromHost(Elemwise{exp,no_inplace}.0), '\\n', 'You forced the use of gpu device gpu, but CUDA initialization failed with error:\\ncuda unavilable')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-de93af56eb1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msandbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_from_host\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoposort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/silver/anaconda3/lib/python3.5/site-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    264\u001b[0m                 \u001b[0mallow_input_downcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                 \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m                 profile=profile)\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/silver/anaconda3/lib/python3.5/site-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    509\u001b[0m     return orig_function(inputs, cloned_outputs, mode,\n\u001b[0;32m    510\u001b[0m             \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m             on_unused_input=on_unused_input)\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/silver/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input)\u001b[0m\n\u001b[0;32m   1464\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m                        defaults)\n\u001b[0m\u001b[0;32m   1467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m     \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/silver/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, input_storage, trustme)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[1;32m-> 1324\u001b[1;33m                 input_storage=input_storage_lists)\n\u001b[0m\u001b[0;32m   1325\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/silver/anaconda3/lib/python3.5/site-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[1;34m(self, input_storage, output_storage)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_thunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m--> 519\u001b[1;33m                              output_storage=output_storage)[:3]\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/silver/anaconda3/lib/python3.5/site-packages/theano/gof/vm.py\u001b[0m in \u001b[0;36mmake_all\u001b[1;34m(self, profiler, input_storage, output_storage)\u001b[0m\n\u001b[0;32m    895\u001b[0m                                                  \u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m                                                  no_recycling))\n\u001b[0m\u001b[0;32m    898\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lazy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m                     \u001b[1;31m# We don't want all ops maker to think about lazy Ops.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[1;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[0;32m    255\u001b[0m                                     \u001b[0mdefault_to_move_computation_to_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                                     \u001b[0mmove_shared_float32_to_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                                     enable_cuda=False)\n\u001b[0m\u001b[0;32m    258\u001b[0m         return super(GpuOp, self).make_thunk(node, storage_map,\n\u001b[0;32m    259\u001b[0m                                              compute_map, no_recycling)\n",
      "\u001b[1;32m/home/silver/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36muse\u001b[1;34m(device, force, default_to_move_computation_to_gpu, move_shared_float32_to_gpu, enable_cuda, test_driver)\u001b[0m\n\u001b[0;32m    337\u001b[0m                                    \u001b[1;34m\"but CUDA initialization failed \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                                    \"with error:\\n%s\" % (\n\u001b[1;32m--> 339\u001b[1;33m                 device, cuda_initialization_error_message))\n\u001b[0m\u001b[0;32m    340\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnvcc_compiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_nvcc_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         _logger.error('nvcc compiler not found on $PATH.'\n",
      "\u001b[1;31mOSError\u001b[0m: ('The following error happened while compiling the node', GpuFromHost(Elemwise{exp,no_inplace}.0), '\\n', 'You forced the use of gpu device gpu, but CUDA initialization failed with error:\\ncuda unavilable')"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.sandbox.cuda.basic_ops\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), 'float32'))\n",
    "f = function([], sandbox.cuda.basic_ops.gpu_from_host(T.exp(x)))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "print(\"Numpy result is %s\" % (numpy.asarray(r),))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
